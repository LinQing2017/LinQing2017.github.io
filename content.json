{"meta":{"title":"LQing的博客","subtitle":"“做程序员太辛苦了, 我想换行，我该怎么办?” “敲一下回车。”","description":"弄个博客假装技术宅！？？","author":"LQing","url":"https://LinQing2017.github.io","root":"/"},"pages":[],"posts":[{"title":"Spark调优笔记","slug":"Spark调优笔记","date":"2020-05-14T07:59:03.314Z","updated":"2020-05-14T07:59:03.314Z","comments":true,"path":"2020/05/14/Spark调优笔记/","link":"","permalink":"https://linqing2017.github.io/2020/05/14/Spark%E8%B0%83%E4%BC%98%E7%AC%94%E8%AE%B0/","excerpt":"","text":"基础概念每一个Spark任务，从不同的角度、从大到小涉及到以下概念： 按照占用的资源：Application –&gt; Driver/若干Executor –&gt; 每一个Executor 运行多个 Task（线程） 按照任务的逻辑：Application –&gt; 若干连续执行的Job –&gt; 每个Job划分为多个Stage 知识点： 从JVM角度每个Executor是一个独立进程，每个task是该进程的一个子线程。 Job 中划分 Stage 以shuffle操作作为边界，而划分 Job 边界的是代码中的 action。 每个数据 partition 由一个 Task 处理，因此每个 Task 处理的数据量和 partition num 的大小成反比，控制 partition num 可以防止 Executor 发生 Oom。 Task 的并行度由 slot 决定，slots = spark.num.executors * spark.executor.cores / spark.task.cpus。 Shuffle 操作Shuffle描述着数据从map task输出到reduce task输入的这段过程。在分布式情况下，每个 Reduce task 从不同 Map task 输出中拉取相同 Key 的 Records，这一过程会产生网络资源、内存、磁盘IO的消耗。 通常情况下，Shuffle 分成两个部分， 每个 Stage 从上一个 Stage Shuffle Read 得到数据，处理后进行 Shuffle Write 输出给下一个任务 Map阶段的数据准备：Shuffle Write Reduce阶段的数据拷贝：Shuffle Read 在Spark的中负责shuffle的主要组件是ShuffleManager。 ShuffleManager随着Spark的发展有两种实现的方式： HashShuffleManager：Hash Shuffle Spark 1.2以前默认配置 会产生大量的中间磁盘文件，影响磁盘性能 SortShuffleManager：Sort Shuffle 从1.2 开始成为默认配置，并且在 Spark 2.0 时成为唯一的选择 每个Task在进行shuffle后，将所有的临时文件合并(merge)成一个磁盘文件，因此每个Task就只有一个磁盘文件。Reduce Task 根据索引读取每个磁盘文件中的部分数据 参考资料： Spark Shuffle的技术演进 Spark performance optimization: shuffle tuning 内存管理Spark中一个Executor对应一个JVM进程，Executor占用的内存分为两部分：ExecutorMemory和MemoryOverhead。ExecutorMemory是堆区内存，MemoryOverhead是Spark的堆外内存。 在Spark1.6以前的版本中，heap内存是静态管理的，而新版中内存使用动态管理方案进行管理。通过配置项spark.memory.useLegacyMode可以在两种方式中进行切换。 堆内存动态配置的情况下，Heap内存包括以下几个部分： Reserved Memory：预留给系统使用，是固定不变的。默认300MB，并且这一部分是不可变的。 User Memory：临时数据或者是自己维护的一些数据结构使用的内存空间， 默认大小：(Java Heap - Reserved Memory) x （1-spark.memory.fraction）（默认情况下1GB大小的Executor为289MB）。 Spark Memory：系统框架运行时需要使用的空间，这是从两部份构成的分别是 Storage Memeory 和 Execution Memory。前者用来进行RDD缓存，后者用来Shuffle缓存。Storage 和 Execution (Shuffle) 采用了 Unified 的方式共同使用一个内存区域，默认情况下两者各站这一部分内存的50%，当一方内存不足时两者会相互占用对方内存，但是通常情况下Execution (Shuffle)的优先级更高！ 堆外内存Spark的堆外内存称为Memory-Overhead是JVM进程中除Java堆以外占用的空间大小，包括方法区（永久代）、Java虚拟机栈、本地方法栈、JVM进程本身所用的内存、直接内存（DirectMemory）等。通过spark.yarn.executor.memoryOverhead设置，单位MB。 默认情况下，spark.yarn.executor.memoryOverhead的大小按照下面的方式决定： 123MEMORY_OVERHEAD_FACTOR = 0.07 MEMORY_OVERHEAD_MIN = 384min（MEMORY_OVERHEAD_FACTOR*spark.executor.memory，MEMORY_OVERHEAD_MIN） 默认情况下，spark单个Executor占用的内存资源为Spark堆外内存和Heap内存，当堆外内存超出限制时会产生OOM，使Yarn直接杀死容器，这时候没有任何异常。 浅析 Spark Shuffle 内存使用 分区Spark任务的输入在绝大多数场景下存在分区的概念，默认情况下一个分区的数据在一个Task线程中执行。 可以使用以下两个参数，控制 Spark 任务的分区： spark.default.parallelism：控制RDD的分区数量 spark.sql.shuffle.partitions：控制SQL的分区数量 需要注意，进行Shuffle时分区的数目是会发生变化的。 Spark SQL 自动调整 Shuffle Partition这个特性可以将Spark SQL执行Shuffle时，较小的连续分区进行合并，从而自适应 shuffle 时的分区数目。 该特性是 Intel 在Intel-bigdata/spark-adaptive中设计，并且合入了Spark 2.3.1(SPARK-23128)。 spark-adaptive 针对 Spark 有下面三个优化： 在Spark SQL下，自适应 Shuffle Partition 的数量 动态调整执行计划（基于一些中间结果的数据量大小，动态的改变执行计划） 自动处理数据倾斜 参考: Adaptive Execution 让 Spark SQL 更高效更智能 Spark SQL在100TB上的自适应执行实践 Spark Web UISpark UI 的页面中，对检查任务的运行状态最有意义的是 Executors 和 Stages： Executors 可以查看以下内容 Task 运行情况，包括：正在运行数、失败数目、已经完成数目、以及 Task 的总数； Task 累积运行时间，以及累积的GC时间 executor的Input（累积输入）、Shuffle Read（Reduce端读）、Shuffle Write（Map端写） driver和executor的聚合日志 Stages 可以查看以下内容 每个Stages的持续时间（Duration） Stages 的输入/ 输出数据大小（Input/Output）,这个指读写到Hadoop等外部存储的数据，以及从Spark Storage读到的数据 Shuffle Read / Shuffle Write 序列化后，shuffle读写的数据量 Stages 中每个 Task 的统计细节 Shuffle Read Size / Records : 序列化的shuffle读数据大小 Shuffle Write Size / Records : 序列化的shuffle写数据大小 Shuffle Spill (Memory) ： shuffle 过程中spill到disk的数据大小，这里指的是反序列化后数据大小 Shuffle Spill (Disk) ： shuffle 过程中spill到disk的数据大小，这里指的是序列化后数据大小 Oom处理Oom的一些手段： Driver Oom 时通常有以下可能： Executor 返回的序列化结果集太小，而 spark.driver.maxResultSize（默认1g） 太小 Executor Oom 时通常有以下可能： 分区数量过小，那么单个Executor需要处理数据量会增多，使Executor的压力过大。 spark.yarn.executor.memoryOverhead太小，这时候堆外内存溢出，yarn会直接杀死容器，可能spark上可能看不到任何异常。 由于Spark executor中多个Task并行时是共享内存的，因此减少slot可以改善Oom的情况； 数据倾斜造成 OOM（主要原因是，堆内对象的分配和释放是由 JVM 管理的，而 Spark 是通过采样获取已经使用的内存情况，有可能因为采样不准确而不能及时 Spill，导致OOM）；","categories":[],"tags":[]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-pxf使用","date":"2019-11-06T16:00:00.000Z","updated":"2020-05-14T07:59:03.289Z","comments":true,"path":"2019/11/07/Greenplum-pxf使用/","link":"","permalink":"https://linqing2017.github.io/2019/11/07/Greenplum-pxf%E4%BD%BF%E7%94%A8/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 PXFPXF 是 Greenplum 自带的外表连接插件，可以用来连接包括：Hive、HDFS、HBase、S3、JDBC 等外部数据源。 PXF 运行在每个 Segment 主机上，可以直接连接外部数据源获取数据，具有：谓词下推、列式查询等特性。 PXF 基于JAVA，使用时需要在所有Greenplum集群上安装jdk，其安装目录位于$GPHOME/pxf。 配置文件Conf配置目录当用户初始化PXF时，初始化进程会在 $PXF_CONF 目录下创建用户配置目录。 用户配置目录以 /usr/local/greenplum-db/pxf 下的文件为模板，用户应该在 $PXF_CONF 目录中修改配置文件。 用户需要关注 $PXF_CONF/conf 目录中的三个配置文件： pxf-env.sh：定义 PXF 使用的环境变量 pxf-log4j.properties：日志文件 pxf-profiles.xml：自定义profiles 1234567profiles 配置是用来定义：数据格式、依赖jar等内容的，可以认为一个profiles 是 pxf 和外部数据源之间的连接器。通常一个数据源有多种连接器，PXF 已经预定义了一些 profiles 配置（参考$GPHOME/pxf/conf/pxf-profiles-default.xml文件）Hadoop作为外部数据源时，数据格式和数据源（HIVE、HBASE、HDFS）对应的 profiles 参考：[Connectors, Data Formats, and Profiles](https://gpdb.docs.pivotal.io/6-1/pxf/access_hdfs.html) 数据源配置PXF 中将外部数据源定义为Server，用户在创建外部表时可以指定不同的Server，当不指定Server时从默认数据源（default server）拉取数据。 Server的配置存放在 $PXF_CONF/servers/{server_name}/ 目录中，每一个Server表示一种外部数据源。 Server的配置文件取决于类型，可能包含多种配置文件。PXF 还允许不同Greenplum用户连接数据源时，使用不同的配置用户可以在 $PXF_CONF/servers/{server_name}/ 中创建 {greenplum_user_name}-user.xml 文件，根据需要编辑该用户使用参数（比如，身份信息和其他配置）。 12345678910111213&lt;!-- 使用JDBC作为外部存储时，在用户配置中定义JDBC的用户信息 --&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;jdbc.user&lt;/name&gt; &lt;value&gt;pguser1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;jdbc.password&lt;/name&gt; &lt;value&gt;changeme&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 建表语句1234567891011-- 建表语句-- pxf协议中，包含以下参数：-- PROFILE ：指定了配置文件地址-- SERVER ：配置文件中定义的数据源配置-- path-to-data ：数据地址，HDFS上的地址或者HIVE上的表名CREATE [WRITABLE] EXTERNAL TABLE &lt;table_name&gt; ( &lt;column_name&gt; &lt;data_type&gt; [, ...] | LIKE &lt;other_table&gt; )LOCATION('pxf://&lt;path-to-data&gt;?PROFILE=&lt;profile_name&gt;[&amp;SERVER=&lt;server_name&gt;][&amp;&lt;custom-option&gt;=&lt;value&gt;[...]]')FORMAT '[TEXT|CSV|CUSTOM]' (&lt;formatting-properties&gt;); QuickStart以下Demo展示了Greenplum访问HDFS上的Text文本数据： 12345678910111213141516171819202122232425262728# 所有节点指定PXF配置目录echo \"export PXF_CONF=/home/gpadmin/conf/pxf_conf\" &gt;&gt; ~/.bashrc source ~/.bashrc# 在整个集群中初始化pxf服务，pxf会在 $PXF_CONF 指定的路径下生成一系列配置文件$GPHOME/pxf/bin/pxf cluster init# Copy Hadoop环境的以下文件到$PXF_CONF/servers/default目录# core-site.xml# hdfs-site.xml# mapred-site.xml# yarn-site.xml# 当Hadoop使用Kerberos认证时，执行以下操作# # 1. 安装kerberos客户端，yum -y install krb5-libs krb5-workstation# 2. 配置krb5.conf# 3. copy keytab文件到所有segment机器# 4. 修改$PXF_CONF/conf/pxf-env.sh中的以下几个配置项：# PXF_KEYTAB： 使用的keytab# PXF_PRINCIPAL： 认证的账号# PXF_USER_IMPERSONATION：建议改为false# 执行以下命令同步配置到所有segment节点# $GPHOME/pxf/bin/pxf cluster sync$GPHOME/pxf/bin/pxf cluster start 1234567891011121314151617181920-- HDFS 上文本内容如下，文件位于/tmp/pxf_hdfs_simple.txt目录---- Prague,Jan,101,4875.33-- Rome,Mar,87,1557.39-- Bangalore,May,317,8936.99-- Beijing,Jul,411,11600.67-- 创建Greenplum用户用来访问外部表，这里建议和PXF_PRINCIPAL中定义一致，否则需要配置 PXF_USER_IMPERSONATIONCREATE ROLE idata LOGIN REPLICATION CREATEDB CREATEEXTTABLE PASSWORD 'idata';-- 创建db，并且注册pxf插件CREATE DATABASE idata;CREATE EXTENSION pxf;-- 为用户赋pxf协议权限GRANT INSERT ON PROTOCOL pxf TO idata; GRANT SELECT ON PROTOCOL pxf TO idata; -- 建表语句CREATE EXTERNAL TABLE pxf_hdfs_textsimple(location text, month text, num_orders int, total_sales float8) LOCATION ('pxf://tmp/pxf_examples/pxf_hdfs_simple.txt?&amp;PROFILE=hdfs:text')FORMAT 'TEXT' (delimiter=E','); 需要注意：当使用Kerberos认证的Hadoop时，只能使用default作为server配置 参考Introduction to PXF Accessing Hadoop with PXF：读写不同格式的HDFS文件、读写hive表、HBase表 Configuring PXF for Secure HDFS Troubleshooting","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-历史拉链表","date":"2019-11-05T16:00:00.000Z","updated":"2020-05-14T07:59:03.301Z","comments":true,"path":"2019/11/06/Greenplum-历史拉链表/","link":"","permalink":"https://linqing2017.github.io/2019/11/06/Greenplum-%E5%8E%86%E5%8F%B2%E6%8B%89%E9%93%BE%E8%A1%A8/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 《Greenplum企业应用实战》一书第三章读书笔记。 历史拉链表历史拉链表主要用于：记录一个事务从开始一直到当前状态的所有变化信息。相比于定时快照，历史拉链表结构可以避免数据海量存储，是处理缓慢变化数据的常见方式。 记录member事实表的变换情况，可以通过以下四张表实现历史拉链表： member_fatdt0：事实表，添加 dw_end_date 和 dw_beg_date 字段分别表示记录的失效时间和生效时间，使用 dw_end_date 分区，最后一个分区（取值为无穷大）是最新数据，其余分区是失效数据。 member_delta：当天数据库的变更，action字段表示数据操作类型（I,U,D）。 member_tmp0：刷新过程的临时表，有二个分区记录历史数据（当天失效数据）和当前数据，结构和member_fatdt0一样。 member_tmp1：刷新过程的临时表，用来交换分区，结构和member_fatdt0一样。 更新过程： 当天任意数据变更插入到 member_delta 中，但次日凌晨将 member_delta 合并到 member_fatdt0 中 member_delta 和 member_fatdt0 的合并参考下面的步骤： member_fatdt0 和 member_delta 最后一个分区使用 member_id 进行左外连接，关联上了说明数据发生变更，关联不上说明没有发生变更。 关联上的数据修改 dw_end_date 插入 member_tmp0 的历史分区 关联不上的数据插入 member_tmp0 的当前分区（即今天没有发生变更的数据） 将 member_delta 中 action 类型为（I,U）的插入到 member_tmp0 当前数据分区（dw_end_date = 无穷大， dw_beg_date = 当天时间） 将 member_fatdt0 的当天分区和 member_tmp0 历史数据分区交换 将 member_fatdt0 的最后一个分区和 member_tmp0 当前分区交换 查询 member_fatdt0 时通过 dw_end_date 和 dw_beg_date 可以回溯到任意一天的状态。 1select * from public.member_fatdt0 where dw_beg_date &lt;= date'2011-12-01' and dw_end_date &gt;= date'2011-12-01' order by member_id;","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-数据字典","date":"2019-11-05T16:00:00.000Z","updated":"2020-05-14T07:59:03.302Z","comments":true,"path":"2019/11/06/Greenplum-数据字典/","link":"","permalink":"https://linqing2017.github.io/2019/11/06/Greenplum-%E6%95%B0%E6%8D%AE%E5%AD%97%E5%85%B8/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 《Greenplum企业应用实战》一书第四章读书笔记。 数据字典这里的数据字典指的是 Greenplum 中元数据信息，包括：pg_ 和 gp_ 开头的一些系统表。 oidoid 是 PG/GP 中用来表示对象（包括：表、函数、操作符等等）的全局递增 id （是32位数字），GP 中绝大多数数据字典通过oid相互关联。 表名、函数名、操作符名可以和 oid 相互转换： 123456789-- 类似的还有-- regclass : pg_class中的oid关联-- regproc/regprocedure : pg_proc 中的oid关联-- regoper/regoperator：pg_operator中的oid关联-- pg_class表的oid是1259select 1259::regclass ; select oid,relname from pg_class where oid='pg_class'::regclass; 数据库集群信息gp_configurationgp_segment_configuration","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-性能相关的知识","date":"2019-11-05T16:00:00.000Z","updated":"2020-05-14T07:59:03.302Z","comments":true,"path":"2019/11/06/Greenplum-性能相关的知识/","link":"","permalink":"https://linqing2017.github.io/2019/11/06/Greenplum-%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%E7%9A%84%E7%9F%A5%E8%AF%86/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 收集统计信息123456789101112131415-- ANALYZE无参数数时收集整个数据库的统计信息；-- 可以收集单个表，或表的单个列的统计信息；-- 不收集外表的统计信息；-- 通常建议每天运行一次 VACUUM 和 ANALYZE；-- ANALYZE 在表中产生 UPDATE EXCLUSIVE 锁，因此一些查询语句可能为产生冲突；ANALYZE [VERBOSE] [ROOTPARTITION [ALL] ] [table [ (column [, ...] ) ]];-- 收集分区表的根表信息，该命令不会收集普通表的信息ANALYZE ROOTPARTITION ALL;-- 审定test表的note列不收集统计信息；alter table test alter note SET STATISTICS 0; 相关配置： optimizer_analyze_root_partition：为on，ANALYZE命令同样会收集分区表的统计信息 default_statistics_target：ANALYZE命令进行随机抽样时的采样系数，值越大采样越准确，所花的时间更长。一些特定的列可以单独设定这个值（ALTER TABLE … ALTER COLUMN … SET STATISTICS） gp_enable_relsize_collection：没有统计值时，使用表的大小进行估计 gp_autostats_mode：NONE（不收集）、ON_CHANGE（变化收集）、no_no_stats（建表时收集一次） gp_autostats_on_change_threshold：自动收集阈值，默认是20亿 分析执行计划1EXPLAIN [ANALYZE] [VERBOSE] statement EXPLAINEXPLAIN语句不会实际执行语句，只是根据当前收集的统计信息生成执行计划树来评估SQL运行成本。 计划树中每一个节点代表SQL需要进行的操作，并包含以下信息： cost：当前节点以及子节点需要读取的磁盘页，格式为：cost=xxx..xxx(第一行输出时..输出完成时) rows: 表示该节点需要读取的行数 width：平均每行的字节数 EXPLAIN ANALYZEEXPLAIN ANALYZE 会实际执行SQL语句并且提供一些额外的统计信息： actual time：实际执行时间，单位是ms，格式和cost相同 rows：实际返回的行数 loops：？？？ 每个Slice使用的内存情况（应该包括：work_mem和statement_mem的内存使用情况，测试中只要statement_mem够大就不会发生磁盘IO能一定程度提高性能） 执行计划中的重要关键字 数据扫描（Scan）： Seq Scan：顺序扫描，有时候可能带有Dynamic前缀，表示分区顺序扫描 Shared Scan：扫描shared_buffer中的某个slice Index Scan：索引扫描 其他扫描子句：Bitmap Heap Scan、Tid Scan、Subquery Scan、Function Scan 数据移动（Motion）： Gather Motion(N:1)：在master上聚合 Broadcast Motion(N:N)：所有Segment上广播 Redistribute Motion(N:N)：重分布，常见关联、Group by、开窗函数中发生。 重分布除了IO开销之外，还会带来数据不均衡的问题！！ union合并表时，去重会导致重分布，并且此时以整行（所有列）进行重分布，因此慎用union（整行重分布 –&gt; 排序 –&gt; 去重 –&gt; 插入结果集），另外union all虽然和并时不涉及去重，但是在写入结果集时任然会引发重分布，需要注意。 Slice：将SQL拆分多个切片，Montion操作都会产生一个切片，通常Montion操作后会表名其切片号，以及涉及的segment数目。 数据聚合： HashAggregate：基于Group By字段的hash值维护内存hash表，hash表的长度正比于聚合字段的distinct值，对n个聚合字段Greenplum需要维护n个hash表。 GroupAggregate：基于聚合字段排序后，对数据进行一次全扫描从而得到聚合结果。 建议：GroupAggregate的性能相比HashAggregate较为稳定，当聚合函数的种类较多并且聚合键的重复性较差时会使HashAggregate使用的内存急剧上升，此时应该选择GroupAggregate方式聚合。 关联：涉及到广播和重分布 Hash join：通过内存中的Hash表来实现关联 NestLoop：效率最低，执行笛卡尔积时使用该方式 Merge Join：两表按照关联键排序，之后通过归并排序的方式关联（性能不如hash join） 开窗函数： 当开窗函数的分布键不是表的分布键时，会引起表多次的重分布。 如果开窗函数没有partition字段，只有Order字段那么为了维护一个全局序列，所有数据必须汇聚到Master上进行排序操作，此时Master会成为系统瓶颈。 有些参数可以控制优化器的执行计划,参考enable_xxx配置! 优化器开销的计算优化器通过开销的计算结果选择SQL的执行步骤，其Cost值的计算方式是可以用参数控制的。 通常以抓取顺序页的开销作为基准单位(seq_page_cost取值为1)，以下是不同开销的默认值： seq_page_cost：磁盘顺序读的开销 random_page_cost：磁盘随机读取的开销 cpu_tuple_cost：处理一行数据的开销 cpu_index_tuple_cost：索引扫描每个索引行的开销 cpu_operator_cost：一次查询中执行一个操作符或者函数的开销 gp_motion_cost_per_row：motion操作的开销 effective_cache_size Greenplum优化器会根据pg_class表中的relname、relpages、reltuples的值每种运行方式的cost成本，之后选择cost最小值做为执行方案。 调整经验： 如果内存充足random_page_cost可以适当降低； seq_page_cost和 random_page_cost同时降低时，会使CPU开销上升； Join的广播和重分布Join通常涉及单库关联、以及跨库关联： 单库关联：关联键和分布键一致，此时没有数据重分布 跨库关联：关联键和分布键不一致，数据重新分布，装换为单库关联 表名 字段 分布键 数据量 A id，id2 id M B id，id2 id N 以下是A、B表进行内连接时的场景，左连接和其原理类似（PS：左连接时一般不广播左表）。 遇到全连接时，Greenplum中使用Merge Join方式实现（即排序方式实现Join），全连接通常进行重分布。 123456789101112-- 由于A，B表的分布键均是id，且此关联的关联键也是id，此时A，B中id取值相同的行在同一个pg库中，可以直接关联select * from A,B where A.id=B.id-- 表A的关联键是分布键，但是表B的关联键不是分布键-- 方式一：将表B按照id2字段重分布到每一个节点上 ———— 重分布（处理数据量是N）；-- 方式二：将表A广播到每个节点中 ———— 广播（处理数据量是M*节点数）；select * from A,B where A.id=B.id2-- 表A、B的关联键都不是分布键-- 方式一：将表A和表B都按照id2字段，将数据重分布到每个节点，代价是M+N-- 方式二：将小表广播select * from A,B where A.id1=B.id2 PS：Greenplum判断表的大小是通过统计信息决定的，因此如果统计信息不准确可能会使重分布策略选择错误。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- MyCat","slug":"MyCat-介绍","date":"2019-11-03T16:00:00.000Z","updated":"2020-05-14T07:59:03.313Z","comments":true,"path":"2019/11/04/MyCat-介绍/","link":"","permalink":"https://linqing2017.github.io/2019/11/04/MyCat-%E4%BB%8B%E7%BB%8D/","excerpt":"数据库调研笔记 – MyCat","text":"数据库调研笔记 – MyCat 介绍MyCat基于Ali的开源组件cobar演变而来，是一款知名度较高的SQL中间件。 当前MyCATApache/Mycat-Server项目有7.2K Star，89个贡献者。 当前的稳定版本是MyCat 1.6.5，另外社区还有Mycat2项目，但是当前还没有正式发行版。 目前，MySQL后端支持以下数据库： Mysql、SQL Server、Oracle、 DB2、PostgreSQL、部分NoSQL PS：”MyCat目前在Github上有655个open状态的issue，绝大数issue没有人回应。” 安装第一次安装使用MySQL 8.0作为后端，但是安装完成后发现存在兼容性问题：Mysql 8.0自带的mysql客户端无法正常连接mycat，但是使用低版本的mysql客户端可以正常连接（这个问题Github上有issues，但是没有修复）。 mysql 8.0 安装过程如下： 1234567891011121314151617181920212223242526272829# 安装mysql 8.0wget -i -c https://repo.mysql.com//mysql80-community-release-el7-1.noarch.rpmyum -y install mysql80-community-release-el7-1.noarch.rpmyum -y install mysql-community-server# 安装mysql5.7#wget -i -c https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar#tar -xvf mysql-5.7.28-1.el7.x86_64.rpm-bundle.tar#yum -y localinstall mysql-community-*5.7.28-1*.rpmecho \"lower_case_table_names=1\" &gt;&gt; /etc/my.cnfecho \"default-authentication-plugin=mysql_native_password\" &gt;&gt; /etc/my.cnf # 安装低版本mysql时无需配置systemctl enable mysqld.service &amp;&amp; systemctl start mysqld.servicegrep \"password\" /var/log/mysqld.logmysql -uroot -p# 连接mysql修改默认密码，以及认证加密方式，并创建数据库ALTER USER 'root'@'localhost' IDENTIFIED BY 'Ruijie@123';ALTER USER 'root'@'%' IDENTIFIED BY 'Ruijie@123' PASSWORD EXPIRE NEVER; # 使用低版本mysql时无需执行ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'Ruijie@123'; # 使用低版本mysql时无需执行use mysql;#GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'Ruijie@123'; UPDATE USER SET HOST = '%' where USER = 'root'; flush privileges;CREATE DATABASE mycat_db; 部署mycat 1234567891011121314groupadd -g 550 mycatuseradd -g 550 -u 550 -m -d /home/mycat -s /bin/bash mycatecho \"ruijie\" | passwd --stdin mycatwget http://dl.mycat.io/1.6.7.3/20190927161129/Mycat-server-1.6.7.3-release-20190927161129-linux.tar.gztar -xzvf Mycat-server-1.6.7.3-release-20190927161129-linux.tar.gz -C /usr/localchown -R mycat:mycat /usr/local/mycatecho \"export MYCAT_HOME=/usr/local/mycat\" &gt; /etc/profile.d/mycat.shecho \"export PATH=$PATH:$MYCAT_HOME/bin\" &gt;&gt; /etc/profile.d/mycat.shsource /etc/profile# 如果后端数据库使用的是mysql 8.0需要替换lib目录下的驱动文件 安装过程中，注意schema.xml和server.xml文件： schema.xml定义mycat的逻辑表和逻辑库信息，以及后端mysql信息； server.xml文件定义连接信息和用户信息，安装包中默认提供二个用户 root/123456 和 user/user； 1234567891011121314151617181920212223242526&lt;!-- PS：该配置文件中mycat规划了2个shard，小于默认的三个，需要修改autopartition-long.txt，将最后一行配置删掉--&gt;&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://io.mycat/\"&gt; &lt;schema name=\"TESTDB\" checkSQLschema=\"true\" sqlMaxLimit=\"100\"&gt; &lt;table name=\"travelrecord\" dataNode=\"dn1,dn2\" rule=\"auto-sharding-long\" /&gt; &lt;/schema&gt; &lt;dataNode name=\"dn1\" dataHost=\"mycat71\" database=\"mycat_db\" /&gt; &lt;dataNode name=\"dn2\" dataHost=\"mycat72\" database=\"mycat_db\" /&gt; &lt;dataHost name=\"mycat71\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"mycat71\" url=\"mycat71:3306\" user=\"root\" password=\"Ruijie@123\"&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;dataHost name=\"mycat72\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"mycat72\" url=\"mycat72:3306\" user=\"root\" password=\"Ruijie@123\"&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; 安装完成后执行mycat start启动mycat服务。 配置信息Mycat的配置信息主要位于schema.xml、server.xml、rule.xml三个文件中。 MyCat权威指南第七章详细说明了这三个配置文件。 schema.xml文件1. schema标签该标签用来定义mycat中的逻辑数据库，对于一个mycat实例可以配置多个逻辑数据库。 schema字段包含以下属性： name：逻辑库名称 dataNode：将逻辑库绑定到后端一个具体的数据库，此时没有配置分片的表落在该数据库中 checkSQLschema：将发送到后端DB的schema关键字去除 sqlMaxLimit：自动加上limit语句 2. table标签该标签是schema的子标签，所有需要拆分的表在该标签中定义，形成一张拆分逻辑表。 name：逻辑表名 dataNode：逻辑表分片到的实际数据库，可以定义多个，或者通配符定义 rule：逻辑表遵循的分片规则，规则在rule.xml中定义（tableRule标签中的name属性一一对应） ruleRequired：该属性用于指定表是否绑定分片规则，如果配置为true primaryKey：该逻辑表对应真实表的主键。使用非主键分片时，指定实际主键会使Mycat缓存查询结果，提高性能 type：表属性，包括全局表、普通表两种 autoIncrement：指定这个表有使用自增长主键 subTables：启用分表属性（指对大表宽表进行垂直拆分），分表放在一个数据库中，并且不支持各种条件的join needAddLimit：自动的在每个语句后面加上limit 限制（默认为true） 3.childTable标签childTable用于定义E-R分片的子表 4. dataNode/dataHost标签dataNode 标签定义了MyCat中的数据节点。一个dataNode 标签就是一个独立的数据分片，对应一个后端数据库。 dataHost 标签定义了一个数据库的具体链接方式，读写分离配置、和心跳语句。dataHost 标签比较重要的参数包括： maxCon/minCon：连接池配置 balance ：读操作负载均衡配置，当前支持单点读、读写分离、双主双从、随机分发 writeType ：写操作的服务负载均衡 switchType：主备切换配置 dbType：数据库类型，目前实际上mycat只支持mysql的二进制协议，但是这里可以填写MongoDB、Oracle、Spark等类型的数据库，但本质上都是通过jdbc连接的后端。 dbDriver：驱动类型支持natvie、jdbc两种 heartbeat 标签：心跳语句 writeHost标签、readHost标签 server.xml文件server.xml几乎保存了所有mycat需要的系统配置信息。其在代码内直接的映射类为SystemConfig类。其中最为重要的标签是： user标签：定义mycat的连接用户和权限。 system标签：定义mycat的系统配置，如字符集、sql解析器、线程数据等等 rule.xml文件该文件定义了一些分片策略，在schema.xml文件中指定表分片时可以指定这些分片策略。 123456789101112&lt;tableRule name=\"rule1\"&gt; &lt;!--表规则名称--&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;!--片键名称--&gt; &lt;algorithm&gt;func1&lt;/algorithm&gt; &lt;!--路由算法名称--&gt; &lt;/rule&gt;&lt;/tableRule&gt;&lt;!--分片算法通过java类实现，配置指定在文件中--&gt;&lt;function name=\"hash-int\"class=\"io.mycat.route.function.PartitionByFileMap\"&gt; &lt;property name=\"mapFile\"&gt;partition-hash-int.txt&lt;/property&gt;&lt;/function&gt; 常用分片算法：参考MyCat权威指南第10.5章节！ Mycat的join操作Mycat支持以下几种场景的Join： 全局表全局表在所有节点保存完整副本，具备以下特性： 全局表的插入、更新操作会实时在所有节点上执行，保持各个分片的数据一致性 全局表的查询操作，只从一个节点获取 全局表可以跟任何一个表进行JOIN操作 Mycat通过在MYSQL中额外添加内部列进行全局一致性检查 检查全局表是否存在内部列 检查全局表的记录总数 检查全局表的时间戳最大值 插入全局表时必须带列名插入 E-R JoinE-R Join是在schema.xml文件中预先定义两张的关联key，使分片时关联键相同的数据分布在同一个数据库中，从而保证不会发生跨跨库join。 12345&lt;!-- customer 和 orders 的关联关系为 orders.customer_id=customer.id --&gt;&lt;table name=\"customer\" dataNode=\"dn1,dn2\" rule=\"sharding-by-intfile\"&gt; &lt;childTable name=\"orders\" joinKey=\"customer_id\" parentKey=\"id\"/&gt;&lt;/table&gt; 其他Join方式 Share Join：ShareJoin是一个简单的跨分片Join,基于HBT的方式实现，目前支持两个表join，官方文档称只在开发版中有该功能。 Catlet Join：通过自定义接口实现Join操作。 Spark/Storm对join扩展：还没有实现！！ 全局序列号Mycat中全局序列号主要用来为表提供自增组件，用户可以通过：本地文件、数据库配置、时间戳配置、以及分布式ZK ID生成器。 总结经过几天的测试，对 MyCat 使用总结以下几点： 优势： 使用较为简便，主要因为是，原理简单（就是SQL变换），并且是国产软件有很多中文资料； 预留了很多API进行自定义开发； 纯java实现，有进行源码级别Debug、自定义开发的可能； 劣势： 并非真正意义上的分布式数据库，相比Greenplum、TiDB这种真正的分布式方案架构上有先天缺陷（个人认为，Mycat的发展方向路子走偏了） 有很多功能性的限制，以下是官方权威指南中提到的功能限制（不包括网络上其他人发现的） - 跨库Join 功能太弱，只能支持全局表Join或者片内join（Share join支持2个表的跨库join，但是性能堪忧），考虑以下场景MyCat很难胜任：123456789-- A表：id1 、id2-- B表: 片键id-- C表: 片键id---- E-R join无法同时满足下面二个Join，只能选择将一张表设定成全局表：select * from A , B where A.id1 = B.id;select * from A , B where A.id2 = C.id; - Mycat的全局表的一致性检查机制比较简单，且出现不一致时没有自动恢复机制（个人认为：误操作、掉电等场景都有可能出现不一致，Mycat的一致性检查太low了） - 官方文档提到MyCat不支持复杂子查询，但没有给出具体说明（很迷！！） - 保证高可用需要用户自行设计组网方案：MyCat本身不提供后端数据库的HA能力，也不提供MyCat服务本身的HA功能。每个接入的MySQL都需要进行主从复制的配置，而MyCat本身使用HAProxy避免单点故障。（这一点实际上又引申出如何保障mysql高可用的问题，从IData的应用场景来看还是比较复杂的） - Mycat没有提供备份和恢复方案，只能使用mysql的备份工具备份每一个mysql数据库，并且为了保证数据一致性每一次备份需要业务停止写入； - MyCat使用配置文件定义分片策略，如果分片规则变更数据无法同步更新。换句话说，如果生产中发现之前定义的分片不均衡或者节点扩容，那么需要重写整张表。 - 分布式事务能力较弱，官方文档提到：Mycat 目前没有出来跨分片的事务强一致性支持，目前单库内部可以保证事务的完整性，如果跨库事务，在执行的时候任何分片出错，可以保证所有分片回滚，但是一旦应用发起commit 指令，无法保证所有分片都成功考虑到某个分片挂的可能性不大所以称为弱xa。（看到一篇blog说MyCat基于 Mysql XA 接口实现分布式事务，但是 Mysql XA 接口本身用户不多，因此性能和可用性都可能存在问题！）","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MyCat","slug":"MyCat","permalink":"https://linqing2017.github.io/tags/MyCat/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-分区表介绍","date":"2019-10-24T16:00:00.000Z","updated":"2020-05-14T07:59:03.300Z","comments":true,"path":"2019/10/25/Greenplum-分区表介绍/","link":"","permalink":"https://linqing2017.github.io/2019/10/25/Greenplum-%E5%88%86%E5%8C%BA%E8%A1%A8%E4%BB%8B%E7%BB%8D/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 分区表Greenplum 支持分区表，但是分区的概念需要和分布式区别开，Greenplum中所有表都是分布式的（分布在不同Segment上），但是并非所有表都是分区表。 分区是进行逻辑划分，而分布是表的物理划分，前者减少查询的数据扫描量，后者提供并行查询能力。 分区操作会创建一个顶层（父）表以及一层或者多层子表； 这些父表和子表都可以独立查询，子表看上去和普通表无异； 使用pg_relation_size函数查询父表的存储空间时，大小为0； 父表和子表之间存在继承关系（即表结构、NOT NULL，DEFAULT，CHECK三种约束），修改父表的表结构子表会同步修改； 创建分区后可以用带有PARTITION子句的ALTER TABLE修改父表； 可以在父表插入语句，或者直接插入到子表。直接插入子表时会触发check检查，如果检查失败会返回一个错误； 删除父表时，子表会一并删除 复制表复制表不能进行分区； 多级分区可以基于同一个分区键，如可以基于年、月、日进行三级分区； 已有表不能改为分区表，用户需要创建新的表将原有表的数据导入； 分区表最多能有32,767个分区； 分区表上的主键或者唯一约束必须包含所有的分区列； 分区策略：除非查询优化器能基于查询谓词排除一些分区，否则分区技术不能改进查询性能。 对外部表进行分区时会产生一些限制：参考分区表的限制 分区操作默认分区 默认分区用来装载其他分区CHECK约束失败的行； 优化器在扫描时总会扫描默认分区（因此默认分区会影响分区表性能； 多级分区，一旦存在默认分区，那么每一级都需要保存默认分区； 含有默认分区时，用户可以从默认分区中分裂出新的分区； pg_partitions视图 pg_partition：跟踪分区表以及它们的继承层次关系。 pg_partition_templates：展示使用一个子分区模板创建的子分区。 pg_partition_columns：显示在一个分区设计中用到的分区键列。 分区表操作增加分区： 123456789101112131415161718-- 假设原有分区范围为“2016-01-01 - 2017-02-01”，下面的语句将分区范围拓宽ALTER TABLE sales ADD PARTITION START (date '2017-02-01') INCLUSIVE END (date '2017-03-01') EXCLUSIVE;、ALTER TABLE sales ADD PARTITION START (date '2017-02-01') INCLUSIVE END (date '2017-03-01') EXCLUSIVE ( SUBPARTITION usa VALUES ('usa'), SUBPARTITION asia VALUES ('asia'), SUBPARTITION europe VALUES ('europe') );-- 为sales表的第12分区，添加一个2级分区ALTER TABLE sales ALTER PARTITION FOR (RANK(12)) ADD PARTITION africa VALUES ('africa');-- 添加默认分区ALTER TABLE sales ADD DEFAULT PARTITION other; 重命名分区： 123456-- 分区表使用下列命名习惯：&lt;parentname&gt;_&lt;level&gt;_prt_&lt;partition_name&gt;-- 重命名父表，子表会跟着修改ALTER TABLE sales RENAME TO globalsales;-- 修改表的分区名ALTER TABLE sales RENAME PARTITION FOR ('2016-01-01') TO jan16; 删除分区： 1234-- 删除分区ALTER TABLE sales DROP PARTITION FOR (RANK(1));-- trancate分区ALTER TABLE sales TRUNCATE PARTITION FOR (RANK(1)); 交换分区(参考用外部表交换叶子子分区)： 12345-- 将分区sales_1_prt_1和jan12表交换，此后jan12表成为sales的分区CREATE TABLE jan12 (LIKE sales) WITH (appendoptimized=true);INSERT INTO jan12 SELECT * FROM sales_1_prt_1 ;ALTER TABLE sales EXCHANGE PARTITION FOR (DATE '2012-01-01') WITH TABLE jan12; 分裂分区： 123456789-- 分裂普通分区ALTER TABLE sales SPLIT PARTITION FOR ('2017-01-01') AT ('2017-01-16')INTO (PARTITION jan171to15, PARTITION jan1716to31);-- 分裂默认分区ALTER TABLE sales SPLIT DEFAULT PARTITION START ('2017-01-01') INCLUSIVE END ('2017-02-01') EXCLUSIVE INTO (PARTITION jan17, default partition); 修改分区模板：修改后原有的分区不发生变化，参考修改子分区模板 例子日期范围分区：使用单个date或者timestamp列作为分区键 12345678910111213141516171819202122232425-- 在[START,END)范围内进行分区，每个分区的长度是‘1 day’，因此产生365个分区CREATE TABLE sales (id int, date date, amt decimal(10,2))DISTRIBUTED BY (id)PARTITION BY RANGE (date)( START (date '2016-01-01') INCLUSIVE END (date '2017-01-01') EXCLUSIVE EVERY (INTERVAL '1 day') );-- 指定生成按月的分区，显示指定分区范围CREATE TABLE sales (id int, date date, amt decimal(10,2))DISTRIBUTED BY (id)PARTITION BY RANGE (date)( PARTITION Jan16 START (date '2016-01-01') INCLUSIVE , PARTITION Feb16 START (date '2016-02-01') INCLUSIVE , PARTITION Mar16 START (date '2016-03-01') INCLUSIVE , PARTITION Apr16 START (date '2016-04-01') INCLUSIVE , PARTITION May16 START (date '2016-05-01') INCLUSIVE , PARTITION Jun16 START (date '2016-06-01') INCLUSIVE , PARTITION Jul16 START (date '2016-07-01') INCLUSIVE , PARTITION Aug16 START (date '2016-08-01') INCLUSIVE , PARTITION Sep16 START (date '2016-09-01') INCLUSIVE , PARTITION Oct16 START (date '2016-10-01') INCLUSIVE , PARTITION Nov16 START (date '2016-11-01') INCLUSIVE , PARTITION Dec16 START (date '2016-12-01') INCLUSIVE END (date '2017-01-01') EXCLUSIVE ); 按数字范围分区：表使用单个数字数据类型列作为分区键列。 1234567-- 下面的建表语句会创建11个分区CREATE TABLE rank (id int, rank int, year int, gender char(1), count int)DISTRIBUTED BY (id)PARTITION BY RANGE (year)( START (2006) END (2016) EVERY (1), DEFAULT PARTITION extra ); 定义列表分区表：使用任意允许等值比较的数据类型列作为它的分区键列 12345678-- 创建rank_1_prt_boys、rank_1_prt_girls、rank_1_prt_other三个分区CREATE TABLE rank (id int, rank int, year int, gender char(1), count int ) DISTRIBUTED BY (id)PARTITION BY LIST (gender)( PARTITION girls VALUES ('F'), PARTITION boys VALUES ('M'), DEFAULT PARTITION other ); 多级分区： 1234567891011121314151617181920212223242526272829303132333435363738394041-- 基于时间和值的多级分区CREATE TABLE sales ( trans_id int, date date, amount decimal(9,2), region text) DISTRIBUTED BY (trans_id)PARTITION BY RANGE (date) SUBPARTITION BY LIST (region)SUBPARTITION TEMPLATE ( SUBPARTITION usa VALUES ('usa'), SUBPARTITION asia VALUES ('asia'), SUBPARTITION europe VALUES ('europe'), DEFAULT SUBPARTITION other_regions)( START (date '2011-01-01') INCLUSIVE END (date '2012-01-01') EXCLUSIVE EVERY (INTERVAL '1 month'), DEFAULT PARTITION outlying_dates );-- 三级分区表CREATE TABLE p3_sales (id int, year int, month int, day int, region text)DISTRIBUTED BY (id)PARTITION BY RANGE (year)SUBPARTITION BY RANGE (month)SUBPARTITION TEMPLATE ( START (1) END (13) EVERY (1), DEFAULT SUBPARTITION other_months )SUBPARTITION BY LIST (region)SUBPARTITION TEMPLATE ( SUBPARTITION usa VALUES ('usa'), SUBPARTITION europe VALUES ('europe'), SUBPARTITION asia VALUES ('asia'), DEFAULT SUBPARTITION other_regions )( START (2002) END (2012) EVERY (1), DEFAULT PARTITION outlying_years ); 参考参考文档.","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-TPCDS测试","date":"2019-10-22T16:00:00.000Z","updated":"2020-05-14T07:59:03.299Z","comments":true,"path":"2019/10/23/Greenplum-TPCDS测试/","link":"","permalink":"https://linqing2017.github.io/2019/10/23/Greenplum-TPCDS%E6%B5%8B%E8%AF%95/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 测试工具使用pivotalguru/TPC-DS 工具进行测试，该工具是一个包含：数据生成、执行SQL、输入报告等功能。 安装步骤1234567# 在master节点安装以下工具yum -y install git gcc bc# 切换到gpadmin用户，clone工具的执行脚本git clone https://github.com/pivotalguru/TPC-DS# 编辑gpadmin用户的.bashrc文件，指定Greenplum的环境变量 在gpadmin用户目录中，创建脚本测试脚本tpcds.sh，并执行即可以开始测试： 12345678910111213141516171819202122#!/bin/bashset -eREPO=\"TPC-DS\" ADMIN_USER=\"gpadmin\"INSTALL_DIR=\"/home/gpadmin/pivotalguru\" # 存放TPC-DS工程的目录EXPLAIN_ANALYZE=\"false\"RANDOM_DISTRIBUTION=\"false\" MULTI_USER_COUNT=\"0\" GEN_DATA_SCALE=\"10\" # 1000 约等于生成1T原始数据SINGLE_USER_ITERATIONS=\"1\"RUN_COMPILE_TPCDS=\"false\"RUN_GEN_DATA=\"false\"RUN_INIT=\"true\"RUN_DDL=\"true\"RUN_LOAD=\"true\"RUN_SQL=\"true\"RUN_SINGLE_USER_REPORT=\"true\"RUN_MULTI_USER=\"true\"RUN_MULTI_USER_REPORT=\"true\"RUN_SCORE=\"true\"su --session-command=\"cd \\\"$INSTALL_DIR/$REPO\\\"; ./rollout.sh $GEN_DATA_SCALE $EXPLAIN_ANALYZE $RANDOM_DISTRIBUTION $MULTI_USER_COUNT $RUN_COMPILE_TPCDS $RUN_GEN_DATA $RUN_INIT $RUN_DDL $RUN_LOAD $RUN_SQL $RUN_SINGLE_USER_REPORT $RUN_MULTI_USER $RUN_MULTI_USER_REPORT $RUN_SCORE $SINGLE_USER_ITERATIONS\" $ADMIN_USER pivotalguru/TPC-DS测试工具的入口是rollout.sh，该脚本顺序调用每个子目录下的rollout.sh 1234567891011121314151617181920212223242526272829## - 00_compile_tpcds: tpcds源码脚本会编译该目录的C语言源码，用于生成原始数据#### - 01_gen_data：数据生成脚本、SQL生成脚本## - 工具会在每一个Segment实例安装目录（xxx/pivotalguru）中并行生成原始数据，原始数据的格式为\" XXX_&#123;id&#125;_&#123;num_seg&#125;.dat \"## - 由于SQL脚本中需要处理表的分区信息，因此每次生成数据都会生成相应SQL脚本，生成的SQL被保存到05_sql目录中（sql的模板时TPC-DS本身提供的，位于00_compile_tpcds\\query_templates）#### - 02_init：测试开始前的一些准备工作，包括生成Seg信息、保存配置等## ## - 03_ddl：创建表## - 工具会创建schema：tpcds、ext_tpcds## - 默认根据03_ddl\\distribution.txt文件进行分片，也可以指定RANDOM_DISTRIBUTION#### - 04_load：将外部表导入到Greenplum中## - 工具会为每个SegmentHost启动gpfdist服务，并执行SQL直接将外部表导入## ## - 05_sql：执行单用户测试#### - 06_single_user_reports：生成单用户测试报告## - 测试信息在schemaname是tpcds_reports的表中#### - 07_multi_user：执行多用户测试#### - 08_multi_user_reports：生成多用户测试报告## ## - 09_score：生成测试评分#### - functions.sh：环境准备脚本，由rollout.sh调用#### - rollout.sh：测试入口脚本 参考","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-资源队列","date":"2019-10-16T16:00:00.000Z","updated":"2020-05-14T07:59:03.303Z","comments":true,"path":"2019/10/17/Greenplum-资源队列/","link":"","permalink":"https://linqing2017.github.io/2019/10/17/Greenplum-%E8%B5%84%E6%BA%90%E9%98%9F%E5%88%97/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 资源队列资源队列是Greenplum的默认资源管理方式，包括以下特点： 可以定义多个不同的资源队列，pg_default是默认队列； Role需要和一个队列绑定，不明确绑定时和pg_default绑定； 队列定义了并发数、内存、CPU等资源，ROLE的SQL消耗队列中的资源； 拥有SUPERUSER属性的角色将会不受资源队列的限制，查询立即执行（经过测试：发现并发数限制对SUPERUSER不起作用）； resource_select_only=on时，INSERT、UPDATE、DELETE不会受到资源队列影响； 在执行EXPLAIN ANALYZE命令期间的SQL不受资源队列影响； 资源队列包含以下特性： MEMORY_LIMIT：每个Segment中所有查询所使用的的内存的量； ACTIVE_STATEMENTS：该队列的并发查询限制； PRIORITY：队列的CPU优先级，包括LOW、MEDIUM、HIGH、MAX级别，默认为MEDIUM，级别越高CPU使用越优先； MAX_COST：优化器评估上限，当优化器对SQL的消耗评估大于这个值时，SQL被队列拒绝。 默认队列pg_default的配置为：ACTIVE_STATEMENTS=20、PRIORITY=MEDIUM、没有MEMORY_LIMIT和MAX_COST。 内存管理相关注意： MEMORY_LIMIT不设定时，一个资源队列的可用内存大小是statement_mem*ACTIVE_STATEMENTS（statement_mem指定当前会话分配到内存） 设定MEMORY_LIMIT时，并行度受到当前使用的内存影响； 设定MEMORY_LIMIT时，每个会话分配的内存为MEMORY_LIMIT/ACTIVE_STATEMENTS（不指定statement_mem时）； statement_mem可以覆盖会话的内存分配，取值范围是min(MEMORY_LIMIT, max_statement_mem)，命令为为set statement_mem=’128MB’； 配置文件级别也有statement_mem配置，值为125MB（是否生效？？）； 队列一旦分配出内存，直到查询结束才回收这一部分配额； gp_vmem_protect_limit决定了，单个Segment中所有队列的可用总内存； PRIORITY管理相关注意： SQL按照其资源队列的优先权共享可用的CPU资源; SQL的复杂度不影响CPU的分配; 有新的SQL开始运行时，CPU份额将会被重新计算； 配置资源队列相关配置参数，包括以下： 用于资源队列的一般配置： max_resource_queues max_resource_portals_per_transaction resource_select_only resource_cleanup_gangs_on_wait stats_queue_level 内存利用有关配置： gp_resqueue_memory_policy statement_mem max_statement_mem gp_vmem_protect_limit gp_vmem_idle_resource_timeout（大并发时调整） gp_vmem_protect_segworker_cache_limit （大并发时调整） shared_buffers: 共享内存缓冲区大小，至少为128MB并且至少为16MB以max_connections。 CPU优先级配置： gp_resqueue_memory_policy gp_resqueue_priority_sweeper_interval gp_resqueue_priority_cpucores_per_segment：每个Segment实例分配的CPU核数。Master和Segment的默认值是4，一般需要将HOST的所有CPU都利用上。 SQL命令12345678910111213141516171819-- 创建资源队列CREATE RESOURCE QUEUE adhoc WITH (ACTIVE_STATEMENTS=3);CREATE RESOURCE QUEUE myqueue WITH (ACTIVE_STATEMENTS=20, MEMORY_LIMIT='2000MB');-- 设定优先级ALTER RESOURCE QUEUE adhoc WITH (PRIORITY=LOW);-- 设定并发数 ALTER RESOURCE QUEUE reporting WITH (ACTIVE_STATEMENTS=20);-- 关联队列和roleALTER ROLE name RESOURCE QUEUE queue_name;CREATE ROLE name WITH LOGIN RESOURCE QUEUE queue_name;-- 移除资源队列ALTER ROLE role_name RESOURCE QUEUE none;-- 删除资源队列DROP RESOURCE QUEUE name;-- 查看ROLE绑定的资源队列SELECT rolname, rsqname FROM pg_roles, gp_toolkit.gp_resqueue_status WHERE pg_roles.rolresqueue=gp_toolkit.gp_resqueue_status.queueid; 查看队列中的语句和资源队列状态 gp_toolkit.gp_resqueue_status可以查看队列资源的使用情况； stats_queue_level = on可以收集统计信息和性能，通过pg_stat_resqueues可以查看收集到的信息； gp_toolkit.gp_locks_on_resqueue可以查看等待的SQL； 参看当前活跃或者等待的SQL，如果需要结束这些SQL执行pg_cancel_backend(31905)； 1234567SELECT pg_stat_activity.pid,rolname, rsqname,granted, datname,queryFROM pg_roles, gp_toolkit.gp_resqueue_status, pg_locks, pg_stat_activity WHERE pg_roles.rolresqueue=pg_locks.objid AND pg_locks.objid=gp_toolkit.gp_resqueue_status.queueid AND pg_stat_activity.pid=pg_locks.pid AND pg_stat_activity.usename=pg_roles.rolname; gp_toolkit.gp_resq_priority_statement可以查看SQL优先级，超级用户可以修改某个SQL的优先级（gp_adjust_priority函数） 内存配置对资源队列影响：主要关注：vm.overcommit_ratio、gp_vmem_protect_limit、shared_buffers 主机内存，Segment主机的可用内存，主要由vm.overcommit_ratio配置控制（此处讨论的情况是Segment主机单独部署的情形） 12# 通常配置95即可，若果是资源组模式可以配置50vm.overcommit_ratio = 95 Segment实例内存，每个Segment的可用内存由gp_vmem_protect_limit控制： 123# gp_vmem_rq是GP使用的内存，计算公式为（0.95*RAM - 7.5GB）/1.7# mirror不计入活跃内存gp_vmem_protect_limit = gp_vmem_rq / 最大活跃Segment数目 shared_buffers: 共享内存缓冲区大小，至少为128MB并且至少为16MB以max_connections。 其他知识1. 基线硬件性能gpcheckperf 可以进行： 磁盘I/O测试（dd测试）：默认情况下，在会在磁盘目录下读写2倍内存大小的文件 内存带宽测试（流) ：使用STREAM基准程序来测量可持续的内存带宽（以MB/s为单位），该测试不涉及CPU计算性能。 网络性能测试（gpnetbench*）：当前主机发送5秒钟的数据流到测试中包含的每台远程主机。数据被并行传输到每台远程主机，支持串行（一台一台通信）、并行、全矩阵测试。 测试命令：参考 2. CPU带宽与内存带宽的计算内存带宽： 1内存带宽=内存（等效）频率（内存工作频率X倍频，DDR内存为2，DDR2内存为4，DDR3内存为8）X位宽/8。 CPU的工作频率涉及主频、外频、倍频三个概念 - 主频：CPU自身的工作频率 - 外频：电脑主板提供的系统总线频率，外频是其他组件和CPU通信的基准（lscpu |grep &quot;CPU MHz&quot;，外频可能因为一些其他原因不断变化） - 倍频：主频/外频CPU的带宽一般指：CPU与北桥数据交换的速度，也叫作前端总线FSB。早期，FSB和外频一致的，使用QDR技术后，前端总线的频率成为外频的两倍或者是四倍。 1Intel处理器前端总线（FSB）= 处理器前端总线频率（MHz，处理器外频X4）X位宽（Bit）/8 参考:PC总线带宽与内存带宽的计算","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"Yarn磁盘管理","slug":"Yarn的磁盘管理","date":"2019-10-16T16:00:00.000Z","updated":"2020-05-14T07:59:03.316Z","comments":false,"path":"2019/10/17/Yarn的磁盘管理/","link":"","permalink":"https://linqing2017.github.io/2019/10/17/Yarn%E7%9A%84%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"Yarn磁盘管理Yarn中NodeManager使用的最重要的两个本地目录： yarn.nodemanager.local-dirs：Container的缓存目录 yarn.nodemanager.log-dirs：保存容器日志的本地目录，每个nodemanager都会有这样一个目录，目录格式为：application_${appid}/container_{$contid}，其中包含： stderr stdout prelaunch.err prelaunch.out container-localizer-syslog 通常情况下，Yarn会自动清理这两个目录的日志，涉及到下面的二个参数，日志保留时间为二个参数的时间之和： yarn.nodemanager.delete.debug-delay-sec：App完成后DeletionService会在指定时间后删除本地日志。默认值为0 yarn.nodemanager.log.retain-seconds：保留日志时间，只有当日志聚合关闭时有效。默认值为3小时 Yarn支持HDFS上的日志聚合功能，逻辑为：Application任务运行时日志写在log-dirs，运行完成以后 DeletionService 服务，把日志移动到HDFS上，然后删除本地日志。 日志聚合涉及的配置包括： yarn.log-aggregation-enable：是否开启日志聚合 yarn.nodemanager.remote-app-log-dir：聚合根目录 yarn.nodemanager.remote-app-log-dir-suffix：聚合目录，hdfs上日志保存位置为{yarn.nodemanager.remote-app-log-dir}/${user}/{yarn.nodemanager.remote-app-log-dir-suffix} yarn.log-aggregation.retain-seconds：聚合日志保留时间 yarn.log-aggregation.retain-check-interval-seconds：清理任务运行时间间隔 Yarn支持对local-dirs和log-dirs进行健康检查，相关配置为yarn.nodemanager.disk-health-checker.XXXXXX： min-healthy-disks：log-dir/local-dirs健康目录的最小值，如果低于这个值，nn会被剔除 max-disk-utilization-per-disk-percentage：监控log-dir和local-dirs的使用空间阈值，高于这个值磁盘被标记成不健康。 disk-utilization-watermark-low-per-disk-percentage：bad状态目录恢复为可用的空间水线 min-free-space-per-disk-mb：健康目录的最小剩余空间","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Yarn","slug":"Yarn","permalink":"https://linqing2017.github.io/tags/Yarn/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-导入导出","date":"2019-10-10T16:00:00.000Z","updated":"2020-05-14T07:59:03.301Z","comments":true,"path":"2019/10/11/Greenplum-导入导出/","link":"","permalink":"https://linqing2017.github.io/2019/10/11/Greenplum-%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 数据装载1. INSERT语句INSERT语句只适合在小规模的堆表场景中使用，并且有以下特点： 单个INSERT命令中插入多行，如： 1234INSERT INTO products (product_no, name, price) VALUES(1, 'Cheese', 9.99),(2, 'Bread', 1.99),(3, 'Milk', 2.99); 对于追加优化表， Greenplum数据库支持最多127个并发INSERT 事务插入到一个追加优化表。 性能测试通过pgbench和sysbench进行基于INSERT、UPDATE等简单SQL的性能测试，有以下发现： 测试环境（32核CPU）中插入性能的瓶颈是CPU，50个线程时，单表TPS为15000~16000，单表单线程TPS为500。PS：单块SSD似乎最先到达瓶颈，但是对Greenplum进行扩容性能没有明显提升。 关闭optimizer配置能够极大提升简单SQL的性能，测试中optimizer配置开启时性能只有原来20%，并且巨量消耗Master的CPU资源。PS：这个配置默认开启！ Master节点上每个Client连接会产生多个Postgres进程（似乎和Segment数量有关） 测试中一条INSERT插入多行数据对TPS影响不大 gp_enable_global_deadlock_detector = on 时可以极大的提升UPDATE操作的性能（10倍以上） 关于OLTP的性能测试，Greenplum官方给出了基于Greenplum 6的测试教程，测试在48核的 Master上得到了单表18000 TPS以上的性能。测试结果和实际基本符合。 参考：官方文档、中文社区翻译 2. COPY语句 COPY命令是非并行操作，数据流需要通过Master实例。 COPY只能用于表，不能用于视图。 ，支持常用的文件格式，如：txt、sql、csv、压缩文件、二进制格式等 1234567-- COPY 语句只能在master节点上运行COPY table_name FROM '/path/to/filename' WITH (FORMAT csv);-- \\copy 语句可以在client节点上运行，其基于COPY FROM STDIN语句从STDIN读取输入并发送给Master\\copy table_name FROM '/path/to/filename';-- 使用COPY命令导出数据COPY (SELECT * FROM pgbench_accounts limit 10) TO '/home/gpadmin/pgbench_accounts.csv' WITH csv; 默认情况下，COPY会在第一个错误处停止操作：如果数据包含一个错误，该操作失败并且没有数据被装载(即没有一条数据被导入)。 用户可以使用单行错误隔离模式，Greenplum会跳过包含格式错误的行并且装载正确格式化的行。需要注意的是：这里的错误指数据格式的错误，不包含约束错误。 1234-- 使用单行错误隔离模式，每个segment容许10行错误COPY country FROM '/data/gpdb/country_data' WITH DELIMITER '|' LOG ERRORS SEGMENT REJECT LIMIT 10 ROWS; 性能测试使用COPY命令单线程，导入1亿条数据，数据原始大小9.7GB，用时136692.713 ms，平均731567条/s。 测试过程中，磁盘性能优先达到瓶颈。 调优建议： 在装载数据到新表中时，最后创建索引。 装载完成后执行VACUUM ANALYZE来为查询优化器更新表统计信息 在装载错误后运行VACUUM，清理缓存表。3. 外部表并行读写 通过外部表，可以使向Greenplum导入导出数据并行化，使Greenplum真正具备并行读写的能力。 导入数据： - 准备数据文件 - 创建外部表（初次导入时还要创建数据表） - INSERT INTO tablename SELECT * from tablename_ext_temp; - 删除外部表、外部数据文件外部表根据外部表允许的操作，包括： 可读外部表：仅允许SELECT操作。 可写外部表：仅允许INSERT操作 根据外部表数据源的状态，包括： 普通（基于文件的）：访问静态平面文件 Web（基于Web的）：访问动态数据源 基于命令的web表：建表时将table关联到可执行脚本，每次查询时基于脚本的返回结果，因此该表对应的返回值是动态的，用户可以指定master或者特定segments运行这个脚本。 基于URL的web表：定义表时LOCATION里定义基于http://协议的web服务文件路径，指定的http地址数目取决于GP集群中的Segment数目。 当前支持的外部表协议： 协议类型 表类型 数据存放位置 说明 file 只能是可读表 Segment主机 每个Segment只能处理一个外部文件，所以单个Seg-Host上的文件数目，取决于运行Segment实例数目 gpfdist/gpfdists 可读/写表 gpfdist服务器（一个或者多个） 跨主机协议、支持数据压缩和数据转换 pxf、S3 自定义接口 Hadoop系统、对象存储等 定义外部表： 1234567891011121314-- file 外部表CREATE EXTERNAL TABLE ext_expenses ( name text, date date, amount float4, category text, desc1 text ) LOCATION ('file://host1:5432/data/expense/*.csv', 'file://host2:5432/data/expense/*.csv', 'file://host3:5432/data/expense/*.csv') FORMAT 'CSV' (HEADER); -- 基于命令的web外部表CREATE EXTERNAL WEB TABLE log_output (linenum int, message text) EXECUTE '/var/load_scripts/get_log_data.sh' ON HOST FORMAT 'TEXT' (DELIMITER '|'); gpfdist通常情况下，gpfdist作为一个第三方服务运行在Greenplum集群之外的服务器上（一般是ETL服务器）。Greenplum和gpfdist服务之间通过HTTP/HTTPS协议通信。 12# 启动gpfdist服务，配置数据目录和日志目（建议先安装Greenplum，否则启动可能缺依赖）gpfdist -d /data/data_ssd/gpfdist_files -p 18081 -l /var/log/gpfdist/gpfdist.log &amp; gpfdist的优势： 外部文件支持压缩、CSV等格式 支持将外部XML（json）文件读入Greenplum数据库（通过配置YAML格式的文件） 外部表可以连接一个或多个gpfdist实例（无论一个还是多个，Segment连接外部表时均是并行的） 一台ETL服务器上可以运行多个gpfdist实例（不同的数据目录，以及端口） 性能控制： gp_external_max_segs：数控制能同时访问单一gpfdist实例的Segment实例数量，默认64个； gploadgpload是Greenplum提供的并行导入工具，工作原理基于gpfdist，用户通过定义YAML文件来控制gpload导入的表结构。 支持多种表导入模式 INSERT UPDATE MERGE gpload会在外部文件上重新拉起gpfdist进程，知道导入完成 gpload会创建一张临时外部表，因此执行用户要有建外部表权限，以及写入数据权限 gpload的导入命令为: 1gpload -f insert.yaml -l gpload.log gpload使用的控制文件如下: 123456789101112131415161718192021222324252627---VERSION: 1.0.0.1DATABASE: pgbenchUSER: benchtestHOST: 172.24.9.12PORT: 5432GPLOAD: INPUT: - SOURCE: LOCAL_HOSTNAME: - 172.24.33.35 PORT: 19090 FILE: - /data/data_ssd/gpfdist_files/pgbench/pgbench_accounts/* - COLUMNS: - aid: integer - bid: integer - abalance: integer - filler: character(84) - FORMAT: csv - DELIMITER: ',' - QUOTE: '\"' - HEADER: false OUTPUT: - TABLE: public.pgbench_accounts_gpload_ins - MODE: INSERT 详细参数说明参考官方文档 性能测试使用单点gpfdist服务将外部表导入为GP的系统，导入1亿条数据，数据原始大小9.7GB，用时35962.171 ms，平均2780699条/s。 1234567891011121314151617-- 创建内部堆表CREATE TABLE pgbench_accounts ( aid integer NOT NULL, bid integer, abalance integer, filler character(84)) WITH (fillfactor='100') DISTRIBUTED BY (aid);-- 创建外部表CREATE EXTERNAL TABLE pgbench_accounts_ext_tmp ( aid integer, bid integer, abalance integer, filler character(84) ) LOCATION ('gpfdist://172.24.33.35:18081/pgbench/pgbench_accounts/*') FORMAT 'csv';-- 导入堆表INSERT INTO pgbench_accounts SELECT * from pgbench_accounts_ext_tmp; 使用gpfdist和gpload转换外部数据Greenplum支持将任意格式的数据导入到数据中，或者将数据库中表以任意格式导出，以下说明导入XML到Greenplum的表中。 XML文件内容（文件名为pricerecord.xml）如下，包含：itemnumber和price两个字段。 123456789101112131415&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?&gt;&lt;prices&gt; &lt;pricerecord&gt; &lt;itemnumber&gt;708421&lt;/itemnumber&gt; &lt;price&gt;19.99&lt;/price&gt; &lt;/pricerecord&gt; &lt;pricerecord&gt; &lt;itemnumber&gt;708466&lt;/itemnumber&gt; &lt;price&gt;59.25&lt;/price&gt; &lt;/pricerecord&gt; &lt;pricerecord&gt; &lt;itemnumber&gt;711121&lt;/itemnumber&gt; &lt;price&gt;24.99&lt;/price&gt; &lt;/pricerecord&gt;&lt;/prices&gt; 对应的表结构为： 1234CREATE TABLE prices ( itemnumber integer, price decimal ) DISTRIBUTED BY (itemnumber); 进行导入前，用户需要准备一个脚本工具解析XML文档，该工具不限格式，应当有如下输出： 1234# 需要注意的是：工具输出中不能带空行708421|19.99708466|59.25711121|24.99 用户可以定义config.xml文件，将声明脚本解析工具，该文件中的参数配置文件格式： 123456789101112---VERSION: 1.0.0.1TRANSFORMATIONS: transformation_name1: # 转换名称 TYPE: input # 转换类型，input或者output COMMAND: /bin/sh trans_script.sh %filename% # 转换命令 transformation_name2: TYPE: output COMMAND: /bin/sh trans_script.sh %filename% -- COMMAND中的 %filename% 在执行是被gpload配置文件中的定义替换 创建gpload配置文件如下： 1234567891011121314151617181920212223242526---VERSION: 1.0.0.1DATABASE: pgbenchUSER: benchtestHOST: 172.24.9.12PORT: 5432GPLOAD: INPUT: - TRANSFORM_CONFIG: config.yaml # 定义转换配置 - TRANSFORM: prices_input # 定义要使用的转换 - SOURCE: LOCAL_HOSTNAME: - 172.24.9.12 PORT: 19090 FILE: - pricerecord.xml # 待导入的xml文件（替换%filename% ） - COLUMNS: - itemnumber: integer - price: decimal - FORMAT: TEXT - DELIMITER: '|' - QUOTE: '\"' - HEADER: false OUTPUT: - TABLE: public.prices - MODE: INSERT PS：用户也可以执行 gpfdist -c config.yaml 将装换加载到gpfdist中，创建外表直接读取xml文档 1234CREATE READABLE EXTERNAL TABLE prices_readable (LIKE prices) LOCATION ('gpfdist://hostname:8080/prices.xml#transform=prices_input') FORMAT 'TEXT' (DELIMITER '|') LOG ERRORS SEGMENT REJECT LIMIT 10; 数据导出使用CREATE WRITABLE EXTERNAL TABLE命令定义外部表时，可以将数据导出到本地。 Segment把数据发送到gpfdist，后者会把数据写到指定的文件中； 外部表定义中定义多个gpfdist URI时，输出数据划分到多个文件之间； 可写的外部Web表把输出行发送到一个脚本（或者应用）作为输入。 123456789101112131415161718192021-- 写入到gpfdist的外部表CREATE WRITABLE EXTERNAL TABLE unload_expenses ( LIKE expenses ) LOCATION ('gpfdist://etlhost-1:8081/expenses1.out', 'gpfdist://etlhost-2:8081/expenses2.out')FORMAT 'TEXT' (DELIMITER ',') DISTRIBUTED BY (exp_id);-- 写入到HDFS的外部表CREATE WRITABLE EXTERNAL TABLE unload_expenses ( LIKE expenses ) LOCATION ('pxf://dir/path?PROFILE=hdfs:text') FORMAT 'TEXT' (DELIMITER ',') DISTRIBUTED BY (exp_id);-- 可写外部web表，EXECUTE程序处理insert到这个表的每一行-- 参考https://greenplum.cn/gp6/load/topics/g-defining-a-command-based-writable-external-web-table.htmlCREATE WRITABLE EXTERNAL WEB TABLE output (output text) EXECUTE 'export PATH=$PATH:/home/gpadmin/programs; myprogram.sh' FORMAT 'TEXT' DISTRIBUTED RANDOMLY-- 向外部表写入GRANT INSERT ON writable_ext_table TO admin;INSERT INTO writable_ext_table SELECT * FROM regular_table; 直接用COPY命令也可以导出，但是此时性能瓶颈受限于Master服务： 12COPY (SELECT * FROM country WHERE country_name LIKE 'A%') TO '/home/gpadmin/a_list_countries.out'; PGBench测试工具pgbench是 PostgreSQL 上自带一个基准测试工具，能够让用户并发执行多次SQL语句，并且统计测试的TPS。 默认情况下，如果用户不提供自定义的测试SQL，那么测试时使用TPC-B方式进行OLTP测试，执行 123456789101112131415161718-- 初始化测试数据库，比例因子-s（系数是10万），执行后pgbench中包括以下几张表：---- table # of rows-- ----------------------------------- pgbench_branches 1 * 1000-- pgbench_tellers 10 * 1000-- pgbench_accounts 100000 * 1000-- pgbench_history 0 * 1000pgbench -i -s 1000 pgbench-- 重要的测试选项包括-c（客户端数量）、 -t（事务数量）、-T（时间限制）以及-f（指定一个自定义脚本文件）-- pgbench 默认有三个内建脚本，分别是：tpcb-like、simple-update、select-onlypgbench -c 100 -j 100 -r -T 60 -P 1 -s 1000 -b tpcb-like pgbenchpgbench -c 100 -j 100 -r -T 60 -P 1 -s 1000 -b simple-update pgbenchpgbench -c 100 -j 100 -r -T 60 -P 1 -s 1000 -b select-only pgbench 以下是自定义测试脚本： 12345678910\\set scale 10000\\set nbranches 1 * :scale\\set ntellers 10 * :scale\\set naccounts 100000 * :scale\\set aid random(1,:naccounts)\\set bid random(1,:nbranches)\\set tid random(1,:ntellers)\\set delta random(-5000,5000)INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); 附：测试环境服务器：6 * 3 Segment（Master和Segment混合部署，并且共用一块SSD，并且配置mirror）CPU：Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz 32核内存： 125G网络：10GB光纤 附：创建外部表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103-- 普通可读外部表CREATE [READABLE] EXTERNAL TABLE table_name ( column_name data_type [, ...] | LIKE other_table ) LOCATION ('file://seghost[:port]/path/file' [, ...]) | ('gpfdist://filehost[:port]/file_pattern[#transform]' | ('gpfdists://filehost[:port]/file_pattern[#transform]' [, ...]) | ('gphdfs://hdfs_host[:port]/path/file') FORMAT 'TEXT' [( [HEADER] [DELIMITER [AS] 'delimiter' | 'OFF'] [NULL [AS] 'null string'] [ESCAPE [AS] 'escape' | 'OFF'] [NEWLINE [ AS ] 'LF' | 'CR' | 'CRLF'] [FILL MISSING FIELDS] )] | 'CSV' [( [HEADER] [QUOTE [AS] 'quote'] [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [FORCE NOT NULL column [, ...]] [ESCAPE [AS] 'escape'] [NEWLINE [ AS ] 'LF' | 'CR' | 'CRLF'] [FILL MISSING FIELDS] )] | 'AVRO' | 'PARQUET' | 'CUSTOM' (Formatter=&lt;formatter specifications&gt;) [ ENCODING 'encoding' ] [ [LOG ERRORS [INTO error_table]] SEGMENT REJECT LIMIT count [ROWS | PERCENT] ]-- web可读外部表，每次读的数据动态变化CREATE [READABLE] EXTERNAL WEB TABLE table_name ( column_name data_type [, ...] | LIKE other_table ) LOCATION ('http://webhost[:port]/path/file' [, ...]) | EXECUTE 'command' [ON ALL | MASTER | number_of_segments | HOST ['segment_hostname'] | SEGMENT segment_id ] FORMAT 'TEXT' [( [HEADER] [DELIMITER [AS] 'delimiter' | 'OFF'] [NULL [AS] 'null string'] [ESCAPE [AS] 'escape' | 'OFF'] [NEWLINE [ AS ] 'LF' | 'CR' | 'CRLF'] [FILL MISSING FIELDS] )] | 'CSV' [( [HEADER] [QUOTE [AS] 'quote'] [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [FORCE NOT NULL column [, ...]] [ESCAPE [AS] 'escape'] [NEWLINE [ AS ] 'LF' | 'CR' | 'CRLF'] [FILL MISSING FIELDS] )] | 'CUSTOM' (Formatter=&lt;formatter specifications&gt;) [ ENCODING 'encoding' ] [ [LOG ERRORS [INTO error_table]] SEGMENT REJECT LIMIT count [ROWS | PERCENT] ] -- 普通可写外部表CREATE WRITABLE EXTERNAL TABLE table_name ( column_name data_type [, ...] | LIKE other_table ) LOCATION('gpfdist://outputhost[:port]/filename[#transform]' | ('gpfdists://outputhost[:port]/file_pattern[#transform]' [, ...]) | ('gphdfs://hdfs_host[:port]/path') FORMAT 'TEXT' [( [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [ESCAPE [AS] 'escape' | 'OFF'] )] | 'CSV' [([QUOTE [AS] 'quote'] [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [FORCE QUOTE column [, ...]] ] [ESCAPE [AS] 'escape'] )] | 'AVRO' | 'PARQUET' | 'CUSTOM' (Formatter=&lt;formatter specifications&gt;) [ ENCODING 'write_encoding' ] [ DISTRIBUTED BY (column, [ ... ] ) | DISTRIBUTED RANDOMLY ] -- web可写外部表CREATE WRITABLE EXTERNAL WEB TABLE table_name ( column_name data_type [, ...] | LIKE other_table ) EXECUTE 'command' [ON ALL] FORMAT 'TEXT' [( [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [ESCAPE [AS] 'escape' | 'OFF'] )] | 'CSV' [([QUOTE [AS] 'quote'] [DELIMITER [AS] 'delimiter'] [NULL [AS] 'null string'] [FORCE QUOTE column [, ...]] ] [ESCAPE [AS] 'escape'] )] | 'CUSTOM' (Formatter=&lt;formatter specifications&gt;) [ ENCODING 'write_encoding' ] [ DISTRIBUTED BY (column, [ ... ] ) | DISTRIBUTED RANDOMLY ] 参考GP系统配置参数 XML转换示例","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- GreenPlum","slug":"Greenplum-介绍&部署","date":"2019-10-07T16:00:00.000Z","updated":"2020-05-14T07:59:03.300Z","comments":true,"path":"2019/10/08/Greenplum-介绍&部署/","link":"","permalink":"https://linqing2017.github.io/2019/10/08/Greenplum-%E4%BB%8B%E7%BB%8D&%E9%83%A8%E7%BD%B2/","excerpt":"GreenPlum 调研笔记","text":"GreenPlum 调研笔记 1. 概述GreenPlum中文社区的介绍中，将Greenplum定位成开源大数据平台，而不仅仅是一个MPP数据查询引擎。 Greenplum的优势： 处理和分析各种数据源的数据的平台：包括hadoop、Hive、HBase、S3等等，支持结构化、半结构化、非结构化数据 数据水平分布、并行查询执行、专业优化器、线性扩展能力、多态存储、资源管理、高可用、高速数据加载 接口可扩展，支持SQL、JDBC和ODBC等行业标准 集成数据分析平台：MADlib (Github 245个Star，半死不活) 在金融、保险、证券等领域有众多应用案例，具备较为完善的生态 采用 ** Apache 2 协议 ** 架构 Greenplum基本架构包括：Master、SegmentHost。 Master：Greenplum数据系统的入口，Client连接Master提供SQL。Master管理了全局系统目录，包含了有关Greenplum数据库本身的元数据（系统表）。Master的主备基于WAL预写式日志来实现主/备镜像。 SegmentHosts：基于Postgresql 8.3的定制数据库，负责存储和处理用户数据。 一台Segment主机通常运行2至8个Greenplum的Segment。 Interconect：Interconect是Greenplum数据库架构中的网络层，默认协议UDPIFC，如果使用TCP协议，那么Greenplum限制1000个Segment。 事务控制Greenplum支持事务控制，当并发更新时Greenplum通过MVCC模型来保证事务数据一致性。 MVCC模型基于快照机制是Postgresql中的一个特性，能够管理数据行的多个版本。 MVCC数据库ACID特性的描述： Atomicity：事务的操作结果要么全部执行要么全部不执行 Consistency：总是从一个一致的状态转换到另一个一致的状态 Durability：事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响。 Isolation：事务的修改结果在什么时间能够被其他事务看到（SQL1992规范），隔离级别包括以下四个： 未提交读：事务能够看到其他事务没有提交的修改，当另一个事务又回滚了修改后导致读取到脏数据，这种情况又称为 脏读 已提交读：事务能够看到其他事务提交后的修改，这时会出现一个事务内两次读取数据不一致，这种模式下事务是不可重复读的。 可重复读:在两次读取时读取到的同一行数据是一致的，但是两次查询可能查到行数不一致（其他事务出现新的插入），这种情况称为幻读。 序列化级别：不允许出现幻读、脏读、不可重复读。 Greenplum中未提交读、已提交读隔离模式的效果和标准SQL一致；可重复读模式避免了不可重复读和幻读；Greenplum数据库并不完全支持可串行化模式（该模式时自动退化到可重复读模式），并且数据操作并非真正串行化的。 上述特性中，Isolation是关键，不同数据库实现了不同级别的隔离性，并且通常情况下使用锁来解决这些问题。传统方案采用读写锁（读锁和读锁之间不互斥，写锁互斥其他所有锁），MVCC是一种完全使读写操作并发的方案（完全抛弃锁）。 用户执行事务时可以在SQL中指定，事务隔离级别： 1234BEGIN;SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;...COMMIT; MVCC的实现： 在PostgreSQL中，每一个事务(包括单条SQL)都会得到一个被称作为 XID 的事务ID。Session请求事务操作时，PostgreSQL递增XID并赋给这个事务。 每一行记录都存储了事务相关信息，这些信息用于判断当前事务是否可见。 xmin：在创建记录时，记录此时的事务id，后面每次update也会更新。 xmax：在更新或删除或lock时，记录此时的事务id；如果记录没有被删除，那么此时为0。 cmin：多语句事务存储创建这个元组的Command ID cmax：多语句事务删除这个元组的Command ID 一个事务会看到 xid &lt; xmin 的行（这些行已经commit），并且这些行 xid &gt; xmax （这些行已经被删除）。 cmin/cmax：用于多语句事务中，只在事务期间有意义，事务开始时该序列被重置为0。 每一个Segment数据库都有其自己的XID序列，Master会使用一个分布式事务ID，称为gp_session_id，Segment会会维护一个分布式事务ID到其本地XID的映射。 当一个Segment上的事务失败，会回滚所有Segment的修改。 一行支持二十亿个事务，这之后这一行将成为一个新行，通过一次VACUUM操作可以避免这样的情况。可以配置xid_warn_limit和 xid_stop_limit控制事务上限告警。 MVCC的实现存储了多个数据版本，非常容易造成表膨胀。VACUUM命令会标记过期行所使用的空间可以被重用。通常可以使用以下策略运行VACUUM命令： 重度更新的表，可能每天需要运行几次VACUUM。 运行了一个更新或者删除大量行的事务之后运行VACUUM。 VACUUM FULL命令会把表重写为没有过期行，并且将表减小到其最小尺寸，同时该操作会锁表。 运行VACUUM VERBOSE tablename来得到一份Segment上已移除的死亡行数量、受影响页面数以及有可用空闲空间页面数的报告。 用户可以使用LOCK LOCK命令显示加锁（[参考]（https://gp-docs-cn.github.io/docs/ref_guide/sql_commands/LOCK.html））。 数据冗余和故障切换部署Greenplum数据库系统时，Segment可以配置Mirror实例，当Primary节点宕机时，Mirror节点提供服务，如果系统中存在Segment没有配置Mirror，那么Segment会成为整个系统的单点故障。 Greenplum数据库中Segment镜像拓扑： 用户可以选择一台不同于Master节点的主机上部署一个Master实例的备份或者镜像。 后备Master利用事务日志复制进程保持与主Master同步，复制进程运行在后备Master上并且负责在主备Master主机之间同步数据。如果主Master失效，日志复制进程会停止，并且后备Master会被激活以替代它的位置。Master失效时，主备切换不会自动发生，需要外部激励触发。 2. 安装部署官方Github上提供了从源码编译gpdb和gporca的完整步骤，同时也提供了预编译好的RPM和DEB包，以下安装不步骤参考网络上的一些文档，并非官方推荐的安装步骤（我没找到！！）。 准备工作 所有节点配置NTP服务 更新以下系统配置 123456789101112131415161718192021222324252627282930313233sudo bash -c &apos;cat &gt;&gt; /etc/sysctl.conf &lt;&lt;-EOFkernel.shmmax = 500000000kernel.shmmni = 4096kernel.shmall = 4000000000kernel.sem = 500 1024000 200 4096kernel.sysrq = 1kernel.core_uses_pid = 1kernel.msgmnb = 65536kernel.msgmax = 65536kernel.msgmni = 2048net.ipv4.tcp_syncookies = 1net.ipv4.ip_forward = 0net.ipv4.conf.default.accept_source_route = 0net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_max_syn_backlog = 4096net.ipv4.conf.all.arp_filter = 1net.ipv4.ip_local_port_range = 1025 65535net.core.netdev_max_backlog = 10000net.core.rmem_max = 2097152net.core.wmem_max = 2097152vm.overcommit_memory = 2EOF&apos;sudo bash -c &apos;cat &gt;&gt; /etc/security/limits.conf &lt;&lt;-EOF* soft nofile 65536* hard nofile 65536* soft nproc 131072* hard nproc 131072EOF&apos;sysctl -p 在每个节点中创建gpadmin用户用于管理Greenplum，并且打通该节点的集群免密 1234567groupadd -g 530 gpadminuseradd -g 530 -u 530 -m -d /home/gpadmin -s /bin/bash gpadminecho \"ruijie\" | passwd --stdin gpadminsu - gpadmin# 只需要打通第一个节点到其他节点的ssh，之后执行 gpssh-exkeys -f hostlists 打通所有节点之间的互信ssh-copy-id gpadmin@node11 &amp;&amp; ssh-copy-id gpadmin@node12 &amp;&amp; ssh-copy-id gpadmin@node13 安装Greenplum DB下载RPM包，执行以下命令： 1yum localinstall -y greenplum-db-6.0.0-rhel7-x86_64.rpm 安装完成后，安装目录为/usr/local/greenplum-db，同时还要将greenplum-db中的lib添加到ld.so.conf中： 12345sudo bash -c 'cat &gt;&gt; /etc/ld.so.conf.d/greenplum.conf &lt;&lt;-EOF/usr/local/greenplum-db/libEOF'ldconfig 在/home/gpadmin目录下配置环境变量、修改目录权限、创建配置目录 1234567891011121314151617## /usr/local/greenplum-db/greenplum_path.sh 添加以下内容source /usr/local/greenplum-db/greenplum_path.shexport MASTER_DATA_DIRECTORY=/data/data_ssd/greenplum/data/master/gpseg-1export PGPORT=5432export PGDATABASE=gp_sydb# 修改bin文件和数据目录权限：chown -R gpadmin:gpadmin /usr/local/greenplum-db/mkdir -p /data/data_ssd/greenplum # 数据目录chown -R gpadmin:gpadmin /data/data_ssd/greenplum# 创建配置目录su - gpadminmkdir -p /home/gpadmin/conftouch /home/gpadmin/conf/all_hosts #集群all_hosts文件,包含所有节点touch /home/gpadmin/conf/seg_hosts #集群seg_hosts文件,包含所有segment节点 初始化数据库登录master的gpadmin用户，验证免密是否成功： 12345678910# 验证免密是否可用gpssh-exkeys -f /home/gpadmin/conf/all_hosts # gpssh命令提供类似ansible的公众gpssh -f /home/gpadmin/conf/all_hosts# 所有节点创建数据目录mkdir -p /data/data_ssd/greenplum/data/master mkdir -p /data/data_ssd/greenplum/data/primarymkdir -p /data/data_ssd/greenplum/data/mirror 配置文件模板位于/usr/local/greenplum-db/docs/cli_help/gpconfigs目录中，参考gpinitsystem_config创建配置文件/home/gpadmin/conf/gpinitsystem_config 12345678910111213141516RRAY_NAME=&quot;Greenplum Data Platform&quot;SEG_PREFIX=gpsegPORT_BASE=6000declare -a DATA_DIRECTORY=(/data/data_ssd/greenplum/data/primary /data/data_ssd/greenplum/data/primary /data/data_ssd/greenplum/data/primary)MASTER_HOSTNAME=node11MASTER_DIRECTORY=/data/data_ssd/greenplum/data/master MASTER_PORT=5432TRUSTED_SHELL=sshCHECK_POINT_SEGMENTS=8ENCODING=UNICODEDATABASE_NAME=gp_sydbMACHINE_LIST_FILE=/home/gpadmin/conf/seg_hosts# 需要配置冗余时# MIRROR_PORT_BASE=7000# declare -a MIRROR_DATA_DIRECTORY=(/data/data_ssd/greenplum/data/mirror /data/data_ssd/greenplum/data/mirror /data/data_ssd/greenplum/data/mirror) 执行gpinitsystem -c /home/gpadmin/conf/gpinitsystem_config 初始化数据库。 如果需要使用冗余配置，则执行 1gpinitsystem -c gpinitsystem_config -h hostfile_exkeys -s &#123;master备份节点&#125; -S &#123;master备份目录&#125; 配置文件初始化完成后，master节点的 MASTER_DIRECTORY 目录下会自动生成gpseg-1目录，该目录中的文件类似pg的配置文件，包含：postgresql.conf、pg_hba.conf等内容。 初始化成功后，Greenplum会自动创建管理员用户（默认情况下为执行初始化化程序的用户）。 初次启动时，用户需要使用管理员用户登录，并创建Client使用的账号以及修改账号登录方式（pg_hba.conf）。 基本操作1234567891011121314151617# 下面的所有操作在Master节点上运行，且使用gpadmin用户# 启动gpstart -a# 关闭，-M fast表示关闭所有事务，并且回滚gpstop -M fast# 重启服务gpstop -r# 重载 pg_hba.conf 和 postgresql.conf，部分参数需要通过完全重启才能生效gpstop -u# 部分情况下，客户端进程会出现卡死，gp集群无法关闭，此时需要具有SUPERUSER权限的Greenplum用户登录postgres，杀死客户端进程。操作过程，参考：https://greenplum.cn/gp6/managing/startstop.html# SELECT usename, pid, waiting, query, datname FROM pg_stat_activity;# 上面的sql可以查出当前GP的活跃client，使用pg_cancel_backend(pid)、pg_terminate_backend(pid)可以强制退出这些线程。# 查看服务的状态gpstate -s 配置文件Greenplum集群中包括：master参数和本地参数，这些参数存储每个实例的postgresql.conf、pg_hba.conf文件中。 master参数： 系统范围参数：编辑$MASTER_DATA_DIRECTORY/postgresql.conf文件 数据库级别参数：使用ALTER DATABASE xxx SET xxx TO xxx修改 Role级别参数：使用ALTER ROLE xxx SET xxx TO xxx;修改 会话基本参数：在会话中使用SET XXX TO XXX修改 本地参数：本地参数保存在每一个postgresql.conf文件（包括：primary和mirror）中，要更新参数，可以使用 gpconfig 命令（如，gpconfig -c xxx -v xxx），也可以使用这个命令查看Seg的参数（如，gpconfig –show xxx）。 关于参数配置说明，可以参考服务器配置参数。 日志文件以下方式可以查看GP集群的日志文件： 每个Master和Segment实例都在其数据目录的 pg_log中有它们自己的日志文件。 Master的日志文件包含了大部分信息，应该总是首先检查它。 gplogfilter工具可以用来检查Greenplum数据库日志文件。 如果要检查segment日志文件，使用gpssh在segment主机上执行gplogfilter工具。默认查找$MASTER_DATA_DIRECTORY 目录下的日志文件，用户也可以手工指定。 3. 高可用高可用方案 硬件级别RAID：在磁盘级别实现数据冗余 数据存储总和校验：该机制是默认开启的，数据被写入磁盘时会计算校验和，下一次读取时检查校验和，从而达到防止磁盘上数据损坏的目的（涉及的配置项包括：ignore_checksum_failure和HEAP_CHECKSUM）。 Segment镜像 Master镜像 双集群：双ETL方案、”备份/恢复”方案 备份和恢复：gpbackup/gprestore工具备份/恢复Greenplum数据库，参考 配置Segment镜像默认情况下，在GP集群运行时执行gpaddmirrors -p 10000，就能在本集群内创建Segment镜像，执行期间会提示输入mirror数据的存储位置。10000表示mirror服务的端口号在原primary基础上加上10000。 上述命令，以group方式创建mirror，如果用户需要mirror分散部分或者分布在另外的HOST上，那么需要定义文件指定mirror到primary的映射（参考）。 gp_segment_configuration表记录了所有primary和mirror的状态，以及连接信息，这张表常用用于判断mirror的状态。这张表中mode字段描述了Seg的三种状态： Change Tracking Mode ：没有找到mirror实例 resync：重新同步 in-sync：同步完成 当mirror因为一些原因同步失败时，可以使用 gprecoverseg 进行一次增量同步，或者使用 gprecoverseg -F 进行全量同步。 gp_segment_configuration表的role和preferred_role表示 Segment 表示当前的状态，以及偏好状态。当它们不匹配时，就可能有每台硬件主机上活动主Segment数量造成的倾斜。为了重新平衡该集群并且让所有的Segment回到它们的首选角色，可以用-r选项运行gprecoverseg命令。 当Segment故障时，有以下恢复手段： 在Master节点执行gprecoverseg，将下线Segment重新上线。gprecoverseg会恢复数据文件，此时数据库的写活动被禁止。 当所有Segment状态为Synchronized时，可以运行gprecoverseg -r使Segment回到它们的首选角色。 gprecoverseg -F是全量恢复手段：从活动segment实例（当前主实例）复制数据前， 删除离线segment实例的数据目录。 gprecoverseg -i recover_config_file将失效Segment恢复到其他主机，参考 配置Master镜像当GP集群正在运行时，通过gpinitstandby -s {standby_host}能够快速启动一个Master的镜像。 通过gpstate -f，可以检查Master Mirror的运行状态，正常情况下：standby master的状态应该是passive，WAL sender状态应该是streaming。 需要注意：StandbyMaster不能提供任何服务！ 主master故障时，需要手工执行gpactivatestandby（如，gpactivatestandby -d /data/master/gpseg-1）来激活后备Master。激活Master主机后，可以执行psql dbname -c ‘ANALYZE;’。 关于Master恢复的一些问题： 激活后备Master后，官方建议一直将该Master作为主Master使用，并且初始化一个新的后备Master 要恢复原来的主Master遵循下面的步骤（下面将原Master主机成为MHost，当前Master主机称为SMHost）： 备份 MHost 上的gpseg-1 在 SMHost 上运行：gpinitstandby -s MHost 检查 MHost 上后备Master的状态：gpstate -f（standby master 状态应该是passive，WAL sender状态应该是streaming） 停止 SMHost 上的Master：gpstop -m （即把当前的主master停掉） 在MHost上运行：gpactivatestandby -d $MASTER_DATA_DIRECTORY（即将当前备升级为主Master） 移除 SMHost 上的gpinitstandby，并在MHost上执行：gpinitstandby -s SMHOST 4. 数据备份和恢复数据备份和恢复有以下两种方式： 并行：每台Segment主机都同时把其数据写入到本地的磁盘存储上 非并行：数据必须通过网络从Segment被发送到Master，后者把所有的数据写入它的存储中。 推荐使用并行方式，非并行方式时间上是将GP集群当做一个pg来执行任务。 gpbackup和gprestoregpbackup和gprestore在github上是一个独立项目，不属于gpdb工程，其release是两个可执行文件，下载后放到gpadmin用户目录下就可以使用。 gpbackup 和 gprestore 支持以下功能： 并行备份恢复 全量备份、增量备份 备份整个数据库，或者 数据库特定scheme和表 gpbackup将元数据和数据分开成不同文件可读文件，这些文件放在各个节点上 1234# 基本备份操作./gpbackup --dbname benchtest --backup-dir /home/gpadmin/backups# 基本恢复操作./gprestore --backup-dir /home/gpadmin/backups/ --timestamp 20191010111727 --create-db --jobs 8 其他关于GP集群备份的内容： 增量备份 在特定存储设备上备份 自定义存储插件 5. 扩容对GP集群进行扩容需要注意的几点： 当Segment使用Mirror时，一次扩容最少需要两台机器（如果不使用Mirror则没有这种要求）； 括容之后Segment需要对表进行重平衡： 重平操作是一次数据重写，会极大消耗磁盘IO，以及占用磁盘空间 表在重平衡期间不可用 用户可以控制表的重平衡顺序 重平衡不影响新创建的表 扩容之前的数据备份文件不可用，需要重新备份 扩容步骤： 准备节点： 配置系统变量，必要时候进行性能测试 安装Greenplum软件 创建gpadmin用户 配置SSH免密 初始化新节点：这个步骤将新的节点添加到GP集群中 创建扩容文件，这个文件可以手工编辑，也可以通过 gpexpand 命令生成（如何生成该文件参考）。 运行gpexpand -i input_file，将扩容实例上线，如果上述过程失败可以执行gpexpand –rollback回滚。 重分布表：此步骤一旦开始，那么需要重平衡的表变得不再可读写 执行gpexpand -d 60:00:00可以开始表扩容操作，-d表示重分布的最大时间限制 进行重分布时，通过gpexpand.status、gpexpand.expansion_progress、gpexpand.status_detail表可以查看重分布表的状态，调整gpexpand.status_detail的rank值还可以控制重分布表的顺序。 移除扩容操作 gpexpand -c 6.数据对象数据库Greenplum中数据库有模板的概念，用户可以从模板创建数据库，新的数据库会拥有模板的所有表和数据。 默认模板包括：template1所有新建库的默认模板、template0系统数据库（如postgres）的模板 从模板克隆数据库：CREATE DATABASE new_dbname TEMPLATE old_dbname; 本质上模板和数据库等价，任何数据库都能当做模板 查看当前数据库：\\l、查看pg_database表 表空间表空间用于将数据库中的对象(如表、索引等)到不同的存储介质上，不同表空间的区别在于存储介质不同。 12345678-- 创建一个表空间，其存放目录要事先创建，并且所有Seg都能访问CREATE TABLESPACE fastspace LOCATION '/fastdisk/gpdb';-- 为Role赋权，使他能够在表空间上创建对象GRANT CREATE ON TABLESPACE fastspace TO admin;-- 创建表时指定表空间CREATE TABLE foo(i int) TABLESPACE fastspace;-- 指定默认表空间SET default_tablespace = fastspace; 查询pg_tablespace可以得到当前环境的所有表空间，Greenplum创建之后包含默认表空间： pg_default ：默认表空间。由template1和template0数据库使用，存储位置为$PADATA/base/。 pg_global ：用于共享系统的catalogs，存储位置为$PADATA/global/。 Greenplum中的表空间和PG是一致的，可以参考这个文章。 SCHEMASchema从逻辑上组织一个数据库中的对象和数据。 Schema允许用户在同一个数据库中拥有多于一个对象（例如表）具有相同的名称而不发生冲突，只要把它们放在不同的Schema中就好，Public是默认SCHEMA。 用户可以设置search_path配置参数来指定在其中搜索对象的可用schema的顺序。 在该搜索路径中第一个列出的方案会成为默认schema。 如果没有指定方案，对象会被创建在默认schema中。 1234567891011-- 指定查找特定schema下的表SELECT * FROM myschema.mytable;-- 设定数据库的搜索顺序ALTER DATABASE mydatabase SET search_path TO myschema, public, pg_catalog;-- 查看搜索路径、以及schemaSHOW search_path;SELECT current_schema(); 表表分布策略支持三种分布策略： DISTRIBUTED BY（哈希分布） DISTRIBUTED RANDOMLY（随机分布） DISTRIBUTED REPLICATED（全分布） 关于分布策略有以下几点需要注意： 不显示指定分布策略时，表如何分布取决于gp_create_table_random_default_distribution 使用随机分布时，不能在表中指定PRIMARY KEY 或者 UNIQUE 列 对于DISTRIBUTED BY可以自定义操作符 表存储模型 堆存储： 默认配置，模型和PostgreSQL相同 堆表存储在OLTP类型负载下表现最好，适合频繁修改的的小表 行级存储方式 追加优化存储：指定appendoptimized=true 成批地被载入并且被只读查询访问的事实表； 不推荐单行的INSERT语句 更新表时有功能限制（如事务中不支持UPDATE和DELETE等） 选择面向行或者面向列的存储：列表(appendoptimized=true, orientation=column) 支持行，列或两者的组合 面向列的表存储只能用于追加优化表 频繁的插入时，行表优于列表 压缩表： 指定(appendoptimized=true, compresstype=zlib, compresslevel=5); 只适用于追加优化表 可以进行整个表的压缩、或者指定列的压缩 参考介绍：选择表存储模型 其他对象 序列：Greenplum数据库序列对象是一个特殊的单行表，用作数字生成器。 参考 索引：参考 视图:参考 参考GreenPlum中文社区 PGSQL MVCC机制 Greenplum 表统计信息 PgBouncer客户端配置 Greenplum数据库参考指南 Greenplum系统表表结构 gprestore和gpbackup介绍","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://linqing2017.github.io/tags/GreenPlum/"}]},{"title":"数据库调研笔记 -- Sysbench","slug":"sysbench","date":"2019-09-29T16:00:00.000Z","updated":"2020-05-14T07:59:03.315Z","comments":true,"path":"2019/09/30/sysbench/","link":"","permalink":"https://linqing2017.github.io/2019/09/30/sysbench/","excerpt":"Sysbench 性能测试工具","text":"Sysbench 性能测试工具 SysbenchSysbench是基于LuaJIT的可编写脚本的多线程基准测试工具，提供系统软硬件层面的性能基准测试，包括; 数据库基准测试 文件系统基准测试 CPU性能基准测试 内存性能基准测试 线程调度基准测试 POSIX信号量基准测试 安装部署12curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bashsudo yum -y install sysbench 性能测试Postgresql测试命令： 123456789sysbench oltp_insert.lua --time=60 --percentile=99 --pgsql-host=172.24.9.11 --pgsql-port=9999 --pgsql-user=root --pgsql-password=rjbigdata --pgsql-db=sysbench --threads=20 --table-size=10000000 --db-driver=pgsql --tables=20 preparesysbench oltp_insert.lua --time=60 --percentile=99 --pgsql-host=172.24.9.11 --pgsql-port=9999 --pgsql-user=root --pgsql-password=rjbigdata --pgsql-db=sysbench --threads=20 --table-size=6000000 --db-driver=pgsql --tables=20 runsysbench oltp_read_only.lua --time=60 --percentile=99 --pgsql-host=172.24.9.11 --pgsql-port=9999 --pgsql-user=root --pgsql-password=rjbigdata --pgsql-db=sysbench --threads=20 --table-size=6000000 --db-driver=pgsql --tables=20 runsysbench oltp_read_write.lua --time=60 --percentile=99 --pgsql-host=172.24.9.11 --pgsql-port=9999 --pgsql-user=root --pgsql-password=rjbigdata --pgsql-db=sysbench --threads=20 --table-size=100000 --db-driver=pgsql --tables=20 runsysbench oltp_insert.lua --pgsql-host=172.24.9.11 --pgsql-port=9999 --pgsql-user=root --pgsql-password=rjbigdata --pgsql-db=sysbench --threads=20 --db-driver=pgsql --tables=20 cleanup 硬件配置： 128GB + 32CPU + SSD + 万兆网络 pg配置： shared_buffers = 32GB（huge_pages = on ），其他默认 测试条件： - 预写20张表，每张1000w记录 Case insert read 混合 配置 1 20线程：2488532线程：3689364线程：5813796线程：66747 20线程：10578196线程：230311 20线程：5448596线程：58404 单机PG 2 20线程：724932线程：875264线程：999196线程：10545 20线程：3553196线程：72351 略： 2PG+pgpool 参考官方GitHub","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Sysbench - 数据库","slug":"Sysbench-数据库","permalink":"https://linqing2017.github.io/tags/Sysbench-%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"数据库调研笔记 -- PostgreSQL","slug":"Postgresql-安装介绍","date":"2019-09-23T16:00:00.000Z","updated":"2020-05-14T07:59:03.313Z","comments":true,"path":"2019/09/24/Postgresql-安装介绍/","link":"","permalink":"https://linqing2017.github.io/2019/09/24/Postgresql-%E5%AE%89%E8%A3%85%E4%BB%8B%E7%BB%8D/","excerpt":"PostgreSQL 调研笔记","text":"PostgreSQL 调研笔记 Quick-Start安装部署1234567891011121314151617181920yum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpmyum -y install postgresql11 postgresql11-server# 安装完成后，安装目录为/usr/pgsql-11，postgresql-11-setup脚本用来进行环境初始化和升级操作。# 需要注意的是：安装完RPM包后，如果想要修改默认的数据目录（/var/lib/pgsql/11/data），需要修改/usr/lib/systemd/system/postgresql-11.service中的环境变量PGDATA，并执行systemctl daemon-reload。# postgresql-11-setup会创建postgres和postgres，并且设定用户的home目录为/var/lib/pgsql/usr/pgsql-11/bin/postgresql-11-setup initdbsystemctl enable postgresql-11# 修改 pg_hba.conf文件，放开用户远程登录权限，注意同时要放开listen_addresses配置为*# host all root all md5# host all postgres 172.24.9.1/24 md5#systemctl start postgresql-11# 修改postgres用户登录密码，创建root用户sudo -u postgres psql -d postgres -c \"ALTER USER postgres WITH PASSWORD 'rjbigdata_admin';\"sudo -u postgres psql -d postgres -c \"CREATE ROLE root LOGIN REPLICATION CREATEDB PASSWORD 'rjbigdata'\" 关于系统配置的最佳实践： 建议将硬盘挂载点的Owner设置为PostgreSQL用户，数据目录直接放置在该目录之下（有利于避免升级、clean时的权限问题）。 使用NAS文件系统时可以会导致数据损坏（官方建议 synchronously 方式挂载，并且关闭caching），参考NFS的可能的问题。 关于型号量和共享内存的配置（参考）,Linux涉及到的配置需要关注的有kernel.shmmax（最大段尺寸）和kernel.shmall（最大共享内存页面） 使用systemd必须注意IPC资源（共享内存和信号量） 不会被操作系统过早删除，默认情况下要避免这种情况需要将启动pg的用户设定为系统用户（id小于1000）以及修改/etc/systemd/logind.conf 中RemoveIPC=no。（参考18.4.2. systemd RemoveIPC） 资源限制：maxproc、openfiles、datasize 防止PG在内存过渡调拨时Killer 设定systemd文件中的PG_OOM_ADJUST_VALUE=-1000，这样保证子进程不被Killer ； 设定systemd文件中的PG_OOM_ADJUST_FILE=/proc/self/oom_score_adj 的值为-1000（echo -1000 &gt; /proc/self/oom_score_adj），保证postmaster不被kill 降低PG内存相关配置，如shared_buffers 和work_mem） shared_buffers配置较大时，可以开启大页配置。评估页面数vm.nr_hugepages，可以参考 postmaster VmPeak / Hugepagesize 关闭数据库时，发送不同的信号量，PG关闭的方式不同（kill -INT head -1 /usr/local/pgsql/data/postmaster.pid），不要用-9关闭PG，危！！ SIGTERM：智能关闭模式，不在接收新连接、会让现有的会话正常结束它们的工作。仅当所有的会话终止后它才关闭。 SIGINT：服务器不再允许新的连接，并向所有现有服务器进程发送SIGTERM，让它们中断当前事务并立刻退出。然后服务器等待所有服务器进程退出并最终关闭。 如果服务处于在线备份模式，备份模式将被终止并致使备份无用。 SIGQUIT：服务器将给所有子进程发送 SIGQUIT并且等待它们终止。如果有任何进程没有在 5 秒内终止，它们将被发送 SIGKILL。主服务器进程将在所有子进程退出之后立刻退出，而无需做普通的数据库关闭处理。这将导致在下一次启动时（通过重放 WAL 日志）恢复。 账号管理 PostgreSQL 基于 roles 对数据库用户进行权限管理。 根据roles的创建方式不同，可以指特定用户或者某一组用户，即包含user和groups两个概念）。 Roles 基于可以和数据库的Objects绑定，或者将名下objects的权限赋予其他roles。 Roles 与操作系统的用户是完全分开的，不会相互影响。 系统预创建的超级用户为 postgres ，可以 su - postgres 切换到该用户后登录pg。 用户也可以在执行psql命令时，使用-U指定登录的用户。 123456789101112131415161718CREATE ROLE name; -- 创建ROLE，等价于使用createuser nameDROP ROLE name; -- 删除ROLE，等价于使用dropuser nameSELECT rolname FROM pg_roles; -- 查询已有rolesCREATE USER name; -- 创建用户ALTER ROLE XXX -- 修改用户权限CREATE ROLE root LOGIN REPLICATION CREATEDB CREATEEXTTABLE PASSWORD 'rjbigdata' -- 创建一个roles，并赋予各种权限GRANT group_role TO role1, ... ; -- role赋权REVOKE group_role FROM role1, ... ; -- role回收权限DROP ROLE name;-- 关于Roles权限继承的实例，其中joe被设计成user，admin、wheel被设计成Role Group：CREATE ROLE joe LOGIN INHERIT; -- joe可以重其他roles中继承权限CREATE ROLE admin NOINHERIT; -- 不允许从其他roles中继承权限CREATE ROLE wheel NOINHERIT;GRANT admin TO joe;GRANT wheel TO admin;SET ROLE admin / SET ROLE wheel; -- 获取admin和wheel的权限SET ROLE joe / SET ROLE NONE / RESET ROLE; -- 恢复权限 权限类型PostgreSQL中的权限（pg_roles中有roles的权限明细）： login privilege：roles有该权限时，可以作为一个普通用户登录 superuser status：除了login privilege以外所有权限 database creation role creation initiating replication：流复制的角色权限？？ 用来副本同步？ password INHERIT：具有INHERIT属性的成员角色会自动使用其所属成员角色的特权，通常用来在pg中区别roles和users 删除账号删除账号时有以下注意点： 需要将roles名下所有objects收回（ALTER TABLE bobs_table OWNER TO alice;） REASSIGN OWNED 可以将一个role名下所有object转义给另一个object DROP OWNED 删除role名下所有的object 默认RolesPG提供的一些默认Role，参考，这些Role名下关联了许多系统表。 Client 认证 用户认证相关的配置文件为：pg_hba.conf，如何配置可以参考 PG支持password、ldap、gss等方式的认证 12# 配置pg允许远程连接，pg_hba.conf中追加下面一行host all root all md5 SQL Language12345678910111213141516-- 建表,支持的类型包括：int，smallint，实数，双精度，char（N），varchar（N），date, time, timestamp, interval，以及自定义类型CREATE TABLE weather ( city varchar(80), temp_lo int, -- low temperature temp_hi int, -- high temperature prcp real, -- precipitation date date);-- 插入INSERT INTO weather VALUES ('San Francisco', 46, 50, 0.25, '1994-11-27');INSERT INTO weather (date, city, temp_hi, temp_lo) VALUES ('1994-11-29', 'Hayward', 54, 37);-- 文件批量导入，后端进程直接导入COPY weather FROM '/home/user/weather.txt'; 设置参数 postgresql.conf文件可以通过pg_ctl reload命令，或者 pg_reload_conf() 函数重载（部分配置可能要重启生效） postgresql.conf文件可以包含 include ‘filename’ 配置、include_dir ‘directory’配置 postgresql.auto.conf不应该手工编辑，这个文件保存了通过ALTER SYSTEM命令提供的设置，并且会覆盖postgresql.conf中的配置 通过SQL进行配置：ALTER SYSTEM（全局配置，等效于配置文件）、ALTER DATABASE、ALTER ROLE show/set 命令可以查看当前会话的配置，以及针对会话更新配置。 服务端启动时可以使用-c 指定配置，这些配置覆盖ALTER SYSTEM和配置文件配置 启动Client时可以使用环境变量指定，如（env PGOPTIONS=”-c geqo=off -c statement_timeout=5min” psql） 配置内容 文件位置：参考 连接和认证：参考 max_connections：最大连接数配置，默认是100，实际user的最大连接数为max_connections - superuser_reserved_connections 资源消耗：参考 内存配置： shared_buffers : 一个合理的shared_buffers开始值是系统内存的 25%。默认是128mb work_mem ：内部排序操作和哈希表使用的内存量，默认4mb。一个查询可能有好几个排序或者hash操作，每个操作会使用work_mem大小的内存。 maintenance_work_mem ：维护性操作（例如VACUUM、CREATE INDEX和ALTER TABLE ADD FOREIGN KEY）中使用的 最大的内存量，默认64mb。 autovacuum_work_mem : 指定每个自动清理工作者进程能使用的最大内存量。 temp_buffers ：每个数据库会话使用的临时缓冲区的最大数目，是会话的本地缓冲区，只用于访问临时表，默认是 8 兆字节（8MB）。 工作线程配置：max_worker_processes 、 max_parallel_workers 、max_parallel_workers_per_gather （并行查询参考） 强制刷盘配置：backend_flush_after（默认不强制刷盘） 日志相关配置，参考 流复制配置,参考 其他配置，参考 高可用方案各种方案一览表 参考安装部署 Pgpool-II 官方参考资料 &amp; 论文集 PG的编码集配置 PostgreSQL 10.1 手册","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://linqing2017.github.io/tags/PostgreSQL/"}]},{"title":"OpenCV 学习笔记","slug":"0-OpenCV-学习","date":"2019-09-03T16:00:00.000Z","updated":"2020-05-14T07:59:03.203Z","comments":false,"path":"2019/09/04/0-OpenCV-学习/","link":"","permalink":"https://linqing2017.github.io/2019/09/04/0-OpenCV-%E5%AD%A6%E4%B9%A0/","excerpt":"关于OpenCV-Python Tutorials、ROS By Example的学习笔记","text":"关于OpenCV-Python Tutorials、ROS By Example的学习笔记 ROS和OpenCV在ROS中使用OpenCV进行图像处理是开发ROS程序的常见需要，以下是在Ros中使用OpenCV开发的一些随笔。 摄像头的驱动ROS中使用摄像机包括下面的情景： 1.Kinect在ROS上安装Kinect驱动有以下几种方案： freenect_stack：底层基于libfreenect，需要额外安装libfreenect或者libfreenect2 openni_camera：同样基于openni需要分别安装依赖OpenNI和SensorKinect，安装过程可以参考博客 openni_kinect：基于openni是PrimeSense官方负责维护项目，包含：openni_camera、openni_launch、openni_tracker等多个模块。但是PrimeSense被收购之后，该项目的源码已经无法访问。 kinect_aux: Kinect上accelerometer/tilt/led相关功能的独立驱动。 2. IntelRealSense/XtionIntelRealSense官方提供了ROS上的相关驱动，官方github。 Xtion使用openni2_camera驱动，使用相机是可以参考openni2_launch。 3. webcam webcam是指通过USB接入的“即插即用”相机，此类相机没有专门的驱动程序，通过系统工具工作。 libuvc_camera usb_cam gscam ROS处理图像ROS中相机信息通过Topic进行发布，其中关键Topic包括： image_raw：摄像头拍摄的原始数据，格式为sensor_msgs/Image.msg camera_info：摄像头元数据信息，包括内参矩阵、对准系数等等，通常这个Topic中数据需要用户进行对准之后，进行人为设定。 ROS图像处理方面涉及到下面这些工具： 1. cv_bridgecv_bridge用来在sensor_msgs/Image和numpy两种格式之间进行转换，使OpenCV能够处理Topic中的图像数据。 目前cv_bridge提供C++/Python/JAVA 的接口。 2. image_transportimage_transport 可以将Image数据重新转发到新的topic中，其输入可以是Topic、图片、视频。 3. image_pipelineimage_pipeline是ROS的图像处理工具包，包括以下几个部分： camera_calibration：摄像头标定包 image_proc：图像校正包 stereo_image_proc：处理双目相机 depth_image_proc：处理深度相机 image_view、stereo_view：可视化 简单介绍一下image_pipeline的各个组件： image_proc image_proc主要用来处理rgb图片，提供node、nodelet两种运行方式。 1234# node方式运行ROS_NAMESPACE=my_camera rosrun image_proc image_proc# 上述node会寻找/my_camera/image_raw和/my_camera/camera_info，并根据后者的参数去校正前者中的图像。最终输出以下（未）校正图像：灰度（去）畸变、彩色（去）畸变。 image_proc 还提供了四个nodelet： debayer：将image转换成灰度、彩色两个版本并输出 rectify：校正图像 crop_decimate：图像抽样，即将图像的像素减小 resize：调整图像大小 depth_image_proc depth_image_proc主要用来处理深度图像，其所有的功能通过nodelet来提供： convert_metric：量测值变换，将深度信息的单位从mm变为m disparity：将深度图重变为disparity格式（disparity是一种视差图，可以通过双目相机生成） point_cloud_xyz：将深度图转换成点云图像，输出格式为sensor_msgs/PointCloud2 point_cloud_xyzrgb：将深度图和RGB合成，并转换成点云图像，输出格式为sensor_msgs/PointCloud2 register：将深度相机的frame-id变换到另一个坐标系中。 4. nodelet由于ROS使用Topic方式传输数据存在一定的延时和阻塞。在数据量小、频率低的情况下，传输耗费的时间可以忽略不计。但当传输图像流，点云等数据量较大的消息，或者执行有一定的实时性要求的任务时，因传输而耗费的时间就不得不考虑。nodelet的作用是让多个node在一个进程中用实现零拷贝通信。 用户需要manager节点，该节点管理多个nodelet节点，并为它们提供高性能数据通信。用户需要根据实际需要开发nodelet（nodelet基于pluginlib插件机制），并将nodelet加载到nodelet manager中（blog关于nodelet的介绍）。 当前图像相关的包多数提供了node和nodelet两种运行模式，使用以下命令可以查看当前系统中所有可运行nodelet 1rosrun nodelet declared_nodelets 以下是调用了image_proc和depth_image_proc中的nodelet实例： 1234567891011121314151617181920212223242526272829303132333435&lt;launch&gt; &lt;group ns=\"camera\"&gt; &lt;include file=\"$(find rgbd_launch)/launch/includes/manager.launch.xml\"&gt; &lt;arg name=\"name\" value=\"camera_nodelet_manager\" /&gt; &lt;arg name=\"debug\" value=\"false\" /&gt; &lt;!-- Run manager in GDB? --&gt; &lt;arg name=\"num_worker_threads\" value=\"2\" /&gt; &lt;/include&gt; &lt;!-- decimated to 160x120，将像素变为原来1/4 --&gt; &lt;node pkg=\"nodelet\" type=\"nodelet\" name=\"crop_decimate\" args=\"load image_proc/crop_decimate /camera/camera_nodelet_manager\" output=\"screen\"&gt; &lt;remap from=\"camera/image_raw\" to=\"rgb/image_raw\" /&gt; &lt;remap from=\"camera/camera_info\" to=\"rgb/camera_info\" /&gt; &lt;remap from=\"camera_out\" to=\"depth_downsample\" /&gt; &lt;param name=\"decimation_x\" value=\"4\" /&gt; &lt;param name=\"decimation_y\" value=\"4\" /&gt; &lt;param name=\"queue_size\" value=\"1\" /&gt; &lt;/node&gt; &lt;!--&amp;lt;!&amp;ndash; downsampled XYZ point cloud &amp;ndash;&amp;gt;--&gt; &lt;node pkg=\"nodelet\" type=\"nodelet\" name=\"points_downsample\" args=\"load depth_image_proc/point_cloud_xyz /camera/camera_nodelet_manager\" ns=\"depth_cloud\" output=\"screen\"&gt; &lt;remap from=\"image_rect\" to=\"/camera/depth/image_raw\"/&gt; &lt;remap from=\"camera_info\" to=\"/camera/depth/camera_info\"/&gt; &lt;/node&gt; &lt;/group&gt;&lt;/launch&gt; ROS By Example的教程ROS By Example第十章介绍了以下内容： 如何ROS中安装摄像头驱动。 在ROS框架之中使用Python-OpenCV，并通过Topic实现数据交互。 基于普通相机，使用OpenCV实现一个人脸追踪程序。 基于OpenNI的Skeleton Tracking程序。 PCL库的介绍 上述内容的源码在Github的pirobot/rbx1项目中，使用官网这份代码有以下几点值得注意： rbx1中不少例子基于OpenCV1编写，由于OpenCV2以上版本和OpenCV1接口并非完全兼容，因此需要修改后才能正常使用。 OpenCV2中完全整合numpy，如cv.CreateImage、cv.GetSize等接口已经完全取消，使用numpy的接口替代。 cv2中不少常量的名称也和cv中不同 video2ros.py能够将avi格式的视频输出到ROS的指定topic中，使用这个脚本可以替代RGB摄像机，该脚本支持视频暂停、循环播放。 ros2opencv2.py订阅指定Topic的图像输出，并在process_image函数中进行数据处理。编写视频处理程序时，可以继承该脚本中ROS2OpenCV2类，并复写process_image实现处理逻辑。 在catkin中如何编写python项目的makefile可以参考catkin_python_setup 人脸追踪DemoROS By Example 10.8 详细介绍了如何利用一个OpenCV的现成算法实现一个人脸追踪程序，该Demo识别/最终流程如下： 通过 Haar分类器，在一帧图像中识别人脸（face_detector.py）； 通过 goodFeaturesToTrack方法在人脸区域提取KeyPoint（good_features.py）； 通过 OpenCV’s Lucas-Kanade optical flow 的实现跟踪上述KeyPoint； 验证当前追踪角点的有效性，添加新KeyPoint或移除失效KeyPoint； PS：上述方案基于灰度图像进行人脸追踪，教程中还提供了了一种基于图像色彩进行追踪的方案（基于CamShift算法）。 Haar分类器在图片中识别人脸是一个分类过程，OpenCV提供基于haar特征和lbp特征的分类器。 使用Haar分类器，需要注意下面几点： 需要提供训练好的xml格式的模型文件，每个模型文件可以初始化一个分类器（CascadeClassifier） 对分类器调用detectMultiScale可以返回frame中的匹配结果，可以将多个分类器并联（串联）提高识别准确率 OpenCV的Haar分类器实现：Haar-like特征 + AdaBoost + 积分图（用来加速计算特征） 角点检测OpenCV中的角点检测： cornerHarris：Harris角点检测，其检测原理、以及特征可以参考blog 参数α对角点检测的灵敏度成反比 Harris角点检测算子对亮度和对比度的变化不敏感 旋转不变性、尺度不变性、不具备仿射不变性(但是可以通过Harris-Affine实现) goodFeaturesToTrack：Shi-Tomasijiao FAST：SUSAN算法 参考: 图像局部特征点检测算法综述 特征点追踪ROS by Example 中使用calcOpticalFlowPyrLK追踪，人脸上的角点运动，从而实现人脸跟踪的目的。 calcOpticalFlowPyrLK 是OpenCV提供的Lucas Kanade光流算法的实现，能够计算两帧图像之间特征点的位移。 光流定义为：空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。 个人理解，光流可以认为是素点的瞬时位移向量，是世界中可以感觉到的明显的视觉运动。 参考：对OpenCV中光流函数的介绍 总结ROS by Example 中人脸跟踪的Demo全都是基于传统CV算法的。由于我没有摄像头，输入使用的是一个清晰度很差的视频图像，最终导致识别和追踪效果全都差强任意。 参考ROS图像相关包 Haar分类器","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://linqing2017.github.io/tags/OpenCV/"}]},{"title":"摄像头标定","slug":"摄像头标定","date":"2019-09-01T16:00:00.000Z","updated":"2020-05-14T07:59:03.316Z","comments":false,"path":"2019/09/02/摄像头标定/","link":"","permalink":"https://linqing2017.github.io/2019/09/02/%E6%91%84%E5%83%8F%E5%A4%B4%E6%A0%87%E5%AE%9A/","excerpt":"简述张氏标定法，以及深度相机标定原理。","text":"简述张氏标定法，以及深度相机标定原理。 RGB相机标定矩阵变换关系相机标定涉及到以下坐标系，其最终目的是建立世界坐标系和像素坐标系之间的关系 世界坐标系$$(x_w,y_w,z_w)$$：用户定义的三维世界坐标系 相机坐标系$$(x_c,y_c,z_c)$$：相机坐标系z轴和光轴重合，且垂直于图像坐标系 图像坐标系$$(x,y,z)$$：成像平面坐标 像素坐标系$$(u,v)$$：和图像坐标系同平面，但是原点在左上角，单位为像素 上述坐标系有如下变换： 1. 世界坐标系到相机坐标系 将三维坐标中的点使用其次坐标表示，且刚体坐标系的变换可以通过旋转和平移得到：$$\\left[\\begin{matrix}x_c \\y_c \\z_c \\ 1\\end{matrix}\\right]=\\left[\\begin{matrix} R &amp; t \\ 0 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix}x_w \\y_w \\z_w \\ 1\\end{matrix}\\right]=\\left[\\begin{matrix}r_1 &amp; r_2 &amp; r_3 &amp; t\\end{matrix}\\right]\\left[\\begin{matrix}x_w \\y_w \\0 \\ 1\\end{matrix}\\right]=\\left[\\begin{matrix}r_1 &amp; r_2 &amp; t\\end{matrix}\\right]\\left[\\begin{matrix}x_w \\y_w \\1\\end{matrix}\\right] \\tag{1}$$上面的公式假定，物体的$$z_w=0$$，即可以认为标定时棋盘平面和$$X_wOY_w$$重合。 2. 相机坐标到图像坐标系 这一过程进行了从三维坐标到二维坐标的转换，即寻找相机坐标系中点（三维坐标）在图像坐标系中的成像点（二维坐标）。 根据上述图中的变换关系，即可推到出以下公式（$$f$$为相机焦距）。 $$z_c\\left[\\begin{matrix}x \\y \\ 1\\end{matrix}\\right]=\\left[\\begin{matrix} f&amp;0&amp;0\\0&amp;f&amp;0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix}x_c \\y_c \\z_c\\end{matrix}\\right]=\\left[\\begin{matrix} f&amp;0&amp;0\\0&amp;f&amp;0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix} 1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0 \\end{matrix}\\right]\\left[\\begin{matrix}x_c \\y_c \\z_c \\1\\end{matrix}\\right]\\tag{2}$$ *3. *图像坐标系考虑畸变 畸变是相机内部的固有性质，主要包括：径向畸变、切向畸变、薄透镜畸变。 $$[x，y，1]^T$$如何矫正畸变转换为实际图像坐标系暂时不介绍。 4. （实际）图像坐标系到像素坐标系 由于定义的像素坐标系原点与图像坐标系原点不重合，假设像素坐标系原点在图像坐标系下的坐标为$$(u0，v0)$$，每个像素点在图像坐标系x轴、y轴方向的尺寸为：dx、dy（每个像素点代表的实际尺寸），且像点在实际图像坐标系下的坐标为$$(x，y)$$，于是可得到像点在像素坐标系下的坐标为： $$u=x/d_x+u_0, v=v/d_y+v_0$$，矩阵形式为：$$\\left[\\begin{matrix}u \\v \\1\\end{matrix}\\right] =\\left[\\begin{matrix} 1/d_x&amp;0&amp;u_0\\0&amp;1/d_y&amp;v_0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix}x \\y \\1\\end{matrix}\\right] \\tag3$$ 将上面的三个公式合并，得到世界坐标系——相机坐标系——图像坐标系——像素坐标系的转换公式：$$\\left[\\begin{matrix}u \\v \\1\\end{matrix}\\right] =\\left[\\begin{matrix} 1/d_x&amp;0&amp;u_0\\0&amp;1/d_y&amp;v_0\\0&amp;0&amp;1 \\end{matrix}\\right]1/z_c\\left[\\begin{matrix} f&amp;0&amp;0\\0&amp;f&amp;0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix} 1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0 \\end{matrix}\\right]\\left[\\begin{matrix}r_1 &amp; r_2 &amp; r_3 &amp; t\\end{matrix}\\right]\\left[\\begin{matrix}x_w \\y_w \\0 \\ 1\\end{matrix}\\right]$$根据上面的公式化简，得到：$$M=\\left[\\begin{matrix} 1/d_x&amp;0&amp;u_0\\0&amp;1/d_y&amp;v_0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix} f&amp;0&amp;0\\0&amp;f&amp;0\\0&amp;0&amp;1 \\end{matrix}\\right]=\\left[\\begin{matrix} f/d_x=f_x&amp;0&amp;u_0\\0&amp;f/d_y=f_y&amp;v_0\\0&amp;0&amp;1 \\end{matrix}\\right]$$ $$\\left[\\begin{matrix} R &amp; t \\ 0 &amp; 1 \\end{matrix}\\right]=\\left[\\begin{matrix} 1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0 \\end{matrix}\\right]\\left[\\begin{matrix}r_1 &amp; r_2 &amp; r_3 &amp; t\\end{matrix}\\right]$$ 上面两矩阵即是标定时的相机内参矩阵和外参矩阵，根据两者的含义可以知道： 内参矩阵是固定不变的 外参矩阵由于相机相对于世界坐标系的位置不同，会发生变化。 PS：外参矩阵通常表示成棋盘格到相机坐标系的变换，相机到棋盘格有多少个视角，就有多少个外参矩阵。此处实际上是假设：棋盘平面和$$X_wOY_w$$重合，即$$Z_w=0$$。 如果按照以下方式标定： 将机械臂基座作为世界坐标系 相机位置固定 棋盘格固定在TCP上，通过移动TCP点获取相机到棋盘格的不同视角，然后进行标定 按上述方式，世界坐标系到相机的变换是固定的，外参实际上是机械器末端执行器的一个坐标系到相机坐标系的变换。 张氏相机标定由像素坐标系到世界坐标系的成像关系如下：$$\\left[\\begin{matrix}u \\ v \\ 1\\end{matrix}\\right]=s\\left[\\begin{matrix} f_x&amp;\\gamma&amp;u_0\\0&amp;f_y&amp;v_0\\0&amp;0&amp;1 \\end{matrix}\\right]\\left[\\begin{matrix}r_1 &amp; r_2 &amp; t\\end{matrix}\\right]\\left[\\begin{matrix}x_w \\ y_w \\ 1\\end{matrix}\\right]=sM\\left[\\begin{matrix}r_1 &amp; r_2 &amp; t\\end{matrix}\\right]\\left[\\begin{matrix}x_w \\ y_w \\ 1\\end{matrix}\\right]$$上面公式中s是尺度因子，$$fx、fy、u0、v0、γ$$（由于制造误差产生的两个坐标轴偏斜参数，通常很小）表示5个相机内参，$$R,t$$表示相机外参。 上述变换实际上是一个单应性变换，对应变换矩阵为单应性矩阵，它同时包含了相机内参和外参。 所以相机标定的目标即根据标定图得到单应矩阵，具体的过程如下： 打印一张棋盘格标定图纸，将其贴在平面物体的表面 拍摄一组不同方向棋盘格的图片，可以通过移动相机来实现，也可以移动标定图片来实现 对于每张拍摄的棋盘图片，检测图片中所有棋盘格的特征点（角点，也就是下图中黑白棋盘交叉点）。可以定义打印的棋盘图纸位于世界坐标系Zw=0的平面上，世界坐标系的原点位于棋盘图纸的固定一角，像素坐标系原点位于图片左上角。 棋盘标定图纸中所有角点的空间坐标是已知的，角点对应在拍摄的标定图片中的角点的像素坐标也是已知的 使用OpenCV中的现成函数求解单应矩阵 深度相机标定深度相机同样需要标定相机内参和外参，并且当深度相机和RGB相机的安装位置有区别时，还需将深度相机的信息转换到RGB相机中，这个过程称为配准。 深度相机标定：求解深度相机的内参和外参 深度相机配准：求解深度相机到RGB相机的变换矩阵 不同于RGB相机，深度相机的标定有很多方法，这里不介绍原理。 相机标定工具1. GMLGML C++ Camera Calibration Toolbox是基于OpenCV的一款开源相机标定工具，具有图形界面，能够对RGB相机进行标定，并返回相机内参矩阵、畸变函数。 2. camera_calibrationCamera Calibration是ROS上的一个标定工具，该工具使用棋盘格的方式可以标定深度和彩色相机。 其他知识1. 齐次坐标齐次坐标表示是计算机图形学的重要手段之一，它既能够用来明确区分向量和点，同时也更易用于进行仿射（线性）几何变换。 基本内容： 对于向量$$\\vec v$$及其基$$( \\boldsymbol a, \\boldsymbol b, \\boldsymbol c)$$存在一组坐标$$(v_1,v_2,v_3)$$，使得$$\\vec v=v_1a+v_2b+v_3*c$$ 对于空间中的点$$p$$则存在坐标$$(p_1,p_2,p_3)$$，使得$$p-p_0=p_1a+p_2b+p_3*c$$，其中$$p_0$$是坐标系的原点 考虑以上两个公式，在三维坐标系中，统一表示向量和点可以使用四维向量$$(v_1,v_2,v_3,w)$$，当$$w=0$$时该四维向量表示三维空间的一个向量，当$$w=1$$时该四维向量表示三维空间的一个点。 2. 单应性变换单应性变换是一种图像变换方式，常见图像变换包括以下几种。 线性变换：旋转、镜像、缩放、推移 仿射变换：线性变换 + 平移 透视变换：图片投影到一个新的视平面 单应性变换：一个平面到另外一个平面的投影映射。如，二维平面上的点映射到摄像机成像仪上的映射。 单应性变换会涉及到单应性矩阵，有以下公式： $$b = Ha^T$$，其中$$a=[x,y,1]^T$$，$$a=[x_1,y_1,1]^T$$为同一个点在原图像和映射图像上的坐标，$$H$$为$$3*3$$的单应性矩阵。 要求解上述公式中的H，只需获得四个点对即可，在OpenCV等视觉算法库中通常都有相应函数。 参考齐次坐标 单应性变换求解 张氏相机标定1 张氏相机标定2","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"https://linqing2017.github.io/tags/OpenCV/"},{"name":"标定","slug":"标定","permalink":"https://linqing2017.github.io/tags/%E6%A0%87%E5%AE%9A/"}]},{"title":"ROS学习笔记（六）","slug":"6-ROS学习笔记","date":"2019-08-20T16:00:00.000Z","updated":"2020-05-14T07:59:03.280Z","comments":false,"path":"2019/08/21/6-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/21/6-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"tf包的介绍，以及基本使用。","text":"tf包的介绍，以及基本使用。 介绍示例程序示例程序展示了二维平面内：通过TF实现的坐标跟随（turtle仿真中两只小乌龟相互跟随，参考）。 整个示例程序包括以下node： turtle1_tf_broadcaster：广播turtle1的绝对坐标 turtle2_tf_broadcaster: 广播turtle2的绝对坐标 turtle_tf_listener: 从/tf获取turtle2到turtle1的相对变换，并控制turtle2向turtle1移动 turtlesim_node、turtle_teleop_key：turtle套件 广播Node广播node监听/turtle/pose中的位置数据，并计算其绝对坐标系。 针对其接收到的每一条消息，调用回调函数将其广播到/tf中。 123456789101112131415161718192021222324252627282930def handle_turtle_pose(msg, turtlename): \"\"\" 回调函数，负责广播turtle的相对坐标系 :param msg: :param turtlename: :return: \"\"\" br = tf.TransformBroadcaster() br.sendTransform((msg.x, msg.y, 0), tf.transformations.quaternion_from_euler(0, 0, msg.theta), # 将欧拉角，转换为四元数 rospy.Time.now(), turtlename, \"world\")if __name__ == '__main__': \"\"\" turtle_tf_broadcaster节点负责发布某个turtle的绝对坐标系（即相对于World坐标系的变换）。 进程启动后完成以下工作： 1. 获取传入的参数turtle； 2. 监听/~turtle/pose的消息，并对每个消息调用回调函数handle_turtle_pose \"\"\" rospy.init_node('turtle_tf_broadcaster') turtlename = rospy.get_param('~turtle') rospy.Subscriber('/%s/pose' % turtlename, turtlesim.msg.Pose, handle_turtle_pose, # 回调函数 turtlename) # 回调函数的参数 rospy.spin() 完整代码 监听变换接收并缓冲系统中广播的所有坐标系，查询特定坐标系之间的变换关系。 1234567891011121314151617181920212223if __name__ == '__main__': rospy.init_node('turtle_tf_listener') listener = tf.TransformListener() \"\"\" 在turtlesim_node中在模拟出第二个turtle，并指定其初始位置和位置控制topiccmd_vel \"\"\" ..... rate = rospy.Rate(10.0) # 指定以下循环的频率 while not rospy.is_shutdown(): try: # 获取/turtle2到/turtle1的坐标系变换，rospy.Time(0)表示最新的变换 (trans,rot) = listener.lookupTransform('/turtle2', '/turtle1', rospy.Time(0)) except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException): continue # 以下内容控制turtle2向turtle1移动，即将turtle2坐标系向turtle1移动 ...... rate.sleep() 完整代码 TF和时间的关系计算frame-A在Ta相对于frame-B在Tb的时间坐标系： 12345try: now = rospy.Time.now() past = now - rospy.Duration(5.0) listener.waitForTransformFull(\"/turtle2\", now,\"/turtle1\", past,\"/world\", rospy.Duration(1.0)) (trans, rot) = listener.lookupTransformFull(\"/turtle2\", now,\"/turtle1\", past,\"/world\") 命令行工具12345678# 打印当前tf的拓扑到pdf文件中rosrun tf view_frames# 打印tf拓扑rosrun rqt_tf_tree rqt_tf_tree# 打印坐标系的变换关系，输出为 Translation（原点平移关系）、Rotation（转动角度，包括：四元数和欧拉角）rosrun tf tf_echo [frame_id_1] [frame_id_2] 参考tf Debugging tf problems tf2","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"},{"name":"tf","slug":"tf","permalink":"https://linqing2017.github.io/tags/tf/"}]},{"title":"ROS学习笔记（四）","slug":"4-ROS学习笔记","date":"2019-08-12T16:00:00.000Z","updated":"2020-05-14T07:59:03.279Z","comments":false,"path":"2019/08/13/4-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/13/4-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"理解URDF、XACRO文件的内容","text":"理解URDF、XACRO文件的内容 URDFURDF(Unified Robot Description Format), 是一种特殊的xml文件格式，绝大多数机器人应当提供xxx_description安装包，可以通过apt-get直接安装。 xacro文件是一种提供了一些更为高级编辑方式的宏文件，通过下面的命令可以将xacro解析出urdf。 12# 生成urdf，一个xacro中可能include了多个子文件，但是最终只会生成一个urdf文件rosrun xacro xacro.py /opt/ros/melodic/share/franka_description/robots/panda_arm_hand.urdf.xacro &gt; panda_arm_hand.urdf URDF文件格式URDF文件主要包括： link：表示Robot的一个关节 visual：描述形状，可以是简单形状，或者一个stl格式的文件 collision：描述link的碰撞体积 joint：表示link之间的相对位置关系，包括： parent child xyz：原点的平移向量(父链 ——&gt; 子链)，单位为米 rpy: child和parent坐标系的旋转矢量（转动顺序为：x -&gt; y -&gt; z） Joint 类型参考URDF中joint标签的定义，Joint有六种类型： revolute ：绕固定轴旋转有角度上下限 continuous ：绕固定轴旋转没有角度限制 prismatic ：滑动接头，沿轴线滑动，并具有由上限和下限指定的有限范围 fixed ：固定连接 floating ：该关节允许所有6个自由度的运动。 planar ：该关节允许在垂直于轴的平面中运动。 上述六种类型中，除fixed和floating以外，其余四种类型的运动轴均通过axis标签（默认情况下是(1,0,0)）定义。 sensor_msgs/JointState描述revolute和prismatic描述机器人的姿态，格式为： 123456Header headerstring[] name # 所有joint名称序列float64[] position # joint的运动幅度，是角度或者位移float64[] velocity # 当前的速度float64[] effort # 关节上施加的力 XACROXACRO文件和URDF实质上是等价的， 但是提供了一些更高级的方式来组织编辑机器人描述，包括： 复用URDF段落 嵌入简单的计算 include多个xacro文件 可视化化URDF查看urdf文件, 可以使用urdf_tutorial包: 12345roslaunch urdf_tutorial display.launch model:=/home/ruijie/Desktop/panda_arm_hand.urdfroslaunch urdf_tutorial display.launch model:=/home/ruijie/Desktop/panda_arm_hand.urdf gui:=trueroslaunch urdf_tutorial xacrodisplay.launch model:=/opt/ros/melodic/share/franka_description/robots/panda_arm_hand.urdf.xacro 上述命令中display.launch包含以下内容： 12345678910111213141516&lt;launch&gt; &lt;!-- 输入参数包括模型文件、是否使用GUI、rvizconfig配置--&gt; &lt;arg name=\"model\" default=\"$(find urdf_tutorial)/urdf/01-myfirst.urdf\"/&gt; &lt;arg name=\"gui\" default=\"true\" /&gt; &lt;arg name=\"rvizconfig\" default=\"$(find urdf_tutorial)/rviz/urdf.rviz\" /&gt; &lt;!-- 将urdf上传到robot_description中 --&gt; &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro.py $(arg model)\" /&gt; &lt;param name=\"use_gui\" value=\"$(arg gui)\"/&gt; &lt;node name=\"joint_state_publisher\" pkg=\"joint_state_publisher\" type=\"joint_state_publisher\" /&gt; &lt;node name=\"robot_state_publisher\" pkg=\"robot_state_publisher\" type=\"state_publisher\" /&gt; &lt;node name=\"rviz\" pkg=\"rviz\" type=\"rviz\" args=\"-d $(arg rvizconfig)\" required=\"true\" /&gt;&lt;/launch&gt; 上述launch启动三个node： joint_state_publisher： 一个发布JointState数据的工具。joint_state_publisher读取robot_description参数，查找所有非固定关节并发布一个JointState消息，其中定义了所有这些关节。joint_state_publisher的输出是/joint_states ,输入可以是GUI、其他Topic等。 robot_state_publisher：订阅/joint_states，根据其中的JointState数据以及Robot的关节参数，更新/tf和/tf_static树中的坐标变换关系。 rviz：可视化显示 组合机器人UR + Barrett：参考文章中的xacro，没有成功。 Git上有不少UR + Barrett的例子，如ur5_barrett_moveit、ur5_barrett_description SDFSDF格式是一种从世界级到机器人级的所有内容的完整描述，能够描述Gazebo环境中的Robot以及其他内容。 SDF可以认为是URDF格式的扩展，提供了以下能力： 在URDF基础上描述了物体的质量、惯性等更丰富的力学性质； 通过插件的方式可以描述摄像机、IMU等Gazebo支持的传感器； Gazebo提供了一些开源SDF格式的模型，用户可以在线下载或者离线下载后解压到.gazebo/models目录下。 Gazebo同时还支持一种.world格式的模型文件，这个文件同样支持SDF格式的语法，同SDF文件本质上没有差异。 一个空白的world文件，只定义了灯光、背景、视角等信息等信息 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" ?&gt;&lt;sdf version=\"1.4\"&gt; &lt;!-- We use a custom world for the rrbot so that the camera angle is launched correctly --&gt; &lt;world name=\"default\"&gt; &lt;include&gt; &lt;uri&gt;model://ground_plane&lt;/uri&gt; &lt;/include&gt; &lt;!-- Global light source --&gt; &lt;include&gt; &lt;uri&gt;model://sun&lt;/uri&gt; &lt;/include&gt; &lt;!-- Focus camera on tall pendulum --&gt; &lt;gui fullscreen='0'&gt; &lt;camera name='user_camera'&gt; &lt;pose&gt;4.927360 -4.376610 3.740080 0.000000 0.275643 2.356190&lt;/pose&gt; &lt;view_controller&gt;orbit&lt;/view_controller&gt; &lt;/camera&gt; &lt;/gui&gt; &lt;/world&gt;&lt;/sdf&gt; SDF格式和URDF格式的差异SDF比URDF多出了inertial标签和gazebo标签： inertial标签：定义在link标签中，用来描述物体物理属性（如，质心位置、质量、惯性矩阵） gazebo标签：gazebo标签可能定义在robot、link、joint等各种地方，其内容一般是Gazebo的扩展属性，通常一般使用.gazebo文件额外定义gazebo标签中的内容。 通过下面的launch文件，可以将World和URDF文件加载到Gazebo中： 1234567891011121314151617181920212223242526272829&lt;launch&gt; &lt;!-- Gazebo的启动参数，通常是paused、use_sim_time、gui、headless、debug --&gt; &lt;!-- 运行empty_world.launch可以加载，world_name指定的world文件 --&gt; &lt;include file=\"$(find gazebo_ros)/launch/empty_world.launch\"&gt; &lt;arg name=\"world_name\" value=\"$(find rrbot_gazebo)/worlds/rrbot.world\"/&gt; &lt;!-- Gazebo的启动参数 --&gt; &lt;/include&gt; &lt;!-- 将URDF文件加载到robot_description --&gt; &lt;param name=\"robot_description\" command=\"$(find xacro)/xacro --inorder '$(find rrbot_description)/urdf/rrbot.xacro'\" /&gt; &lt;!-- spawn_model是gazebo_ros中提供的Python脚本可以解析URDF格式的文件，并在软件中展示。 参考：rosrun gazebo_ros spawn_model -h 会打印该命令的详细用法。 --&gt; &lt;node name=\"urdf_spawner\" pkg=\"gazebo_ros\" type=\"spawn_model\" respawn=\"false\" output=\"screen\" args=\"-urdf -model rrbot -param robot_description\"/&gt; &lt;!-- ros_control rrbot launch file --&gt; &lt;!--include file=\"$(find rrbot_control)/launch/rrbot_control.launch\" /--&gt;&lt;/launch&gt; Gazebo插件Gazebo包括三种类型的插件： ModelPlugins SensorPlugins VisualPlugins 需要注意SensorPlugins需要附加到link标签中进行定义，而不能单独定义。 1234567891011121314&lt;robot&gt; &lt;!-- ... robot description ... --&gt; &lt;link name=\"sensor_link\"&gt; &lt;!-- ... link description ... --&gt; &lt;/link&gt; &lt;gazebo reference=\"sensor_link\"&gt; &lt;sensor type=\"camera\" name=\"camera1\"&gt; &lt;!-- 传感器参数 --&gt; &lt;plugin name=\"camera_controller\" filename=\"libgazebo_ros_camera.so\"&gt; &lt;!-- 插件参数 --&gt; &lt;/plugin&gt; &lt;/sensor&gt; &lt;/gazebo&gt;&lt;/robot&gt; 参考moveit！中文参考资料 Tutorial: Using roslaunch to start Gazebo, world files and URDF models","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"},{"name":"URDF","slug":"URDF","permalink":"https://linqing2017.github.io/tags/URDF/"}]},{"title":"ROS学习笔记（三）","slug":"3-ROS学习笔记","date":"2019-08-12T16:00:00.000Z","updated":"2020-05-14T07:59:03.278Z","comments":false,"path":"2019/08/13/3-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/13/3-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"Moveit系统结构 Moveit！中最重要的模块是move_group节点，该节点起到接口整合，以及提供Action、Service的作用。 关于move_group有以下几点： 整合C++（move_group_interface）、Python（moveit_commander）、GUI接口（Rviz），提供Action、Service功能 从ROS Param Server加载URDF、SRDF、Moveit！以及其他配置 move_group节点通过ROS的topic和actions控制robot，告知其位置信息、点云信息、以及其他传感器数据 相关问题下面的问题是阅读Concepts后提炼出来的： URDF文件如何定义？如何展示(gazebo、Rviz)？ SRDF文件包含哪些信息？详细了解SRDF文件文件内容？movit-setup-assistant 生成了哪些文件？这些文件有啥用途？ 3. /joint_states 中的数据含义是啥？如何通过他控制机械臂动作？ 4. robot_state_publisher 如何从/joint_states以及robot_description的URDF中计算TF？TF详细场景如何？ ros_control 如何工作？对hardwareInterface、actuator如何理解？ 如何使用control？如何定义control？control如何工作？control中涉及的数据有哪些含义？怎么生成的？ move_group 和 robot_state_publisher、joint_state_publisher如何交互？ FollowJointTrajectoryAction 是啥？如何通过他控制机器人？ PlanRequestAdapters和montion_planner如何工作？如何发送Request？Response是什么样的？规划路径时如何添加限制条件？ PlanningScene中包含哪些东西？都有什么作用？ 了解正/逆运动学原理？ 了解如何进行碰撞检测？如何应用ACM矩阵？ 运行UR5的movit! Demo开源项目- jontromanab/ur5_barrett_description - jontromanab/ur5_barrett_bringup - jontromanab/ur5_barrett_moveit仿真过程Step 1： roslaunch ur5_barrett_bringup ur5_barrett_table_world.launch limited:=true Step 2： roslaunch ur5_barrett_moveit ur5_barrett_moveit_planning_execution.launch limited:=true Step 3： 运行下面的代码控制末端执行器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import sysimport PyKDLimport rospyimport moveit_commanderimport moveit_msgs.msgimport geometry_msgs.msgfrom math import pifrom std_msgs.msg import Stringfrom moveit_commander.conversions import pose_to_listif __name__ == \"__main__\": moveit_commander.roscpp_initialize(sys.argv) rospy.init_node('control_ur5', anonymous=True) robot = moveit_commander.RobotCommander() scene = moveit_commander.PlanningSceneInterface() group_name = \"manipulator\" # endeffector #manipulator move_group = moveit_commander.MoveGroupCommander(group_name) display_trajectory_publisher = rospy.Publisher('/move_group/display_planned_path', moveit_msgs.msg.DisplayTrajectory, queue_size=30) current_pose = move_group.get_current_pose() print(\"================= Before Pos ================= \") print(current_pose) pose_goal = geometry_msgs.msg.Pose() # Goal position rot = PyKDL.Rotation.Quaternion(current_pose.pose.orientation.x, current_pose.pose.orientation.y, current_pose.pose.orientation.z, current_pose.pose.orientation.w) # set goal rot print(rot.GetQuaternion()) rot.DoRotY(-pi/2) pose_goal.orientation = geometry_msgs.msg.Quaternion(*list(rot.GetQuaternion())) print(rot.GetQuaternion()) # set goal position pose_goal.position.x = current_pose.pose.position.x pose_goal.position.y = current_pose.pose.position.y pose_goal.position.z = current_pose.pose.position.z # pose_goal.position.x = 0.7 # pose_goal.position.y = 0 # pose_goal.position.z = 0.4 move_group.set_pose_target(pose_goal) move_group.go(wait=True) move_group.stop() print(\"================= Current Pos ================= \") print(current_pose) 需要注意： 手臂长度有限，臂长大概是0.8m，离地高度大概是0.36m。如果输入的目标位置，不在这个范围内将会规划失败。指定目标点时，使用的是geometry_msgs/Pose参数，包含： geometry_msgs/Point position：世界坐标系下(x,y,z) geometry_msgs/Quaternion orientation：执行器转过的角度(绕x轴转动为手掌转动朝向不变，绕y，x轴转动为手掌不动，朝向变动)。 关于Geometry_Msgs.msg.Pose参考这个帖子 参考moveit！中文参考资料","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"},{"name":"Moveit！","slug":"Moveit！","permalink":"https://linqing2017.github.io/tags/Moveit%EF%BC%81/"}]},{"title":"ROS学习笔记（五）","slug":"5-ROS学习笔记","date":"2019-08-12T16:00:00.000Z","updated":"2020-05-14T07:59:03.279Z","comments":false,"path":"2019/08/13/5-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/13/5-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"ROS Controlros_control是通用Robot控制框架，用来连接ROS中应用和实际（仿真）机器人，它包含一系列控制器接口、传动装置接口、硬件接口、控制器工具箱等等，可以帮助机器人应用快速落地，提高开发效率。 ros_control运行时，包括以下几个部分： Controller Manager：Manger管理了多个Controller，每个Controller代表一个上层ROS应用。 Controller：Controller可以完成每个joint的控制，请求下层的硬件资源，并且提供了PID控制器，读取硬件资源接口中的状态，并发布控制命令。 Hardware Rescource：抽象接口层 RobotHW：硬件抽象层和硬件直接打交道，通过write和read方法来完成硬件的操作，这一层也包含关节限位、力矩转换、状态转换等功能。 下图是ros_control的数据流图： 目前，ros_control中提供了一些现成Controllers和Hardware Interface，用户可以根据需要自己定制这些插件。 其他概念TransmissionsTransmissions就是机器人的传动系统，机器人每个需要运动的关节都需要配置相应的Transmission。 通常情况下Transmission的内容会在URDF文件中直接定义。用户定义Transmission内容时可以定义在单独的文件中，之后include到需要的URDF文件中。 目前，ROS提供的transmission类型包括： DifferentialTransmission FourBarLinkageTransmission SimpleTransmission 机械臂中一般只会用到SimpleTransmission。 以下是一个Joint的传动定义 123456789&lt;transmission name=\"trans_name\"&gt; &lt;type&gt;transmission_interface/SimpleTransmission&lt;/type&gt; &lt;joint name=\"joint_name\"&gt; &lt;hardwareInterface&gt;PositionJointInterface&lt;/hardwareInterface&gt; &lt;/joint&gt; &lt;actuator name=\"joint_motor\"&gt; &lt;mechanicalReduction&gt;1&lt;/mechanicalReduction&gt; &lt;/actuator&gt;&lt;/transmission&gt; 上述定义中，注意以下两点： hardwareInterface：Controller和RobotHw沟通的接口，基本上和controllers的种类是对应的（参考HardwareInterface类型）。 actuator：??? 不知道用来干啥的？？？ Joint LimitsJoint Limits是RobotHW中的一块，维护一个关节限位的数据结构，包含关节速度、位置、加速度、加加速度、力矩等方面的限位，还包含安全作用的位置软限位、速度边界（k_v）和位置边界（k_p）等等。 Joint Limits可以直接定义在URDF文件中，可以通过YAML文件加载到ROS parameter server中，还可以使用joint_limits_interface在代码中定义。 Controller managerController Manager可以加载、开始运行、停止运行、卸载不同的controller，并且提供了多种工具来完成这些操作。 命令行工具： 1rosrun controller_manager controller_manager &lt;command&gt; &lt;controller_name&gt; 参考ROS Control介绍 Gazebo Ros Control","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"},{"name":"Moveit","slug":"Moveit","permalink":"https://linqing2017.github.io/tags/Moveit/"},{"name":"ROS Control","slug":"ROS-Control","permalink":"https://linqing2017.github.io/tags/ROS-Control/"}]},{"title":"ROS学习笔记（二）","slug":"2-ROS学习笔记","date":"2019-08-08T16:00:00.000Z","updated":"2020-05-14T07:59:03.277Z","comments":false,"path":"2019/08/09/2-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/09/2-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"编写一个ROS上运行的Hello World程序。","text":"编写一个ROS上运行的Hello World程序。 TurtlesimTurtlesim是ROS官方提供的一个示例，用来帮助用户理解ROS中基本概念。 观察node之间如何通信参考官方Topic的概念。 示例提供了一个简易的海龟绘图功能，涉及到下面几个Package： Package Publications Pub Subscriptions 备注 turtlesim turtlesim_node /turtle1/color_sensor/turtle1/pose /turtle1/cmd_vel —– turtlesim turtle_teleop_key /turtle1/cmd_vel —– —– turtlesim_node是海龟的绘图界面，通过监听cmd_vel的位置信息来进行绘图。 turtle_teleop_key是基于键盘方向键的发送程序，不断的向cmd_vel中发送geometry_msgs/Twist格式的位置数据。 用户可以通过rostopic直接向cmd_vel发送数据控制海龟运动轨迹，或查看cmd_vel的数据。 使用rqt_graph可以绘制topic连接的具体拓扑信息，命令为rosrun rqt_graph rqt_graph。 使用rqt_plot可以将topic的数值信息绘制为连续曲线，命令为rosrun rqt_plot rqt_plot。 调用Node服务以及修改参数turtlesim_node 提供了一些Services用来清空画板、复制画笔等功能，这些功能可以使用rosservice来调用（参考）。 查看node日志使用roslaunch参考 参考2 定义msg和srv自定义msg假设在beginner_tutorials包中定义一个新msg包含以下内容： Step 1：在beginner_tutorials中创建msg目录，并在该目录下创建Student.msg文件，内容如下： 1234string first_namestring last_nameuint8 ageuint32 score Step 2：修改package.xml文件添加message_generation依赖和message_runtime，其中编译时只用依赖message_generation，而运行时需要依赖message_runtime。 1234&lt;!--编译时依赖--&gt;&lt;build_depend&gt;message_generation&lt;/build_depend&gt;&lt;!--运行时依赖--&gt;&lt;exec_depend&gt;message_runtime&lt;/exec_depend&gt; Step 3： 修改CMakeLists.txt 123456789101112131415161718192021222324252627cmake_minimum_required(VERSION 2.8.3)project(beginner_tutorials)find_package(catkin REQUIRED COMPONENTS roscpp rospy std_msgs message_generation # 添加编译时需要的message_generation组件)add_message_files( FILES Num.msg # 指明msg定义文件的地址 )generate_messages( # message_generation组件主键的依赖，这里只依赖std_msgs DEPENDENCIES std_msgs )catkin_package(# INCLUDE_DIRS include# LIBRARIES beginner_tutorials CATKIN_DEPENDS roscpp rospy std_msgs message_runtime #CATKIN指明依赖# DEPENDS system_lib)include_directories(# include $&#123;catkin_INCLUDE_DIRS&#125;) Step 3：编译beginner_tutorials，使用rosmsg show beginner_tutorials/Student查看生成的msg srv接口定义参考 Python实现Publisher/Subscriberpython脚本放在Package的scripts目录下，可以无需编译直接运行： talker.py 1234567891011121314151617181920212223#!/usr/bin/env python# license removed for brevityimport rospyfrom std_msgs.msg import Stringdef talker(): # 创建一个Topic，名称为chatter，类型为String，队列深度为10 pub = rospy.Publisher('chatter', String, queue_size=10) # 启动一个和roscore通信的node，名称为talker+xxxx rospy.init_node('talker', anonymous=True) rate = rospy.Rate(10) # 10hz的数量发送数据 while not rospy.is_shutdown(): hello_str = \"hello world %s\" % rospy.get_time() # loginfo：将信息打印到屏幕，并写入到rosout和日志文件 rospy.loginfo(hello_str) pub.publish(hello_str) #发送数据 rate.sleep()if __name__ == '__main__': try: talker() except rospy.ROSInterruptException: pass listener.py 1234567891011121314151617181920212223#!/usr/bin/env pythonimport rospyfrom std_msgs.msg import Stringdef callback(data): rospy.loginfo(rospy.get_caller_id() + \"I heard %s\", data.data) def listener(): # In ROS, nodes are uniquely named. If two nodes with the same # name are launched, the previous one is kicked off. The # anonymous=True flag means that rospy will choose a unique # name for our 'listener' node so that multiple listeners can # run simultaneously. rospy.init_node('listener', anonymous=True) rospy.Subscriber(\"chatter\", String, callback) # spin() simply keeps python from exiting until this node is stopped rospy.spin()if __name__ == '__main__': listener() Python实现Service和Client参考 使用rosbag记录和回放Topic参考","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"}]},{"title":"ROS学习笔记（一）","slug":"1-ROS学习笔记","date":"2019-08-08T16:00:00.000Z","updated":"2020-05-14T07:59:03.238Z","comments":false,"path":"2019/08/09/1-ROS学习笔记/","link":"","permalink":"https://linqing2017.github.io/2019/08/09/1-ROS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"了解ROS的基本框架","text":"了解ROS的基本框架 ROS概述ROS是运行在Linux等操作系统上的一个次级操作系统，提供机器人从开发到运行的一系列工具。 ROS的核心模块包括： 1. ROS通信机制 ROS使用分布式架构，将机器人的功能和软件做成一个个node，然后每个node通过topic进行沟通，这些node可以部署在同一台机器上，也可以部署在不同机器上，还可以部署在互联网上。node之间通过topic发布/订阅的方式进行通信，或者基于ROS中Service机制点对点通信。 2. 机器人特性功能 机器人特性功能主要指机器人开发中常用的数据结构、算法、规范等内容。当基于ROS开发机器人程序时，可以直接引用相应lib。 标准机器人消息 机器人几何库 机器人描述语言 抢占式远程过程调用 诊断 位置估计 定位导航 3. 工具集 工具集只机器人开发过程使用的辅助工具，包括： 常用命令工具 工程管理工具catkin 可视化工具是rqt、rviz 包管理工具 第三方工具 ROS系统整体架构文件系统架构文件系统架构实际上指的是ROS程序开发过程中工作空间的架构。 ROS应用在开发过程中使用catkin进行包管理，而catkin的底层是通过CMake工作的。 catkin目录说明： src：放置各个功能包和一个用于这些功能包的CMake配置文件CMakeLists.txt。 build编译空间：放置CMake和catkin编译功能包时产生的缓存、配置、中间文件等。 devel开发空间：放置编译好的可执行程序，这些可执行程序是不需要安装就能直接运行的。 功能包是ROS程序的最小结构，表示一个node。安装完成ROS后所有自带的功能包位于$ROS_PACKAGE_PATH，用户可以自行开发功能包。 功能包中通常包括下面几个文件： CMakeLists.txt：cmake配置文件。 package.xml：功能包的配置信息，如依赖、名称等。 include/：*.h头文件放在这里。 msg：非标准消息定义目录，ROS本身已经定义了许多用于node之间通信标准消息已经标准数据类型。 srv：服务类型定义目录，ROS中node通信可以通过消息和服务两种方式，消息基于topic订阅，服务则是node之间点对点通信机制。 scripts：bash、python或其他脚本的可执行文件。 launch：存放.launch文件，.launch文件用于启动ROS功能包中的一个或多个节点（Ansible？？？） src：源码目录 计算图级架构计算图实际上指的是多个node在ROS中构成的网络，所有计算图中的节点可以进行数据交互。 计算图中包含以下概念： 1. node节点是计算执行进程，功能包中创建的每个可执行程序在被启动加载到系统进程中后，该进程就是一个ROS节点。 节点都是各自独立的可执行文件，能够通过主题（topic）、服务（server）或参数服务器（parameter server）与其他节点通信。 节点如果用c++进行编写，需要用到ROS提供的库roscpp；节点如果用python进行编写，需要用到ROS提供的库rospy。 2. Topicnode间沟通的方式之一是消息订阅/发布，每个消息都必须发布到相应的主题（topic），通过主题来实现在ROS计算图网络中的路由转发。 同一个主题可以有多个订阅者也可以有多个发布者。每个主题都是强类型的，不管是发布消息到主题还是从主题中订阅消息，发布者和订阅者定义的消息类型必须与主题的消息类型相匹配。 3. 服务在一些特殊的场合，节点间需要点对点的高效率通信并及时获取应答，这个时候就需要用服务的方式进行交互。 服务通信过程中服务的数据类型需要用户自己定义，与消息不同，节点并不提供标准服务类型。服务类型的定义文件都是以*.srv为扩展名，并且被放在功能包的srv/文件夹下。 4. 配置管理器参数服务器（parameter server）能够使数据通过关键词存储在一个系统的核心位置。通过使用参数，就能够在节点运行时动态配置节点或改变节点的工作任务。参数服务器是可通过网络访问的共享的多变量字典，节点使用此服务器来存储和检索运行时的参数。 5. 节点管理器节点管理器（master）用于节点的名称注册和查找等，也负责设置节点间的通信。 由于ROS本身就是一个分布式的网络系统，所以你可以在某台计算机上运行节点管理器，在这台计算机和其他台计算机上运行节点（节点管理器需不需要高可用？？）。 ROS中提供了跟节点管理器相关的命令行工具，就是roscore，roscore命令用于启动节点管理器，这个命令会加载ROS节点管理器和其他ROS核心组件。 当ROS开始工作时，用户需要首先执行roscore运行一个节点管理器，以此来保证后续启动的node可以正常通信。 执行roscore的同时，系统会默认启动一个日志node（rosout），执行rosnode可以查看该node的详细信息。 1234567891011121314151617# rosout启动发布了一个topic（rosout_agg），以及两个服务（get_loggers、set_logger_level）--------------------------------------------------------------------------------Node [/rosout]Publications: * /rosout_agg [rosgraph_msgs/Log]Subscriptions: * /rosout [unknown type]Services: * /rosout/get_loggers * /rosout/set_logger_levelcontacting node http://ruijie-virtual-machine:39349/ ...Pid: 31314 消息文件ROS使用消息类型描述语言，描述node之间传递的消息，并可以在不同的编程语言（如c++、python等）书写的程序中使用此消息。 不管是ROS系统提供的标准类型消息，还是用户自定义的非标准类型消息，定义文件都是以*.msg作为扩展名。 消息类型的定义分为两个主要部分：字段的数据类型和字段的名称，简单点说就是结构体中的变量类型和变量名称。 经常用到的类型包括： 基本类型：包括int、boolean等基础数据类型。 通用类型：高级数据类型，如四元数、传感器数据等。 参考SLAM+语音机器人DIY系列","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"}]},{"title":"深度学习读书笔记","slug":"深度学习入门","date":"2019-08-07T16:00:00.000Z","updated":"2020-05-14T07:59:03.317Z","comments":false,"path":"2019/08/08/深度学习入门/","link":"","permalink":"https://linqing2017.github.io/2019/08/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/","excerpt":"一篇关于《动手学深度学习》的读书笔记。 电子书地址：https://zh.gluon.ai/","text":"一篇关于《动手学深度学习》的读书笔记。 电子书地址：https://zh.gluon.ai/ 环境准备《动手学深度学习》基于Apache MXNet提供关于深度学习的Demo，并且提供了配套代码。整个项目通过conda的虚拟环境分发，并提供jupyter方式的可读文档。 安装方式如下： 1234567891011121314151617181920212223# 安装Mincodesh Miniconda3-latest-Linux-x86_64.sh# 下载工程文件 https://zh.d2l.ai/d2l-zh-1.0.zip ，并且解压到到d2l-zh文件夹unzip d2l-zh-1.0.zip# 安装d2l-zh项目的环境source ~/.bashrc &amp;&amp; conda env create -f environment.yml# 切换到项目环境conda activate gluon# 打开项目jupyterjupyter notebook --ip=$HOSTNAME --port=9999# 其他一些命令# 替换pip源pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple# 删除pip源，恢复使用默认源pip config unset global.index-url# 配置国内MXNet站点的数据仓库set MXNET_GLUON_REPO=https://apache-mxnet.s3.cn-north-1.amazonaws.com.cn/ jupyter notebook condaConda是python的软件包管理软件，它和pip的最大不同在于同时提供虚拟环境管理功能，方便用户在多个虚拟环境中切换。 Miniconda是最小的conda安装环境，其中包含了Python2和Python3的运行环境。 Anaconda 是一个开源的Python发行版本，其包含了conda、Python等180多个科学包及其依赖项。 conda常用命令: 1234567891011121314151617181920# envname 指代你新建的环境的名称conda create --name envname# 删除虚拟环境conda remove --name envname --all# 指定python版本conda create -n envname python=3.4# 指定python版本,以及多个包conda create -n envname python=3.4 scipy=0.15.0 astroib numpy# 给虚拟环境安装包conda install -n envname scipy=0.15.0# 从文件创建虚拟环境conda install pytorch torchvision cudatoolkit=9.0 -c pytorchconda env create -f environment.yml# 查看当前环境安装的包conda list# 查看所有环境conda install pytorch torchvision cudatoolkit=9.0 -c pytorchconda info --envs# 激活环境conda activate envnameconda install pytorch torchvision cudatoolkit=9.0 -c pytorch# 退出当前环境conda deactivate 数据操作在MXNet中，NDArray是一个类，也是存储和变换数据的主要工具，提供GPU计算和自动求梯度等多种功能。 NDArray 和 Numpy 有点类似，但是它可以表示多维张量，通过array函数和asnumpy函数可以使数据在NDArray和NumPy格式之间相互变换。 自动求梯度MXNet使用autograd模块自动求梯度。 1234567from mxnet import autograd,ndx = nd.arange(4).reshape((4, 1))x.attach_grad() # 调用attach_grad申请存储梯度所需要的内存with autograd.record(): # 调用record函数后，MXNet会记录并计算梯度。 y = 2* nd.dot(x.T,x) # 目标函数是 y = x_T * x，我们要求这个函数在x = [0,1,2,3]_T时的梯度 y.backward() print(x.grad) # x.grad 就是对应的梯度 参考mxnet api 查询 深度学习基础线性回归机器学习的几个概念： 样本集、样本、标签 模型、模型参数、损失函数 优化算法：当模型没有解析解时，通过优化算法的有限次迭代求得一个使损失函数最小的数值解。 学习率、批量大小 ——&gt; 超参数 模型预测 ~ 推断 ~ 测试 神经网络的几个概念： 输入个数 ~ 特征数、特征向量维度 神经元（计算单元） 全连接层、稠密层 神经网络和线性回归的联系： 线性回归是一个全连接单层神经网络 softmax回归解决分类问题的思路： 输出是离散值的分类问题，且输出个数等于标签类别数 每个输出值是预测该类别的置信度，预测结果取最大输出值对应的类别 上述思路存在问题： 输出值的范围不确定，难以直观判断这些值的含义 SoftMax回归的解决思路： 将输出值变换成值为正且和为1的概率分布（归一化？？？实际上就是：每个输出求exp(y_i)/所有输出exp(y_i)之和） 交叉熵损失函数： 分类问题的label一般这样表示：[1,0,0,0]（四个类别时，分类取值为1） 分类问题的模型输出表示为：[p1,p2,p3,p4]（四个类别时，表示对应类别0~1之间的概率） 分类问题不适合使用平方损失函数，适合使用交叉熵衡量损失。 交叉熵损失函数和最大似然估计的思想一致。 分类的结果： 准确率 = 正确预测数量 / 总预测数量 多层感知机 不引入激活函数时，全连接多层感知机的模型本质上和单层感知机等价的，依然是线性模型。本质上全连接等价于仿射变换（一个向量空间进行一次线性变换并接上一个平移，变换为另一个向量空间。），多个仿射变换的叠加依然是仿射变换。 解决上述问题：隐藏变量使用按元素运算的非线性函数进行变换，这些非线性函数称谓激活函数 ReLU(x)=max(x,0) sigmoid函数：sigmoid(x)=1/(1+exp(−x)) tanh函数: tanh(x)=(1−exp(−2x))/(1+exp(−2x)) 模型选择、欠拟合、过拟合 当模型在训练数据集上更准确时，它在测试数据集上却不一定更准确： 训练误差（training error）：模型在训练数据集上表现出的误差 泛化误差（generalization error）：模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。 K折交叉验证：把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K−1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。 防止过拟合的方法： 增大训练量 使用复杂度合适的模型 丢弃法：倒置丢弃法，原理是将隐藏层的输出丢弃或者拉伸。 权重衰减：为模型添加损失函数惩罚项，使学出的模型参数值较小。原因是复杂模型往往权重参数很多，比较复杂。 权重衰减使，如何定义损失函数： 正向传播、反向传播和计算图知道是什么东西就好？数值稳定性和模型初始化 数值稳定性：典型问题是衰减和爆炸，当神经网络层数较多时，模型的数值稳定性容易变差； 通常需要随机初始化神经网络的模型参数，如权重参数。 kaggle赛题房价预测，通过观摩别人的解题过程发现，大致流程如下： 理解问题：理解数据的规模、指标的含义、数据有哪些确实等等。 单因素研究：关注单一变量（如，输出）作图，并分析 多因素研究：分析因变量和自变量之间的关系。 基础清洗：清洗数据集并且对缺失数据，异常值和分类数据进行一些处理。 检验假设：检查数据是否和多元分析方法的假设达到一致。 建立模型，训练，预测MXNet 常用API接口 1234567891011from mxnet.gluon import nndef get_net(): # 定义一个神经网络模型，定义若干层隐藏层，并且指定隐藏层的激活类型， Dense表示定义一个全连接层。 net = nn.Sequential() net.add( nn.Dense(1024, activation='relu'), nn.Dense(1) ) net.initialize(init.Normal(sigma=0.01)) return net 1234from mxnet.gluon import loss as gloss# 定义损失函数loss = gloss.L2Loss() # 均方根损失loss = gloss.SoftmaxCrossEntropyLoss() # 交叉熵损失 12345678910111213141516171819202122232425262728293031323334353637383940from mxnet.gluon import data as gdatafrom mxnet import autograd, gluon, init, nddef train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size): \"\"\" 训练函数 :param net :模型 :param train_features :训练集的特征 :param train_labels :训练集的标签 :param test_features :测试集的特征 :param test_labels :测试集的标签 :param num_epochs :在训练集上的训练次数，每次训练由多个小批量样本迭代组成 :param learning_rate :每次迭代的学习率 :param weight_decay :权重衰减系数，防止过拟合 :param batch_size :小批量样本迭代中的样本数目 :return :输出结果，即训练误差和测试误差（验证误差） \"\"\" train_ls, test_ls = [], [] train_iter = gdata.DataLoader( # 将训练集进行随机小批量分割 gdata.ArrayDataset(train_features, train_labels), batch_size, shuffle=True ) trainer = gluon.Trainer( # 使用一个trainer，'adam'表示学习迭代的方法，同样还有sgd net.collect_params(), 'adam', &#123;'learning_rate': learning_rate, 'wd': weight_decay&#125; ) for epoch in range(num_epochs): for X, y in train_iter: with autograd.record(): l = loss(net(X), y) # 求损失函数函数 l.backward() # 对损失函数的每个变量求偏导 trainer.step(batch_size) # 进行模型计算 train_ls.append(loss(net, train_features, train_labels)) if test_labels is not None: test_ls.append(loss(net, test_features, test_labels)) return train_ls, test_ls 一些数据集[MNIST]：手写数字识别数据集","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://linqing2017.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"读书笔记","slug":"读书笔记","permalink":"https://linqing2017.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"ROS环境安装部署","slug":"0-ROS环境安装部署","date":"2019-08-07T16:00:00.000Z","updated":"2020-05-14T07:59:03.211Z","comments":false,"path":"2019/08/08/0-ROS环境安装部署/","link":"","permalink":"https://linqing2017.github.io/2019/08/08/0-ROS%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"","text":"操作系统配置安装Ubuntu 18.04版本操作系统，安装完成后进行以下配置： 1. 配置sudo权限123sudo passwd rootsudo -s#编辑/etc/sudoers，添加 ruijie ALL=(ALL:ALL) ALL 2. 安装SSH服务123456sudo apt-get install openssh-server net-tools vim# 修改sshd_config文件，添加PermitRootLogin yes，注释掉PermitRootLogin prohibit-passwordvim /etc/ssh/sshd_configsudo service sshd restart 3. 配置远程桌面Ubuntu 18.04使用官方xRDP时无法正常登陆，参考Goolge上的解决方案安装第三方版本的xRDP。 1234567sudo add-apt-repository ppa:martinx/xrdp-hwe-18.04sudo apt-get updatesudo apt-get install xrdp xorgsudo adduser xrdp ssl-cert# 修改 /usr/share/polkit-1/actions/org.freedesktop.color.policy（直接删掉）# 参考：https://c-nergy.be/blog/?p=12073reboot ROS安装版本选择当前ROS包含两个长期维护版本，分别支持不同版本操作系统。通过 rosdistro 工具，我们可以在系统中安装多个版本的ROS。 通常ROS发行版的维护周期是5年或者2年，当前仍然在维护周期内的发行版是以下两个，均为5年维护周期。 版本名称 Release EOL 操作系统 Melodic Morenia May 23rd, 2018 May, 2023 Ubuntu 18.04、Debian、Win10 Kinetic Kame May 23rd, 2016 April, 2021(Xenial EOL) Ubuntu 15.10Ubuntu 16.04Debian 8 文本选择在Ubuntu 18.04环境中安装，ROS Melodic Morenia。 安装过程参考官方文档，通过以下步骤安装ROS Melodic： 1. 部署repositoriesROS除了提供官方仓库以外，在全球范围提供了镜像仓库，安装过程中为了避免网络问题，同时配置两个国内的仓库地址。 12345678# 添加官方仓库sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list'# 添加中科大仓库sudo sh -c '. /etc/lsb-release &amp;&amp; echo \"deb http://mirrors.ustc.edu.cn/ros/ubuntu/ $DISTRIB_CODENAME main\" &gt;&gt; /etc/apt/sources.list.d/ros-latest.list'# 添加清华仓库sudo sh -c '. /etc/lsb-release &amp;&amp; echo \"deb http://mirrors.tuna.tsinghua.edu.cn/ros/ubuntu/ $DISTRIB_CODENAME main\" &gt;&gt; /etc/apt/sources.list.d/ros-latest.list'# 添加仓库的Keysudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 2. 安装DEB12sudo apt updatesudo apt install ros-melodic-desktop-full Ros提供下面几种安装包，本文选择ros-melodic-desktop-full，完整安装所有工具。 DEB包 包含组件 说明 ros-melodic-desktop-full ROS、rqt、rvizrobot常用lib2D/3D仿真器传感器 完整安装 ros-melodic-desktop ROS、rqt、rvizrobot常用lib ros-melodic-ros-base 不包含GUI ros-melodic-PACKAGE 单个模块 PACKAGE表示模块名，如ros-melodic-slam-gmapping *备注：RViz和rqt均是ROS的可视化工具， 3. 初始化ROS12345678# 初始化rosdep仓库sudo rosdep initrosdep update# 配置环境变量echo \"source /opt/ros/melodic/setup.bash\" &gt;&gt; ~/.bashrcsource ~/.bashrc# 下载依赖sudo apt install python-rosinstall python-rosinstall-generator python-wstool build-essential 参考ROS安装 ROS Tutorials","categories":[{"name":"机器人","slug":"机器人","permalink":"https://linqing2017.github.io/categories/%E6%9C%BA%E5%99%A8%E4%BA%BA/"}],"tags":[{"name":"ROS","slug":"ROS","permalink":"https://linqing2017.github.io/tags/ROS/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://linqing2017.github.io/tags/Ubuntu/"}]},{"title":"Prometheus Alarm","slug":"2-Prometheus","date":"2019-07-30T16:00:00.000Z","updated":"2020-05-14T07:59:03.277Z","comments":true,"path":"2019/07/31/2-Prometheus/","link":"","permalink":"https://linqing2017.github.io/2019/07/31/2-Prometheus/","excerpt":"Prometheus 调研","text":"Prometheus 调研 Prometheus告警告警能力在Prometheus的架构中被划分成两个独立的部分：告警规则(产生告警),告警处理。 告警规则：通过在Prometheus中定义AlertRule（告警规则），Prometheus会周期性的对告警规则进行计算，如果满足告警触发条件就会向Alertmanager发送告警信息。 告警处理： 告警规则一条告警规则，包括以下几个部分： 告警名称：告警规则命名 告警规则：主要由PromQL进行定义，其实际意义为当表达式（PromQL）查询结果持续多长时间（During）后出发告警 告警组：对一组相关的告警进行统一定义，并通过YAML文件来统一管理。 典型告警规则定义： 1234567891011groups:- name: example rules: - alert: HighErrorRate expr: job:request_latency_seconds:mean5m&#123;job=\"myjob\"&#125; &gt; 0.5 # 对Prometheus来说，产生的告警条数由PromQL返回的调数决定，返回几个采样值对应就产生几个告警。 for: 10m labels: severity: page # 附加到告警信息上的标签，可以添加额外的标签 annotations: summary: High request latency #描述告警的概要信息 description: description info #描述告警的详细信息 上述告警规则中，summary和description的值可以使用模板化的值，如$labels.&lt;labelname&gt;变量可以访问当前告警实例中指定标签的值，$value则可以获取当前PromQL表达式计算的样本值 在Prometheus全局配置文件中通过rule_files指定一组告警规则文件的访问路径，Prometheus启动后会自动扫描这些路径下规则文件中定义的内容，并且根据这些规则计算是否向外部发送通知。默认情况下Prometheus会每分钟对这些告警规则进行计算。 1234rule_files: [ - &lt;filepath_glob&gt; ... ]global: [ evaluation_interval: &lt;duration&gt; | default = 1m ] AlertmanagerAlertmanager是一个独立的组件，负责接收并处理来自Prometheus Server(也可以是其它的客户端程序)的告警信息。Alertmanager的安装部署和Prometheus类似，只包含一个可执行文件，以及对应的配置文件alertmanager.yml。 alertmanager.yml包括：global、templates、route、receivers、inhibit_rules这几个部分，如下： 12345678910111213global: #用于定义一些全局的公共参数，如全局的SMTP配置，Slack配置等内容； [ resolve_timeout: &lt;duration&gt; | default = 5m ] # 该参数表示持续多长时间未接收到告警后标记告警状态为resolved templates: # 定义告警通知时的模板，如HTML模板，邮件模板等； [ - &lt;filepath&gt; ... ]route: &lt;route&gt; # 告警路由配置receivers: # 告警接收者，即邮箱、微信等 - &lt;receiver&gt; ...inhibit_rules: # 告警的抑制规则 [ - &lt;inhibit_rule&gt; ... ] Router配置上面配置文件中最关键的配置项是route和receivers，两者一一对应构成告警信息的路由拓扑网络。 所有告警在Alertmanager中从根route进入，并匹配当前节点的子route，直到找到一个最深的匹配点，并将告警信息发送个该匹配点的receivers。 Alertmanager可以对告警通知进行分组，将多条告警合合并为一个。配置文件中可以使用group_by来定义分组规则，基于告警中包含的标签，如果满足group_by中定义标签名称，那么这些告警将会合并为一个通知发送给接收器。 route 的配置模板如下： 12345678910111213141516receiver: &lt;string&gt; # 当前route节点的receivers group_by: '[' &lt;labelname&gt;, ... ']' ] # 告警分组，依据选择的label，将label取值相同的告警合并[ continue: &lt;boolean&gt; | default = false ] # 告警在匹配到第一个子节点是否停止match: # 通过label值判断匹配 [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ]match_re: # 通过label值的re表达判断匹配 [ &lt;labelname&gt;: &lt;regex&gt;, ... ][ group_wait: &lt;duration&gt; | default = 30s ] # 如果在等待时间内当前group接收到了新的告警，这些告警将会合并为一个通知向receiver发送。[ group_interval: &lt;duration&gt; | default = 5m ] # 定义相同的Gourp之间发送告警通知的时间间隔[ repeat_interval: &lt;duration&gt; | default = 4h ] # 表示重复发送告警的时间routes: [ - &lt;route&gt; ... ] Receiver配置当前官方内置的第三方receiver包括：邮件、即时通讯软件（如Slack、Hipchat）、移动应用消息推送(如Pushover)和自动化运维工具（例如：Pagerduty、Opsgenie、Victorops）以及Webhook。 下面的配置信息，展示了Alertmanager对接了企业微信的配置： 1234567891011121314151617181920# 是否接受告警已处理消息消息[ send_resolved: &lt;boolean&gt; | default = false ]# 企业微信使用的api_secret[ api_secret: &lt;secret&gt; | default = global.wechat_api_secret ]# 企业微信的api_url地址[ api_url: &lt;string&gt; | default = global.wechat_api_url ]# 企业微信企业id[ corp_id: &lt;string&gt; | default = global.wechat_api_corp_id ]# 告警信息文本[ message: &lt;tmpl_string&gt; | default = '&#123;&#123; template \"wechat.default.message\" . &#125;&#125;' ]# 告警应用id[ agent_id: &lt;string&gt; | default = '&#123;&#123; template \"wechat.default.agent_id\" . &#125;&#125;' ]# 接收告警的用户、组织、tag（三选一即可）[ to_user: &lt;string&gt; | default = '&#123;&#123; template \"wechat.default.to_user\" . &#125;&#125;' ][ to_party: &lt;string&gt; | default = '&#123;&#123; template \"wechat.default.to_party\" . &#125;&#125;' ][ to_tag: &lt;string&gt; | default = '&#123;&#123; template \"wechat.default.to_tag\" . &#125;&#125;' ] 告警抑制告警抑制配置模板如下，当满足一下三个条件时： 已发送的告警匹配到target_match和target_match_re规则 新的告警匹配到source_match或者source_match_re规则 发送的告警与新产生的告警中equal定义的标签完全相同 1234567891011target_match: [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ]target_match_re: [ &lt;labelname&gt;: &lt;regex&gt;, ... ]source_match: [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ]source_match_re: [ &lt;labelname&gt;: &lt;regex&gt;, ... ][ equal: '[' &lt;labelname&gt;, ... ']' ] 通过Alertmanager的UI临时屏蔽特定的告警通知。 通过定义标签的匹配规则(字符串或者正则表达式)，如果新的告警通知满足静默规则的设置，则不停止向receiver发送通知。 临时静默可以配置持续时间 配置实例alertmanager对接wechat配置文件 1234567891011121314151617181920212223242526272829303132global: resolve_timeout: 10m wechat_api_url: 'https://qyapi.weixin.qq.com/cgi-bin/' wechat_api_corp_id: 'xxxx'templates:- '/opt/alertmanager/templates/*.tmpl'inhibit_rules:- source_match:route: receiver: 'wechat' group_by: ['alertname'] group_wait: 30s group_interval: 5m repeat_interval: 12h routes: - receiver: 'wechat' group_by: ['alertname','cluster'] match: job: idatareceivers:- name: 'wechat' wechat_configs: - send_resolved: false to_user: 'LinQing' message: '&#123;&#123; template \"wechat.default.message\" . &#125;&#125;' agent_id: '1000002' api_secret: 'xxxx' 微信告警模板 12345678910111213141516171819202122232425262728&#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 -&#125;&#125;告警类型: &#123;&#123; $alert.Labels.alertname &#125;&#125;=====================&#123;&#123; end &#125;&#125;故障时间: &#123;&#123; $alert.StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;&#123;&#123; if gt (len $alert.Labels.instance) 0 &#125;&#125;instance: &#123;&#123; $alert.Labels.instance &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; if gt (len $alert.Labels.cluster) 0 &#125;&#125;cluster: &#123;&#123; $alert.Labels.cluster &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; if gt (len $alert.Labels.node) 0 &#125;&#125;node: &#123;&#123; $alert.Labels.node &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 -&#125;&#125;告警类型: &#123;&#123; $alert.Labels.alertname &#125;&#125;=====================&#123;&#123; end &#125;&#125;故障时间: &#123;&#123; $alert.StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;恢复时间: &#123;&#123; $alert.EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;&#123;&#123; if gt (len $alert.Labels.instance) 0 &#125;&#125;instance: &#123;&#123; $alert.Labels.instance &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; if gt (len $alert.Labels.cluster) 0 &#125;&#125;cluster: &#123;&#123; $alert.Labels.cluster &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; if gt (len $alert.Labels.node) 0 &#125;&#125;node: &#123;&#123; $alert.Labels.node &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125;&#123;&#123; end &#125;&#125; Recoding Rules优化通过PromQL可以实时对Prometheus中采集到的样本数据进行查询，聚合以及其它各种运算操作。 而在某些PromQL较为复杂且计算量较大时，直接使用PromQL可能会导致Prometheus响应超时的情况。 这时需要一种能够类似于后台批处理的机制能够在后台完成这些复杂运算的计算，对于使用者而言只需要查询这些运算结果即可。 Prometheus通过Recoding Rule规则支持这种后台计算的方式，可以实现对复杂查询的性能优化，提高查询效率。","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://linqing2017.github.io/tags/Prometheus/"}]},{"title":"Prometheus API","slug":"1-Prometheus","date":"2019-07-22T16:00:00.000Z","updated":"2020-05-14T07:59:03.237Z","comments":true,"path":"2019/07/23/1-Prometheus/","link":"","permalink":"https://linqing2017.github.io/2019/07/23/1-Prometheus/","excerpt":"Prometheus 调研","text":"Prometheus 调研 Client API核心对象： Collector：收集器，根据Metrics的类型 CollectorRegistry：Collector在其中进行注册 1234567891011121314151617181920212223242526272829#!/usr/bin/env python# -*- coding: utf-8 -*-from prometheus_client import start_http_server, Summaryimport randomimport time\"\"\"创建一个Summary类型的指标，Summary是一个Collector对象; 1. 第一个参数是metric的name 2. 第二个参数是metric的Help信息 3. 第三个参数是指标的label名称\"\"\"REQUEST_TIME = Summary('request_processing_seconds', 'Time spent processing request', [\"program\"])REQUEST_TIME_WITH_LABEL = REQUEST_TIME.labels(program=\"Test\") # 在指定的label上赋值# 通过装饰器的方式对process_request的执行时间进行采样@REQUEST_TIME_WITH_LABEL.time()def process_request(t): \"\"\"A dummy function that takes some time.\"\"\" time.sleep(t)if __name__ == '__main__': # 创建http服务，将metric暴露给采集器 start_http_server(6789) # Generate some requests. while True: process_request(random.random()) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/usr/bin/env python# -*- coding: utf-8 -*-from prometheus_client import start_http_server, Gauge, CollectorRegistry, push_to_gateway# 监控指标需要添加的LabelLABELS = &#123;&#125;# 需要进行监控的指标SERVICE_MAP = &#123;&#125;# Metrics_MAPMetrics_MAP = &#123;&#125;Registry = CollectorRegistry()def init_metric(): \"\"\" 初始化量测值 :return: \"\"\" for key in SERVICE_MAP.keys(): gauge_metric = Gauge(\"memory_usage_\" + key.lower(), key + \" service use memory\", set(LABELS.keys()),registry=Registry) Metrics_MAP.update(&#123;key: gauge_metric&#125;)def update_metric(sample): \"\"\" 更新测量值 :param sample: :return: \"\"\" for key in sample.keys(): gauge_metric = Metrics_MAP.get(key) gauge_metric.labels(**LABELS).set(sample[key])if __name__ == \"__main__\": init_metric() if exporter_config.get(\"http_endpoint\"): # 创建一个http服务暴露采样信息 start_http_server(exporter_config.get(\"http_port\"), registry=Registry) while True: sample = metric() # metric()获取采样值 update_metric(sample=sample) time.sleep(30) elif exporter_config.get(\"push\"): # Gateway发送采样信息 push_to_gateway(exporter_config.get(\"gateway\"), job=exporter_config.get(\"job\"), grouping_key=LABELS, registry=Registry) else: exit(-1) Push GatewayPush Gateway的主要用于Batch Job、网络隔离等场景的数据采集。 Gateway不会Cache目标的采样数据，只是将即时数据暴露给prometheus，可以将最近一次的采样数据保存到文件中，用来在Gateway重启时恢复数据。 对于一些分布式计数的需求GateWay无法实现（可以使用weaveworks/prom-aggregation-gateway）。 Gateway 没有实现TimeOut或者TTL机制。 Gateway 中采样的URL地址和label地址相关，格式为: 1&#123;IP&#125;:&#123;Port&#125;/job/&#123;job_name&#125;/instance/&#123;instance_value&#125;/&#123;label_1&#125;/&#123;value_1&#125;/... label相同的采样被合并为一个Group。 timestamps使用Gateway时，metrics中的timestamps会产生歧义，即：Client推送到Gateway的时间和Prometheus从Gateway获得sample的时间。 当Prometheus超过5min无法从target获取采样数据或者得到新采样（只时间变动的采样）时，认为target出现故障或者不存在。为了避免上述问题，Prometheus从Gateway得到的采样信息，以实际抓取的时间为timestamp，而push时间被保存在一个独立的metric中（push_time_seconds ）。 Job 和 instance 标签当prometheus从gateway中获取数据时会将sample中的job和instance标签配置为Gateway服务对应的值，因此需要在配置honor_labels为true。 123456789101112131415161718# 使用curl发送采样到gateway# 发送一个untyped类型的采样，对应到的Group为&#123;job=\"some_job\"&#125;echo \"some_metric 3.14\" | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job# 发送两个采样到gateway，Group为&#123;job=\"some_job\",instance=\"some_instance\"&#125; cat &lt;&lt;EOF | curl --data-binary @- http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance # TYPE some_metric counter some_metric&#123;label=\"val1\"&#125; 42 # TYPE another_metric gauge # HELP another_metric Just an example. another_metric 2398.283 EOF# 删除指定groupcurl -X DELETE http://pushgateway.example.org:9091/metrics/job/some_job/instance/some_instance 参考Pushgateway Java API Pushgateway Python API Pushgateway Go API Pushgateway Ruby API","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://linqing2017.github.io/tags/Prometheus/"}]},{"title":"数据库调研笔记 -- Prometheus","slug":"0-Prometheus","date":"2019-07-15T16:00:00.000Z","updated":"2020-05-14T07:59:03.204Z","comments":true,"path":"2019/07/16/0-Prometheus/","link":"","permalink":"https://linqing2017.github.io/2019/07/16/0-Prometheus/","excerpt":"数据库调研笔记 – Prometheus","text":"数据库调研笔记 – Prometheus PrometheusPrometheus 是SoundCloud公司开源的一款监控框架，提供包括：告警工具、时序数据库、图表展示等功能，主要包括以下Feature： 支持PromQL来查询数据 支持多维时间序列数据存储 集群不依赖共享存储，单个服务器节点是自治的 支持Pull、Push（通过gateway组件）两种方式获取数据 支持动态发现的模式配置服务 高效：单一实例可以处理百万个监控指标、每秒处理数十万的数据点 Prometheus主要包括： Prometheus server：提供TSD能力，以及Pull数据的能力，以及基于PromQL的查询 client libraries：多种语言的客户端接口 Push gateway：数据网关，服务将数据Push到gateway，Server从gateway pull数据 exporters：Exporter将监控数据采集的端点通过HTTP服务的形式暴露给Prometheus Server。一些符本身及支持Prometheus，因此这些服务天生就是exporters，此外还有的exporters以插件的形式被安装到client端。 alertmanager：告警模块 概念Data ModelPrometheus的数据模型基于time series， 由一个Metric名称和若干labels构成，表示为: 1&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;** Prometheus 基于time series存储数据，并且可能会因查询而生成临时派生时间序列。 Time-SeriesTime-Series由若干样本组成，每个Sample包含以下三个部分： 指标（metric）：metric name + 描述当前样本特征的labelsets 时间戳（timestamp）：一个精确到毫秒的时间戳 样本值（value）：一个folat64的浮点型数据表示当前样本的值 以下几点可以注意： metric name实际上是一个特殊的label，名称为name，以“__”作为前缀的标签是系统内置标签 形式上所有metric格式为：&lt;\\metric name&gt;{&lt;label name&gt;=&lt;label value&gt;, …} 一个Time-Series包含多个metric Metric TypesPrometheus当前只支持四种数据类型，并且在不同的API中稍有区别： Counter：只增不减的计数器，只能在重启时将其设置为0 Gauge：常规测量值类型 Histogram：直方图统计类型 Summary：类似Histogram，区别参考 Histogram 和 Summary 本质上都是统计指标，不会保存原始数据。 HistogramHistogram 指标对每个采样点进行统计，并根据bucket将数据划分到不同区域。 假设bucket配置为[1,5,10], Prometheus观察到的数据被划分到[0,1],[1,5],[5,10]，[5,+inf] 四个不同的区间。 对每个Histogram 可以获取以下信息： 12345[basename]_bucket&#123;le=“1”&#125; # 观测小于1的次数[basename]_bucket&#123;le=“5”&#125; # 观测小于5的次数[basename]_bucket&#123;le=“10”&#125; # 观测小于10的次数[basename]_count # 所有观测值的个数[basename]_sum # 所有观测值之和 Prometheus 只会存储每个区间内观测值的个数，并且histogram_quantile使用分段线性近似的方式绘制分布曲线。 summarysummary是精度更高的直方图测量，可以获取以下内容： 1231. [basename]&#123;quantile=&quot;φ&quot;&#125; (0 ≤ φ ≤ 1) # 获取百分之φ数据的最小值，基于第三方库perk2. [basename]_sum #指所有观察值的总和3. [basename]_count #指已观察到的事件计数值 JOBS AND INSTANCES Instances 表示能够pull到数据的一个endpoint通过 Host:Port 表示，多个类型一致 Instances 组成 job。 在Prometheus拉取数据时，会自动为time series添加job和instance的名称到labels，如下： 12345job: api-server instance 1: 1.2.3.4:5670 instance 2: 1.2.3.4:5671 instance 3: 5.6.7.8:5670 instance 4: 5.6.7.8:5671 安装官方提供了包括：二进制安装包、Docker、Ansible等多种安装方式。 Exporters Prometheus 使用命令行和配置文件指定服务的配置项，其中： 命令行参数：指定数据目录、memory配置等服务配置，使用./prometheus -h可以查看相关配置。 配置文件：配置文件基于yaml格式，用于指定jobs、instances、rules，启动时通过–config.file指定配置文件，官方提供配置项有详细说明，以及模板。 Prometheus支持动态刷新配置，用户可以通过向进程发送 SIGHUP 或者 HTTP POST 方式重载配置项。 1234# 执行配置文件动态刷新kill -HUP &#123;pid&#125;curl -X POST http://localhost:9090/-/reload 比较关键的启动配置包括： storage.tsdb.path：决定数据目录的存储位置 config.file：读取的配置文件位置 QuickStart官方网站的QuickStart展示了使用Node Exporter采集主机数据的例子。 Node Exporter是Golang编写，主机系统测量值采集工具，不存在任何的第三方依赖，只需要下载后即可直接运行。 主机启动Node Exporter命令后，会在 9100 端口暴露采集到的监控数据，每次metrics请求会返回多个指标，其格式如下： 12345678910#PS:每个指标的开头包含HELP和TYPE两行，HELP解释指标的内容、TYPE解释指标的类型#PS:每个指标&#123;&#125;中的内容是lable信息，# HELP node_cpu Seconds the cpus spent in each mode. # TYPE node_cpu counternode_cpu&#123;cpu=\"cpu0\",mode=\"idle\"&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 以下配置文件中，包含了两个job：prometheus是服务自身的监控，node是我们定义的主机运行情况监控，共包含三个instance。 重启prometheus后，在console页面执行up函数可以看到每个instance的运行情况（1表示正在运行）。 1234567891011121314151617global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s).# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'node' scrape_interval: 1s static_configs: - targets: ['bdnode1:9100','bdnode2:9100','bdnode3:9100'] PromQL查询时间序列查询时间序列的基本格式为： 1metric_name&#123;label_name OP label_value, ...&#125;[duration] offset offset time OP可以支持完全匹配、正则匹配，允许以下符号：=、!=、=、! duration表示访问指定时间范围的数据，尺度为s、m、h、d、w、y offset表示从当前时间向前位移 聚合、操作符运算Prometheus支持对序列进行聚合、以及运算符操作。 运算符操作运算符操作包括：数学运算符，逻辑运算符，布尔运算符。 运算符的操作对象包括：序列、标量 数学运算符：+、-、*、/、%、^ 布尔运算符：=、！=、&gt;、&lt;、&gt;=、&lt;= 将过滤掉不满足条件的采样，这些运算符用bool修饰之后，不会进行采样过滤，而是返回0或1 逻辑运算符：and、or、unless 根据情况产生两个时间序列的交集、并集、补集 需要注意的是： 操作符之间存在优先级 序列的运算遵循匹配规则： 标签完全一致的元素之间进行计算，没找到匹配元素，则直接丢弃。 通过on、ignoreing可以限定匹配的标签、或者忽略特定标签。 当出现一对多、一对一匹配时，可以通过group运算符使唯一值被匹配多次。 聚合函数基本用法： 1234&lt;aggr-op&gt;([parameter,] &lt;vector expression&gt;) [without|by (&lt;label list&gt;)]# without：表示排除特定标签# by：只考虑特定标签 支持的聚合函数包括： sum (求和) min (最小值) max (最大值) avg (平均值) stddev (标准差) stdvar (标准差异) count (计数) count_values (对value进行计数) bottomk (后n条时序) topk (前n条时序) quantile (分布统计) 其他内置函数 increase(v range-vector)：返回区间的增长量 rate(v range-vector)：返回区间的平均增长率 irate(v range-vector)：返回区间的瞬时增长率 predict_linear(v range-vector, t scalar)：基于简单线性回归，预测序列在t秒后的值 histogram_quantile(0.5, http_request_duration_seconds_bucket)：计算histogram的分位数值 label_replace：动态替换标签 例子123456&#123;instance=\"bdnode3:9100\"&#125; # 返回节点3的所有量测信息node_hwmon_temp_celsius # 查询所有CPU核心的温度node_hwmon_temp_celsius&#123;chip=\"platform_coretemp_0\"&#125; #所有节点CPU0上的核心温度node_hwmon_temp_celsius&#123;chip=\"platform_coretemp_0\",instance=\"bdnode3:9100\"&#125;[10s] offset 30s # 之前40s到30s的温度数据avg(node_hwmon_temp_celsius&#123;chip=~\"platform_coretemp_[0-9]*\" &#125;) by (instance) #查询每个节点CPU的平均温度avg(node_hwmon_temp_celsius&#123;chip=~\"platform_coretemp_[0-9]*\" &#125;) by (instance,chip) #查询每个节点的所有CPU的平均温度 存储本地存储对于本地存储，Prometheus 2.x 采用自定义的存储格式将样本数据保存在本地磁盘当中。 Prometheus按照两个小时为一个时间窗口，将两小时内产生的数据存储在一个块(Block)中，每一个块中包含该时间窗口内的所有样本数据(chunks)，元数据文件(meta.json)以及索引文件(index)。 写入数据时，Prometheus先写内存，并且通过WAL进行重播进行数据会，API对数据删除同样通过tombstone进行标记删除。 Prometheus或周期合并时间窗内的数据，并删除垃圾数据。 远程存储远程存储的目的是要使Prometheus存储大量历史数据、以及进行灵活扩展和迁移。 通过remote_write/remote_read，Prometheus可以将数据存放到其他存储服务中 远程存储是一种附加功能（可以和本地存储同时工作），部分存储服务Prometheus只支持写入，不支持读取，作为一种长期数据备份工具。 当前支持的远程存储包括： Storage 操作 备注 Cortex 读写 这个项目专门为Prometheus提供可扩展的长期存储，并且是 CNCF 的孵化项目github上有1500+的star M3DB 读写 Uber开源的分布式时序数据库,github上有两千多个stars，支持PromQL和Prometheus本身有非常好的兼容性 thanos 写入 专门为Prometheus设计的集群方案，提供了全局查询、降准采样等能力、以及数据转存到对象存储的能力。Github上有4000+star VictoriaMetrics 写 开源分布式时序数据库，支持 PromQL 接口、号称相比TimeScale和Influxdb相比有20以上的性能 Chronix 写入 基于Lucene的一款时序数据库，支持单机集群方式部署。单机模式基于纯Lucene实现可以嵌入到应用程序中，集群模式基于solr实现分布式能力。同时该项目有Spark接口。 CrateDB 读写 分布式数据库，主要的应用场景是IoT场景（智能工厂、智能驾驶等等），涵盖了一整套IoT场景的平台方案（平台不是开源的）。 irondb 读写 一个连官网都没有、Github上只有五个Star的keyvalue数据库 SignalFx 写 SignalFx本身也是一个监控平台 Wavefront 写入 VMware的企业级监控平台 Splunk 读写 商业日志分析工具 AppOptics 写入 商业APM工具，用在应用性能监控场景 Gnocchi 写入 从OpenStack的孵化出的项目，本质上是一个中间件，用来将metric数据进行时序封装，并写入到后端存储中。 graphite 写入 时序数据库、图形渲染层。这个工具只被动收集数据，并且图形渲染能力较弱一般和Grafana配合使用。 其他开源组件存储包括：Elasticsearch、InfluxDB、Kafka、OpenTSDB、TiKV、PostgreSQLTimescaleDB 集群模式目前，Prometheus并没有一套非常完整的集群方案，仅能够通过联邦的方式解决部分问题。 当前可以采用的 基本HA方式：部署多套Prometheus，并且采集相同的Server实例。这种方式只能解决服务可用性的问题，无法解决一致性、故障恢复、动态扩展等问题，只适合：小集群、不进行迁移的场景。 基本HA方式 + 远程存储：Promthus Server将数据保存在远程服务上，并且部署多个实例，用NGINX路由请求。 基本HA + 远程存储 + 联邦集群 其他集群方案，如thanos 常用管理员命令123456789# 删除数据curl -X POST \\ -g 'http://172.24.33.31:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;job=\"node\"&#125;'# 触发压缩操作curl -XPOST http://localhost:9090/api/v1/admin/tsdb/clean_tombstones# 重载配置文件kill -HUP &#123;pid&#125; 参考官方文档 Prometheus-book FUNCTIONS HTTP API","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://linqing2017.github.io/tags/Prometheus/"}]},{"title":"Waterdrop","slug":"waterdrop","date":"2019-06-30T16:00:00.000Z","updated":"2020-05-14T07:59:03.315Z","comments":false,"path":"2019/07/01/waterdrop/","link":"","permalink":"https://linqing2017.github.io/2019/07/01/waterdrop/","excerpt":"关于ETL工具，已经南北向接口工具的调研","text":"关于ETL工具，已经南北向接口工具的调研 WaterdropWaterdrop 是一款开源ETL工具，两个作者分别供职于新浪和一下科技。当前该项目在github上的有将近500个Star，并且在新浪、水滴筹、永辉等实际生产环境上有相关应用。 Waterdrop包括以下特点： 架构简单，其工作流只包含：Input、Filter、Output三个部分。每个部分都是基于 Spark 或者 Flink 现成代码段。部署Waterdrop应用时，通过配置文件自定义Pipline，并提交到Spark集群上运行。 包含Spark和Flink两个版本。 方便易用，包含众多现成plugin，并可以根据实际业务进行定制。 核心逻辑Row：Row 是Waterdrop逻辑意义上一条数据，是数据处理的基本单位。在Filter处理数据时，所有的数据都会被映射为Row。在代码实现中，数据实际上被组织成Dataset[Row] Field：Field是Row中的一个字段，Row可以包含嵌套层级的字段，其中，raw_message 指从Input获得的原始数据。Row中最顶级的字段层级用root表示。 配置文件示例在Waterdrop中数据的流向由配置文件决定。 Waterdrop只支持非常简单的pipline拓扑，其中filter只支持串行，Input到filter支持多个Input扇入，filter到Output支持多个Output扇出 1234567891011121314151617181920# 该部分用于进行Spark参数的配置spark &#123; ...&#125;# 配置input插件input &#123; ...&#125;# 串行配置多个filter插件filter &#123; ...&#125;# 配置多个output插件output &#123; ...&#125; 插件Input 支持File、HDFS、S3上的文件流读取，支持Orc、Parquet、XML等格式； 支持 KafkaStream、ElasticSearch； 支持JDBC、Kudu、Mongdb、MySQL、Hive、Tidb等数据库表数据的抽取； 支持SocketStream； OutPut 支持Clickhouse、JDBC、Kudu、MongoDB、MySQL、Opentsdb、Tidb 支持File、HDFS、S3 支持Kafka、ES 支持Stdout Filter插件 解析固定格式的记录生成一张表 json Grok KV（解析URL中的KV参数） SQL类操作 join(inner join两张表) SQL（执行一句SQL） Table：将文件映射成一张表 对记录中字段的操作 Checksum（计算字段校验码） Convert（字段类型转换） Date（解析时间格式） Lowercase/Uppercase（将指定字段内容全部转换为小/大写字母） Replace（字段正则替换） split（分割一个字段为多个字段） Truncate（截断字段） Script操作（基于QLExpress执行指定脚本逻辑。脚本接收一个指定JSONObject（可以认为是一条记录）, 完成自定义的处理逻辑，再返回一个新的event（记录）。 对记录的操作： Add（添加字段） Remove（删除字段） Drop（删除行） Rename（重命名字段） Sample（数据采样） Uuid：为每行记录添加一个全局唯一的UUID Repartition：调整数据分区数 Watermark：Spark Structured Streaming Watermark 总结 功能过于简单，只能完成一些数据导入导出工作。 与CDH-6.1.0平台存在jar包冲突，需要重新编译才能在CDH-6.1.0上工作。 缺乏HA能力； 缺少监控功能（有监控组件，但是收费版本的功能）； 参考官方网站","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"ETL","slug":"ETL","permalink":"https://linqing2017.github.io/tags/ETL/"},{"name":"南北向接口工具","slug":"南北向接口工具","permalink":"https://linqing2017.github.io/tags/%E5%8D%97%E5%8C%97%E5%90%91%E6%8E%A5%E5%8F%A3%E5%B7%A5%E5%85%B7/"}]},{"title":"数据库调研笔记 -- ClickHouse之SQL语法","slug":"ClickHouse-2","date":"2019-06-18T16:00:00.000Z","updated":"2020-05-14T07:59:03.282Z","comments":true,"path":"2019/06/19/ClickHouse-2/","link":"","permalink":"https://linqing2017.github.io/2019/06/19/ClickHouse-2/","excerpt":"数据库调研笔记 – ClickHouse","text":"数据库调研笔记 – ClickHouse SQL语法下面的内容简单说明Clickhouse中SQL和标准SQL的差别，以及他的特点： SELECT如果查询中不包含DISTINCT，GROUP BY，ORDER BY子句以及IN和JOIN子查询，那么它将仅使用O(1)数量的内存来完全流式的处理查询。对于其他高内存消耗的查询，可以使用参数控制内存，并且通过磁盘进行外部排序。 From语句From 之后可以是表、子查询、表函数、ARRAY JOIN子句、JOIN子句。子查询无需指定别名，即使指定了也会被忽略。 SAMPLE语句MergeTree表中可以使用SAMPLE来进行采样，并且系统在不同的时间，不同的服务器，不同表上总以相同的方式对数据进行采样。 12345678910111213141516SAMPLE k # 0&lt;k&lt;1 表示采样百分比，k&gt;=1 表示采样条数#例子SELECT Title, count() * 10 AS PageViewsFROM hits_distributedSAMPLE 0.1WHERE CounterID = 34 AND toDate(EventDate) &gt;= toDate('2013-01-29') AND toDate(EventDate) &lt;= toDate('2013-02-04') AND NOT DontCountHits AND NOT Refresh AND Title != ''GROUP BY TitleORDER BY PageViews DESC LIMIT 1000 ARRAY JOIN 子句ARRAY JOIN可以进行与数组（nested数据类型）的连接。 假设存在下面的表： 1CREATE TABLE arrays_test (s String, arr Array(UInt8)) ENGINE = Memory s arr Hello [1,2] World [3,4,5] Goodbye [] 使用Array Join将arr列展开 1SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a s arr 1 Hello [1,2] 1 Hello [1,2] 2 World [3,4,5] 3 World [3,4,5] 4 World [3,4,5] 5 Array Join可以使用在nested数据结构上： 1234567CREATE TABLE nested_test (s String, nest Nested(x UInt8, y UInt32)) ENGINE = MemoryINSERT INTO nested_test VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], [])SELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nestSELECT s, nest.x, nest.y FROM nested_test ARRAY JOIN nest.x Array Join的其他特点： 当多个具有相同大小的数组使用逗号分割出现在ARRAY JOIN子句中时，ARRAY JOIN会将它们同时执行（直接合并，而不是它们的笛卡尔积）。假设有2个数组，数组长度为2，ARRAY JOIN的结果依然是2行。 在一个查询中只能出现一个ARRAY JOIN子句。 如果在WHERE/PREWHERE子句中使用了ARRAY JOIN子句的结果，它将优先于WHERE/PREWHERE子句执行，否则它将在WHERE/PRWHERE子句之后执行，以便减少计算。 Join123SELECT &lt;expr_list&gt; FROM &lt;left_subquery&gt;[GLOBAL] [ANY|ALL] INNER|LEFT|RIGHT|FULL|CROSS [OUTER] JOIN &lt;right_subquery&gt;(ON &lt;expr_list&gt;)|(USING &lt;column_list&gt;) ... ANY 和 ALL 的区别：ANY表示存在多个与左表关联的数据，那么系统仅返回第一个与左表匹配的结果，即结果和左边行数一致。ALL表示返回全部结果，这和标准SQL一致。默认情况下是ALL。 GLOBAL … JOIN：当使用普通的JOIN时，查询将被发送给远程的服务器。并在这些远程服务器上生成右表并与它们关联。换句话说，右表来自于各个服务器本身。 当使用GLOBAL … JOIN，首先会在请求服务器上计算右表并以临时表的方式将其发送到所有服务器。这时每台服务器将直接使用它进行计算。 Join的其他限制： 右表（子查询的结果）将会保存在内存中。如果没有足够的内存，则无法运行JOIN。 只能在查询中指定一个JOIN。若要运行多个JOIN，你可以将它们放入子查询中。 每次运行相同的JOIN查询，总是会再次计算 - 没有缓存结果。 在各种类型的JOIN中，最高效的是ANY LEFT JOIN，然后是ANY INNER JOIN，效率最差的是ALL LEFT JOIN以及ALL INNER JOIN。 推荐你使用子查询的方式执行JOIN。 推荐你使用子查询的方式执行JOIN。123456789101112131415161718192021222324SELECT CounterID, hits, visitsFROM( SELECT CounterID, count() AS hits FROM test.hits GROUP BY CounterID) ANY LEFT JOIN( SELECT CounterID, sum(Sign) AS visits FROM test.visits GROUP BY CounterID) USING CounterIDORDER BY hits DESCLIMIT 10# USING子句用于指定要进行链接的一个或多个列，系统会将这些列在两张表中相等的值连接起来。如果列是一个列表，不需要使用括号包裹。同时JOIN不支持其他更复杂的Join方式。 WHERE PREWHERE语句与WHERE子句的意思相同。主要的不同之处在于表数据的读取。 当使用PREWHERE时，首先只读取PREWHERE表达式中需要的列。然后在根据PREWHERE执行的结果读取其他需要的。 在一个查询中可以同时指定PREWHERE和WHERE，在这种情况下，PREWHERE优先于WHERE执行。 如果将’optimize_move_to_prewhere’设置为1，并且在查询中不包含PREWHERE，则系统将自动的把适合PREWHERE表达式的部分从WHERE中抽离到PREWHERE中。 GROUP BY MySQL不同的是（实际上这是符合SQL标准的），SELECT、HAVING、ORDER BY不能够获得一个不在GROUP BY中的非聚合函数列（除了常量表达式）。但是可以使用‘any’（返回遇到的第一个值）、max、min等聚合函数使它工作。 如果查询表达式列表中仅包含聚合函数，则可以省略GROUP BY子句，这时会假定将所有数据聚合成一组空“key”。 12345SELECT count(), median(FetchTiming &gt; 60 ? 60 : FetchTiming), count() - sum(Refresh)FROM hits 对于GROUP BY子句，ClickHouse将 NULL 解释为一个值，并且支持NULL=NULL。 GROUP BY中允许将临时数据转存到磁盘上，以限制对内存的使用。 max_bytes_before_external_group_by这个配置确定了在GROUP BY中启动将临时数据转存到磁盘上的内存阈值。如果你将它设置为0（这是默认值），这项功能将被禁用。如果有临时数据被刷到了磁盘中，那么这个查询的运行时间将会被延长几倍（大约是3倍）。 在分布式查询处理中，外部聚合将会在远程的服务器中执行。为了使请求服务器只使用较少的内存，可以设置distributed_aggregation_memory_efficient为1。 LIMIT N BY 子句LIMIT N BY COLUMNS 子句可以用来在每一个COLUMNS分组中求得最大的N行数据。 123456789101112SELECT domainWithoutWWW(URL) AS domain, domainWithoutWWW(REFERRER_URL) AS referrer, device_type, count() cntFROM hitsGROUP BY domain, referrer, device_typeORDER BY cnt DESCLIMIT 5 BY domain, device_typeLIMIT 100# 查询将会为每个domain, device_type的组合选出前5个访问最多的数据，但是结果最多将不超过100行（LIMIT n BY + LIMIT）。 HAVING 子句 HAVING子句可以用来过滤GROUP BY之后的数据，类似于WHERE子句。 同样，HAVING子句中的条件必须出现在Group by中。 WHERE于HAVING不同之处在于WHERE在聚合前(GROUP BY)执行，HAVING在聚合后执行。 如果不存在聚合，则不能使用HAVING。 ORDER BY 子句 表达式列表中每一个表达式都可以分配一个DESC或ASC（排序的方向）。如果没有指明排序的方向，将假定以ASC的方式进行排序。 对于字符串的排序来讲，你可以为其指定一个排序规则，在指定排序规则时，排序总是不会区分大小写。123#例如：ORDER BY SearchPhrase COLLATE 'tr' # 使用土耳其字母表对它进行升序排序，同时排序时不会区分大小写，并按照UTF-8字符集进行编码。 表达式中相同值的行将以任意的顺序进行输出 NaN 和 NULL 的排序规则： 当使用NULLS FIRST修饰符时，将会先输出NULL，然后是NaN，最后才是其他值。 当使用NULLS LAST修饰符时，将会先输出其他值，然后是NaN，最后才是NULL。 默认情况下与使用NULLS LAST修饰符相同。 当使用浮点类型的数值进行排序时，不管排序的顺序如何，NaNs总是出现在所有值的后面。(升序时NaNs最大，降序时最小)1SELECT * FROM t_null_nan ORDER BY y NULLS FIRST 如果你在ORDER BY子句后面存在LIMIT并给定了较小的数值，则将会使用较少的内存。否则，内存的使用量将与需要排序的数据成正比。 对于分布式查询，如果省略了GROUP BY，则在远程服务器上执行部分排序，最后在请求服务器上合并排序结果。这意味这对于分布式查询而言，要排序的数据量可以大于单台服务器的内存。 如果没有足够的内存，可以使用外部排序（在磁盘中创建一些临时文件）。可以使用max_bytes_before_external_sort来设置外部排序 DISTINCT 子句 如果存在DISTINCT子句，则会对结果中的完全相同的行进行去重。 当不存在ORDER BY子句并存在LIMIT子句时，查询将在同时满足DISTINCT与LIMIT的情况下立即停止查询。 可以与GROUP BY配合使用。 在处理数据的同时输出结果，并不是等待整个查询全部完成。 在SELECT表达式中存在Array类型的列时，不能使用DISTINCT。 DISTINCT可以与 NULL一起工作，就好像NULL仅是一个特殊的值一样，并且NULL=NULL。 LIMIT LIMIT N：选取前n行 LIMIT N,M：选取从低n行开始的m行数据 UNION ALL 子句 UNION ALL子句可以组合任意数量的查询 12345678910SELECT CounterID, 1 AS table, toInt64(count()) AS c FROM test.hits GROUP BY CounterIDUNION ALLSELECT CounterID, 2 AS table, sum(Sign) AS c FROM test.visits GROUP BY CounterID HAVING c &gt; 0 UNION ALL中的查询可以同时运行，它们的结果将被混合到一起。 查询的结果结果必须相同（列的数量和类型）。 如果两个查询中有相同的列，但是类型不同但是兼容，CK会自动进行转化。 列名可以是不同的。在这种情况下，最终结果的列名将从第一个查询中获取。 INTO OUTFILE 子句 INTO OUTFILE filename 子句用于将查询结果重定向输出到指定文件中 执行的结果文件将在客户端建立，如果文件已存在，查询将会失败。 IN 运算符 运算符的左侧是单列或列的元组。12SELECT UserID IN (123, 456) FROM ...SELECT (CounterID, UserID) IN ((34, 123), (101500, 456)) FROM ... 左侧是单个列并且是一个索引，并且右侧是一组常量时，系统将使用索引来处理查询。 右侧可以是一个表的名字，或者是一个子查询 123UserID IN usersUserID IN (SELECT * FROM users)SELECT (CounterID, UserID) IN (SELECT CounterID, UserID FROM ...) FROM ... 如果操作符的右侧是一个Set引擎的表时（数据总是在内存中准备好），则不会每次都为查询创建新的数据集。 IN操作符的左右两侧应具有相同的类型。 IN操作符的子查询中可以出现任意子句，包含聚合函数与lambda函数。 1234567891011SELECT EventDate, avg(UserID IN ( SELECT UserID FROM test.hits WHERE EventDate = toDate('2014-03-17') )) AS ratioFROM test.hitsGROUP BY EventDateORDER BY EventDate ASC IN操作符总是假定 NULL 值的操作结果总是等于0 在分布式表中使用IN或者Join: 1. 当使用普通的IN时，查询总是被发送到远程的服务器，并且在每个服务器中运行“IN”或“JOIN”子句中的子查询。 2. 当使用GLOBAL IN / GLOBAL JOIN时，首先会为GLOBAL IN / GLOBAL JOIN运行所有子查询，并将结果收集到临时表中，并将临时表发送到每个远程服务器，并使用该临时表运行查询。 因此，对于非分布式查询，请使用普通的IN ／ JOIN。在分布式查询中使用IN / JOIN子句中使用子查询需要根据实际情况。12345678910111213141516171819202122232425262728293031323334SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN ( SELECT UserID FROM local_table WHERE CounterID = 34 )# IN子句中的数据集将被在每台服务器上被独立的收集，仅与每台服务器上的本地存储上的数据计算交集。# 如果单个UserID的数据完全分布在单个服务器上，那么这将是正确且最佳的查询方式。# 当UserID的数据分布在不同的服务器上时：SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN ( SELECT UserID FROM distributed_table WHERE CounterID = 34 )# 子查询将在每个远程服务器上执行。# 因为子查询使用分布式表，所有每个远程服务器上的子查询将查询再次发送给所有的远程服务器# 例如，如果你拥有100台服务器的集群，执行整个查询将需要10，000次请求，这通常被认为是不可接受的。因此这里要使用GLOBAL IN来替代IN。SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN ( SELECT UserID FROM distributed_table WHERE CounterID = 34 ) INSERT INTOINSERT1INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ... 可以在查询中指定插入的列的列表，如：[(c1, c2, c3)]。对于存在于表结构中但不存在于插入列表中的列，它们将会按照如下方式填充数据： 如果存在DEFAULT表达式，根据DEFAULT表达式计算被填充的值。 如果没有定义DEFAULT表达式，则填充零或空字符串。 数据可以以ClickHouse支持的任何输入输出格式 传递给INSERT。 ClickHouse会清除数据前所有的空白字符与一行摘要信息 使用SELECT的结果写入 写入与SELECT的列的对应关系是使用位置来进行对应的,尽管它们在SELECT表达式与INSERT中的名称可能是不同的。 在进行INSERT时将会对写入的数据进行一些处理，按照主键排序，按照月份对数据进行分区等。所以如果在您的写入数据中包含多个月份的混合数据时： 数据总是以尽量大的batch进行写入，如每次写入100,000行。 数据在写入ClickHouse前预先的对数据进行分组。 CREATE创建数据库1CREATE DATABASE [IF NOT EXISTS] db_name 建表12345678910111213141516171819# 可以用三种方式指定默认值# # DEFAULT expr： 普通默认值，如果INSERT中不包含指定的列，那么将通过表达式计算它的默认值并填充它。# MATERIALIZED expr ： 物化表达式，被该表达式指定的列不能包含在INSERT的列表中，因为它总是被计算出来的。# ALIAS expr ： 这样的列不会存储在表中。 它的值不能够通过INSERT写入，同时使用SELECT查询星号时，这些列也不会被用来替换星号。 但是它们可以显示的用于SELECT中，在这种情况下，在查询分析中别名将被替换。#CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ...) ENGINE = engine# 创建一张和表2结构相同的表，CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]# 创建一张和SELEC结构相同的表，并且用SELECT填充CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE = engine AS SELECT ... 临时表ClickHouse支持临时表，其具有以下特征： 当回话结束时，临时表将随会话一起消失，这包含链接中断。 临时表仅能够使用Memory表引擎。 无法为临时表指定数据库。它是在数据库之外创建的。 如果临时表与另一个表名称相同，那么当在查询时没有显示的指定db的情况下，将优先使用临时- 表。 对于分布式处理，查询中使用的临时表将被传递到远程服务器。 CREATE VIEW支持普通视图和物化视图： 1CREATE [MATERIALIZED] VIEW [IF NOT EXISTS] [db.]table_name [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ... 普通视图不存储任何数据，只是执行从另一个表中的读取。换句话说，普通视图只是保存了视图的查询，当从视图中查询时，此查询被作为子查询用于替换FROM子句。 物化视图存储的数据是由相应的SELECT查询转换得来的。在创建物化视图时，你还必须指定表的引擎 - 将会使用这个表引擎存储数据。 物化视图只会包含在物化视图创建后的新写入的数据。(不指定POPULATE的情况) 目前对物化视图执行ALTER是不支持的，因此这可能是不方便的。 参考官方文档","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://linqing2017.github.io/tags/ClickHouse/"}]},{"title":"数据库调研笔记 -- ClickHouse","slug":"ClickHouse-1","date":"2019-06-09T16:00:00.000Z","updated":"2020-05-14T07:59:03.282Z","comments":true,"path":"2019/06/10/ClickHouse-1/","link":"","permalink":"https://linqing2017.github.io/2019/06/10/ClickHouse-1/","excerpt":"数据库调研笔记 – ClickHouse","text":"数据库调研笔记 – ClickHouse 特性ClickHouse最大的优势是： OLAP 列式数据 同时支持以下功能： 支持SQL查询 支持Shard分片、Replica容错 支持数据进行向量计算 支持在表中定义组件、以及创建索引，能够实现亚秒级查询 支持近似计算，即通过牺牲精度的方式，挺高查询性能 支持命令行、HTTP、JDBC、ODBC、以及其他第三方客户端 支持多种语言的SDK 支持多种UI工具，如Tabix、HouseOps、LightHouse、DBeaver 支持专门的负载代理工具，如ClickHouse-Bulk、KittenHouse、chproxy 不支持以下功能： 不支持二级索引 没有完整的事务支持 只能批量删除或者修改，高频率修改单条数据的能力很差 不适合通过主键索引单行数据，原因是ClickHouse的索引是稀疏索引 ClickHouse的join逻辑和标准SQL有很大区别，因此如果迁移到ClickHouse中所有含有Join的SQL要重写（改为使用using关键字，或者subquery来实现） OLAP场景Click House是专门为OLAP场景设计，以下是 ClickHouse 官方对 OLAP 场景特征的总结： 大多数是读请求 数据总是大批量写入 不修改已有数据 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列 宽表，即每个表包含着大量的列 较少的查询(通常每台服务器每秒数百个查询或更少)，即对DB的并发能力要求不高 对于简单查询，允许延迟大约50毫秒 列中的数据相对较小： 数字和短字符串(例如，每个URL 60个字节) 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行） 事务不是必须的 对数据一致性要求低 每一个查询除了一个大表外都很小 查询结果明显小于源数据，换句话说，数据被过滤或聚合后能够被盛放在单台服务器的内存中 对大部分应用来说，实际情况可能不一定满足上面所有特点，并非是纯OLAP应用。 列式存储列式存储在OLAP场景中有以下优势： 只读取必要的数据、并且便于压缩，极大的减少IO消耗 按列查询能够极大的提高CPU的利用率 性能官方介绍按照官方介绍，ClickHouse 相比同类产品傲视群雄，并且公布了一份测试报告。报告中ClickHouse的性能是Greenplum的8倍，Vertica的3倍。 官方ClickHouse的性能，有以下指标描述（参考）: 吞吐量 缓存数据：2~10GB/s，对于简单的查询，速度可以达到30GB／s 非缓存数据：取决于数据的压缩比，以及磁盘IO 延时时间 缓存数据：小于50ms 非缓存数据：在使用HDD时，时延为：查找时间（10 ms） * 查询的列的数量 * 查询的数据块的数量 并能力 建议每秒最多查询100次 写入性能 建议每次写入不少于1000行 每秒不超过一个写入请求 单线程时，写入速度大约在50~200MB/s 其他测试从网络上收集到的资料，基本上可以得到下面的结论： 对于单表操作在不涉及Join的情况下，ClickHouse比其他的组件有非常大优势 ClickHouse同时也可以作为一个非常有竞争力的时序数据库来使用 多表Join操作性能，对ClickHouse来说并不理想，之比SparkSQL稍好一点，但是Impala、Presto之类相比依然有非常大差距 案例 比较对象 数据集 测试结果 知乎上一篇blog Spark SQLClickhousePrestoHAWQGreenPlum 基于TPC-DS 10GB数据集，包括：多表关联测试单表查询测试 在多表关联查询（基于TPC-DS数据集）中，Impala性能最佳，ClickHouse 表现只比SparkSQL稍好。 单表查询查询中，ClickHouse的性能非常优秀，基本比第二名 presto/impala 快3倍，比Spark SQL 快4倍！！参考 博客 SparkSQL 使用的测试集是Wiki Pagecount，数据规模是1.2TB。这个测试主要比较单表查询性能，包括进行：聚合、Group by等操作 Clickhouse的性能差不多是SparkSQL的10倍，同时更加节省磁盘和内存 博客 MariaDB ColumnStore Apache Spark 和上一个block类似，但是加入了MariaDB ColumnStore 测试中没有索引的情况下，MariaDB ColumnStore的性能是最差的，但是这个方案成功的在单节点中搞定了260亿规模的数据集。创建索引之后MariaDB KO了Spark，但是依然被ClickHouse吊打 Altinity InfluxDB TSBS ClickHouse有最好的插入性能，是第二名InfluxDB的2~3倍 Influx 最省磁盘毫秒级跨度的查询Influx性能最好大跨度的查询ClickHouse性能最优 基准测试ClickHouse 官方文档中给出了不少的示例数据集可以用来进行基准测试，但是比较遗憾的是没有说明可以使用TPC-DS的测试工具。 个人猜测主要因为以下原因： ClickHouse并不擅长多表关联操作，官方只好藏拙 TPC-DS的Case中大量包含join语法，完全应用到Clickhouse需要改造 官方BenchMark: 单表测试：维基访问数据 星型图测试 表引擎表引擎指的是Clickhouse中表的不同类型，决定了： 表的分片、副本情况； 如何支持并发访问，是否能够进行多线程请求； 如何进行索引； 数据复制参数； ClickHouse中最主要的表引擎是MergeTree族下的表引擎。 MergeTree表这种表引擎的原理实际上和HBase的原理类似，基于合并树将以主键排序的数据顺序写入到后台文件中，并在必要的时候对文件进行合并。 创建MergeTree表的命令如下： 1234567891011121314CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2) ENGINE = MergeTree() # 指定使用MergeTree引擎[PARTITION BY expr] # 指定分区键，如按月分区可以指定 toYYYYMM(date_column) [ORDER BY expr] # 表的排序键[PRIMARY KEY expr] # 表的主键，默认应该和排序键相同 [SAMPLE BY expr] # 采样表达式[SETTINGS name=value, ...] # 其他MergeTree的表达参数 当一批数据写入到MergeTree时，遵循以下过程： 根据分区键数据被分成不同part 每个part中的row按照排序键排序存储存储（ClickHouse后台会定期合并part） 为每个part创建一个索引，索引文件中包含每个索引行的主键 通常情况下主键是排序键的前缀，或者两者相等！ 分布式表ClickHouse通过Distributed引擎实现集群，即表的分片功能，需要注意的是，分片和分区是两种不同的逻辑。 Distributed引擎本身不存储数据，可以他是表分片的一个统一视图，通过这张表可以实现并行读写分片的功能。 1234Distributed(logs, default, hits[, sharding_key])# 上面的Distributed参数从logs集群中的default.hits表所有节点上读写数据# sharding_key 可以是任何能够返回常量的表达式，比如可以使用rand(),或者intHash64(UserID) 向Distributed表写数据的方式有以下两种： 自己制定要将数据写入那个分片，这时候实际上不是从Distributed写入，而是直接在分片表上写入 直接在Distributed表上写入，通过片键来决定实际写入到哪张表。 Distributed表的Shard路由方式是： 计算片键表达式 计算结果除以所有分片的权重总和得到余数 发送row到余数落在[ prev_weight, prev_weights + weight) 的分片。这个区间是这样形成的，假设有三个分片，权重分别为1、2、3，那么形成这样三个区间 [0,1), [1,3), 3,6)。 上述方式中，区间的划分可能由于shard的排列顺序不同出现差别。这样会导致一个row可能会被分到不同的shard。但是对于一批row来说，分到每个shard的row数目不会因为shard的排列出现差异。 通常，来说写Distributed的过程是异步的，即先将数据全部写到本地，然后在发送到各个分片 通常情况下，需要在config.xml文件中定义一个shard集群： 1234567891011121314151617181920212223242526&lt;remote_servers&gt; &lt;!-- logs表示集群的名称，可以是任意值 --&gt; &lt;logs&gt; &lt;!-- 下面定义了表的1个分片，这个分片有二个副本 --&gt; &lt;shard&gt; &lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;!-- 通常情况下internal_replication配置应该是ture，让副本由底层表的副本机制来同步 --&gt; &lt;internal_replication&gt;true&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;node1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;node2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;!-- 其他分片 --&gt; &lt;/shard&gt; &lt;/logs&gt;&lt;/remote_servers&gt; 复制表只有 MergeTree 系列里的表可支持副本，我们只需要在他们的建表语句中加上Replicated前缀即可创建复制表。 创建复制表时需要配置Zookeeper，用户可以参考以下SQL创建副本表： 1234567891011121314CREATE TABLE table_name( EventDate DateTime, CounterID UInt32, UserID UInt32) ENGINE = ReplicatedMergeTree('/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_name', '&#123;replica&#125;') PARTITION BY toYYYYMM(EventDate)ORDER BY (CounterID, EventDate, intHash32(UserID))SAMPLE BY intHash32(UserID)# 上面的SQL中，ReplicatedMergeTree函数传入了两参数，参数中大括号的内容会被config.xml中macros中的宏替换掉# 1. 第一个参数是表在Zookeeper中path，这个路径每张表应该是唯一的，&#123;layer&#125;-&#123;shard&#125;部分实际上是表的分片信息。示例中给出layer，shard两个字段，原因是因为这是一种两级分片的方案。# 2. 第二个参数是副本名称，这个配置要求每个副本唯一，表示同一个分片的不同副本。 复制表有如下特点： create语句来创建副本表时，只会在当前的机器上创建一个副本。因此，如果要创建一个三副本的复制表，那么要在三个不同的机器上执行三次create命令。 drop语句和create类似，只会删除当前机器上的副本 Zookeeper 如果故障，那么会导致所有副本表变成只读状态 复制是多主异步的，INSERT 语句（以及 ALTER ）可以发给任意可用的服务器。数据会先插入到执行该语句的服务器上，然后被复制到其他服务器。 默认写一个副本即写入成功，但是可以配置为insert_quorum 模式 如果写入相同的数据块，那么写入会被去重。 对于轻微的数据不一致，clickhouse会借助ZK自动修复。但是如果某个副本出现数据损坏，或者非常严重的不一致，那么需要用户手工介入来进行修复。 总体来说，ClickHouse的复制机制虽然比较灵活，但是个人认为有以下缺陷： 使用起来比较繁琐，无法向一般表一样建表删表。 官方文档没有说明使用什么机制保证副本间的一致性，从描述中看来应该是一个最终一致的系统 数据不一致时，Failover的机制不够自动化 其他表引擎ClickHouse还支持内存表，Log表等表引擎。但是按照官方文档的叙述，这些表并不适合应用在生产环境中。 安装部署ClickHouse可以使用RPM包部署（Repo），可以使用VPS搭建私库。 12345678# ClickHouse 要求内核支持SSE 4.2指令集，可以使用下面的命令检查grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo \"SSE 4.2 supported\" || echo \"SSE 4.2 not supported\"# 通过yum安装yum -y install clickhouse-client clickhouse-server 安装完成之后，/etc/clickhouse-server目录下包含配置文件：config.xml、users.xml，前者是全局配置文件，后者是用户权限配置文件。同时要注意，config.xml中定义了clickhouse的数据目录，启动时需要将own改为clickhouse:clickhouse！！ 12service clickhouse-server start/stop/status 启动服务之后可以使用clickhouse-client连接服务！ 配置文件示例以下配置创建一个三节点的ClickHouse集群，集群中数据互为备份 总结个人认为很适合IData当前的场景，可以使用ClickHouse替换掉Impala、hive、HBase这三个组件，并且还可充当时序数据库使用。在优化IData的表设计之后，应该能够提升SQL的查询性能！！！ 优点： 性能：9分。 对于单表操作来说，性能基本是傲视群雄的，从收集到压测信息看是Impala、Greenplume等MPP架构的3~5倍。 join性能表现不如单表性能抢眼，但是从别人的测试结果上看下限依然强于SparkSQL，上限可能不会超过Impala ClickHouse能在单机部署的情况下载就展示出非常好的性能，非常适合小集群 可维护性：9分。 RPM包安装，就一个配置文件，安装部署和mysql一样 有中文社区，官方的中文文档比较完善，基本看看就能上手 社区生态：7分。 JDBC、ODBC、UI、SDK接口之类该有的都有 缺点： 分片、副本功能很灵活，但是比较繁琐。ClickHouse的集群方式实际上是在多个实例上套用一个分库分表工具，并没有像Hadoop生态中的大部分工具那样自动Rep、Shard SQL语法不完善，没有做到100%兼容标准SQL 适合批量查询、批量修改，单条查询/修改的能力很差 其他知识稀疏索引稀疏索引,其相对的概念是稠密索引，它们都属于DB的聚集索引。 聚集索引定义：在一个文件中可以有多个索引，分别基于不同的搜索码。如果记录按照某个指定的顺序排序，那么该搜素码对应的索引就是聚集索引。 稠密索引定义: 在稠密索引中，每个搜索码值都对应一个索引值（记录）。 稀疏索引定义：在稀疏索引中，只为某些记录建立索引项。 稀疏索引的优势在于索引占用的空间小，并且插入和删除所需的性能开销同样也小。但是定位单条记录的能力弱。 参考[中文介绍] (https://clickhouse.yandex/docs/zh/) GDPR-通用数据保护条例 ClickHouse 中文论坛 ClickHouse 表引擎介绍","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ClickHouse","slug":"ClickHouse","permalink":"https://linqing2017.github.io/tags/ClickHouse/"}]},{"title":"数据库调研笔记 -- TiDB","slug":"TiDB-安装&介绍","date":"2019-06-02T16:00:00.000Z","updated":"2020-05-14T07:59:03.315Z","comments":true,"path":"2019/06/03/TiDB-安装&介绍/","link":"","permalink":"https://linqing2017.github.io/2019/06/03/TiDB-%E5%AE%89%E8%A3%85&%E4%BB%8B%E7%BB%8D/","excerpt":"NewSQL 调研笔记","text":"NewSQL 调研笔记 TiDB特性 兼容 MySQL，安装完成后，可以使用mysql客户端以及JDBC直连。 支持水平扩展，包括:计算节点和存储节点，同时提供强一致性、高可用性（从官方的说明介绍中，TiDB实际上是一个CA型数据库）。 云原生 SQL DB，能够容易的部署到K8S上，并且提供相应的Kubernetes Operator套件。 支持 TiSpark 工具，通过这个工具可以解决LDAP问题。 整体架构三个核心组件：TiDB Server、PD Server、TiKV Server TiDB架构 组件 作用能力 TiDB Server 负责接收用户请求，并进行聚合计算，无状态，可以横向水平扩展。 PD Server 管理模块，存储集群的元数据、执行TiKV的负载均衡、管理事务ID。 TiKV Server 以Region为单位提供数据的KV存储，并通过Raft协议进行Region的复制，保持数据的强一致性和容灾。 TiSpark Spark SQL 对接 TiDB 的插件，提供LDAP场景能力 安装部署在生产环境中部署，官方提供Ansible安装包，安装过程比较友好。 个人部署了一把，非常的顺利，调整好系统的内核参数后，几乎是一键部署！ 官方文档推荐生产TiDB需要部署9个实例（2计算+3管理+3存储+1监控，共占110核，220G内存，12SSD+5SAS）。 TiDB对PD、TiKV的磁盘性能要求比较高，甚至在安装包中了fio检查，当磁盘性能不达标时终止安装（当然用户可以调整这个指标）。 官方推荐SSD指标，如下： 指标 官方推荐 随机读iops 40000 随机写iops 10000 混合随机读写iops 10000 上面的指标，普通SATA接口的SSD不进行RAID很难达到。 PCI-E 接口的SSD应该可以达到上面的性能。 MySQL兼容性TiDB 支持 MySQL 传输协议，当前Tidb支持MySQL 5.7版本，可以使用MySQL客户端直连，并且支持mysql的备份恢复工具。 TiDB是虽然号称和MySQL完全兼容，但是依旧有部分特性缺失，如：存储过程与函数、视图、触发器、事件、自定义函数、外键约束、全文函数与索引、空间函数与索引、非 utf8 字符集 …… TiDB MySQL的兼容性差异，可以看pingcap的官方文档，其中有详细说明。 TiSparkTiSpark 是 pingCAP 基于 Spark Catalyst 的Tidb扩展数据引擎，能够直接读取 Tikv 上的数据的OLAP插件。 在一个6节点的SSD环境上测试，有如下结果： TiSpark执行复杂查询（TPC-DS）的性能相比直接运行JDBC查询有20%~30%提升。 TiSpark在小数据集（亿以内）的表现不如Impala，稍微优于SparkSQL。但是TiSpark目前只能进行查询，不能进行插入。但是目前使用JDBC大批量插入TiDB的效率远逊于Hive等SQL on Hadoop！ BenchMarkpingCAP公司官网有发布了一个TPC-H 50G 性能,报告主要展示了TiDB 1.0到2.0的性能提升，但是从结果上来看TiDB 2.0的性能相比Greenplum等MPP引擎来说依然没有优势。 TiDB的官方仓库提供了 Bench Market 测试工具，包含OLTP和OLAP两种类型的测试。其中，OLAP测试包括TPC-DS、TPC-H两种测试集，以及聚合性能测试，以及SSB压测工具。 测试结果参考：TPC-DS基准测试.xlsx 总结 易用：8分。 通过官方Ansible工具一键部署，无需额外开发。 能够兼容MYSQL绝大多数特性、允许MySql工具直连。 可以使用TiSpark和Spark无缝集成。 性能：6分。 并不是专门为OLAP场景设计的DB，对偏重OLAP的项目来说，无论写入或者查询都不是太擅长。 TiSpark不支持批量导入，而使用JDBC导入数据的性能太差。 可用性：6分。 极其耗费磁盘性能，上生产需要使用专用SSD！ TiSpark 和 CDH Spark 似乎有一些兼容性问题！ pingCAP提供的官方文档虽然比较全面，但是玩家似乎还比较少遇到故障时，FAQ资料很少，因此故障运维难度不小。 总体来说，目前TiDB和无线的大数据场景并不契合。 其他知识共识算法：Raft 解决简化的拜占庭将军问题：假设将军中没有叛军，信使的信息可靠但有可能被暗杀的情况下，将军们如何达成一致性决定？ 同类型算法包括：Paxos Raft的一致性方案核心思想：先在所有将军中选出一个大将军，所有的决定由大将军来做。 选举方式：每个将军持有一个随机时间的计时器，倒计时最先结束的将军发起投票推举自己为大将军。当获得半数以上投票时，选举结束，否者重复选举。 Raft 算法中保持一致性的几种场景： Leader不停的向Follower发送心跳来保证自己存活。 多个Leader的场景：Raft中每次选举有一个递增的ID来标记，非最新选举的Leader会自动降级为Follower。 选举时出现多个Candidate，如果多个Candidate同票，这种情况下选举会因为超时而失败。下一轮选举会重新投票。 CAP原理 特性 解释 Consistency “一致性”，对于每一次读操作，要么都能够读到最新写入的数据，要么错误。 Availability “可用性”，对于每一次请求，都能够得到一个及时的、非错的响应。 Partition tolerance “分区容错”，即系统出现网络分区后，必须是可恢复的！ 只要是分布式DB，这一条都必须满足。 所有的分布式系统都属于CP或者AP系统，而mysql等传统DB属于CA！ 通常CP系统出现网络故障的话，数据同步时间可能无限延长，此时系统会停止对外提供服务，来保证数据一致性，而AP系统在分区场景下依旧提供服务，但是用户可能发现系统数据存在不一致的情况。 当前一些分布式式系统可以通过一些配置，使系统在提供CP、AP两种不同的特性（比如MongoDB、Kafka）。 参考TiDB 官网 Raft 算法原理","categories":[{"name":"笔记","slug":"笔记","permalink":"https://linqing2017.github.io/categories/%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://linqing2017.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"NewSQL","slug":"NewSQL","permalink":"https://linqing2017.github.io/tags/NewSQL/"},{"name":"TiDB","slug":"TiDB","permalink":"https://linqing2017.github.io/tags/TiDB/"}]},{"title":"HBase 的二级索引方案","slug":"10-二级索引方案","date":"2019-04-28T16:00:00.000Z","updated":"2020-05-14T07:59:03.226Z","comments":false,"path":"2019/04/29/10-二级索引方案/","link":"","permalink":"https://linqing2017.github.io/2019/04/29/10-%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E6%96%B9%E6%A1%88/","excerpt":"说明在CDH环境中，如何使用CDH公司提供hbase-indexers对HBase进行二级索引。","text":"说明在CDH环境中，如何使用CDH公司提供hbase-indexers对HBase进行二级索引。 HBase IndexersHBase的二级索引方案特别多，其中使用Solr建立二级索引是用的比较多的。 CDH集成了Key-Value Store Indexer服务，该服务能够为HBase动态更新Solr上的索引！这个服务在Github上的项目名叫hbase-indexer。 HBase Indexers在CDH中主要用来和 Cloudera Search 配套使用，但是我经过尝试后发现，脱离Search这个东西实际上可以独立使用。 HBase Indexer使用HBase的备份接口拦截对表的读写请求，并异步通知Solr更新索引。由于整个过程交互是异步的，因此使用这个方案不会对HBase产生性能影响，但是Solr上建立索引的时效性可能会延后。Cloudera官方给出的时延级别是秒级！ Key-Value Store Indexer服务依赖Zookeeper实现高可用、以及配置同步，Zookeeper上的Znode节点是hbaseindexer 使用说明我们需要安装：Solr、HBase、Key-Value Store Indexer三个服务。 创建HBase表假设我们的HBase表结构是下面这样的： 1234567# HBase 中dict_app表### 天涯论坛 | 16 | 0 | 博客·论坛 |天涯论坛# rowkey | app_class_id |is_loan| app_class |app_abbr# # 表中所有字段属于info列族，类型都是字符串 我们在创建dict_app表时需要打开表的备份功能： 1create 'dict_app', &#123;NAME=&gt; 'info', REPLICATION_SCOPE=&gt; 1&#125; 创建Solr上的Collection使用solrctl命令我们创建集合dict_app，这个Collection用来保存dict_app表的索引。 12345678910# 创建本地配置文件solrctl instancedir --generate dict_app_config# 修改schema文件# 上传配置文件到Zookeepersolrctl instancedir --create dict_app_config dict_app_config# 创建Collectionsolrctl collection --create dict_app -s 2 -c dict_app_config 我们在创建dict_app前，需要修改 dict_app_config/conf/managed-schema 文件，预先定义相关的字段。 1234567&lt;!-- 我们将dict_app表除row key以外的列都定义成索引，row key是集合的ID--&gt;&lt;field name=\"app_class_id\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"is_loan\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"app_class\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt;&lt;field name=\"app_abbr\" type=\"text_general\" indexed=\"true\" stored=\"true\"/&gt; 创建Indexer我们可以使用hbase-indexer命令创建索引，但是创建之前我们需要编辑两个文件！ indexer.xml 这个文件告诉hbase-indexer我们为哪张表创建索引，以及morphlines.conf文件的位置！ 123456&lt;?xml version=\"1.0\"?&gt;&lt;indexer table=\"dict_app\" mapper=\"com.ngdata.hbaseindexer.morphline.MorphlineResultToSolrMapper\"&gt; &lt;param name=\"morphlineFile\" value=\"/etc/hbase-solr/conf/morphlines.conf\"/&gt;&lt;/indexer&gt; morphlines.conf 这个文件将Collection的字段信息和HBase列对应起来，每个Indexer都要能在同样的节点访问到这个文件： 1234567891011121314151617181920212223242526272829303132333435363738394041morphlines : [ &#123; id : morphline1 importCommands : [\"org.kitesdk.morphline.**\", \"com.ngdata.**\"] commands : [ &#123; extractHBaseCells &#123; mappings : [ &#123; inputColumn : \"info:app_class_id\" outputField : \"app_class_id\" type : string source : value &#125;, &#123; inputColumn : \"info:is_loan\" outputField : \"is_loan\" type : string source : value &#125;, &#123; inputColumn : \"info:app_class\" outputField : \"app_class\" type : string source : value &#125;, &#123; inputColumn : \"info:app_abbr\" outputField : \"app_abbr\" type : string source : value &#125; ] &#125; &#125; &#123; logTrace &#123; format : \"output record: &#123;&#125;\", args : [\"@&#123;&#125;\"] &#125; &#125; ] &#125;] 分发命令后使用以下命令创建Indexer： 12345678910111213141516# 创建Indexerhbase-indexer add-indexer \\--name dict_app \\--indexer-conf indexer.xml \\--connection-param solr.zk=bdnode1:2181,bdnode2:2181,bdnode3:2181/solr \\--connection-param solr.collection=dict_app \\--zookeeper bdnode1:2181,bdnode2:2181,bdnode3:2181# 改动配置文件后更新Indexerhbase-indexer update-indexer -n myIndexer -c indexer.xml # 列出所有Indexerhbase-indexer list-indexers# 查看indexer状态hbase-indexer replication-status 存在的问题 直接truncate表示，索引似乎不会自动更新。 indexer服务退出可能造成部分行写入时没有进行索引，并且当服务恢复以后，需要手工运行MR任务来重建索引！ 12345678910111213141516171819202122232425262728# 先从HBase捞出数据到HDFS上，然后将HDFS中的数据发到solrexport HADOOP_USER_NAME=solr; \\HADOOP_CLIENT_OPTS='-DmaxConnectionsPerHost=10000 -DmaxConnections=10000'; \\hadoop --config /etc/hadoop/conf \\jar /opt/cloudera/parcels/CDH/lib/hbase-solr/tools/hbase-indexer-mr-*-job.jar \\--conf /etc/hbase/conf/hbase-site.xml \\-D 'mapred.child.java.opts=-Xmx500m' \\--hbase-indexer-file indexer.xml \\--zk-host bdnode1:2181,bdnode2:2181,bdnode3:2181/solr \\--collection staSyslog \\--go-live \\--output-dir hdfs://nameservice1/hbase_test/offline_indexer/staSyslog \\ # 这个目录要属于solr用户，并且solr可以访问--overwrite-output-dir \\ --clear-index \\--log4j /etc/hbase-solr/conf/log4j.properties# 直接将Hbase中的数据发到SolrHADOOP_CLIENT_OPTS='-DmaxConnectionsPerHost=10000 -DmaxConnections=10000'; \\hadoop --config /etc/hadoop/conf \\jar /opt/cloudera/parcels/CDH/lib/hbase-solr/tools/hbase-indexer-mr-*-job.jar \\--conf /etc/hbase/conf/hbase-site.xml \\-D 'mapred.child.java.opts=-Xmx500m' \\--hbase-indexer-file indexer.xml \\--zk-host bdnode1:2181,bdnode2:2181,bdnode3:2181/solr \\--collection staSyslog \\--reducers 0 \\ --clear-index \\--log4j /etc/hbase-solr/conf/log4j.properties 参考hbase-indexer官网Lily HBase Near Real Time Indexing for Cloudera SearchLily HBase Batch Indexing for Cloudera Search","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Solr","slug":"Solr","permalink":"https://linqing2017.github.io/tags/Solr/"},{"name":"HBase","slug":"HBase","permalink":"https://linqing2017.github.io/tags/HBase/"}]},{"title":"Solr 认证配置","slug":"1-认证","date":"2019-04-15T16:00:00.000Z","updated":"2020-05-14T07:59:03.238Z","comments":false,"path":"2019/04/16/1-认证/","link":"","permalink":"https://linqing2017.github.io/2019/04/16/1-%E8%AE%A4%E8%AF%81/","excerpt":"简单说明 Solr 认证配置相关配置","text":"简单说明 Solr 认证配置相关配置 Solr包含一些开箱即用的插件，通过这些插件能够完成Solr的认证、授权等功能。 默认情况下Solr中的所有认证、授权配置保存在security.json文件中。Standlone模式下，Security.json文件保存在$SOLR_HOME目录下，SolrCloud模式时该文件保存在ZK中。 security.json文件示例： 12345678&#123; \"authentication\" : &#123; \"class\": \"class.that.implements.authentication\" &#125;, \"authorization\": &#123; \"class\": \"class.that.implements.authorization\" &#125;&#125; 使用CDH时以下命令可以将security.json文件upload到zk中： 1234567891011121314# 上传或者覆盖配置信息/opt/cloudera/parcels/CDH/lib/solr/bin/zkcli.sh -zkhost bdnode1:2181 -cmd putfile &#123;znode_name&#125; &#123;src_path&#125;# 下载配置信息/opt/cloudera/parcels/CDH/lib/solr/bin/zkcli.sh -zkhost bdnode1:2181 -cmd getfile &#123;znode_name&#125; &#123;dest_path&#125;# 使用 ./bin/solr zk命令能够操作ZNODEsolr zk upconfig|downconfig -d &lt;confdir&gt; -n &lt;configName&gt; [-z zkHost]solr zk cp [-r] &lt;src&gt; &lt;dest&gt; [-z zkHost]solr zk rm [-r] &lt;path&gt; [-z zkHost]solr zk mv &lt;src&gt; &lt;dest&gt; [-z zkHost]solr zk ls [-r] &lt;path&gt; [-z zkHost]solr zk mkroot &lt;path&gt; [-z zkHost] 认证插件当前Solr提供了以下几种认证插件： 插件名称 官方参考 Kerberos 参考 Basic 参考 Hadoop 参考 Solr还支持PKIAuthenticationPlugin插件，该插件用来处理Solr内部请求的安全认证。需要注意的是PKIAuthenticationPlugin的加密原理对时间敏感，Solr节点间需要保证5s以内的时间同步。通过pkiauth.ttl参数可以控制密钥失效时间。 Basic 插件Basic 插件基于用户名/密码进行认证。 下面的配置，同时启用了baseAuthPlugin和授权插件。 123456&#123;\"authentication\":&#123; \"blockUnknown\": true, \"class\":\"solr.BasicAuthPlugin\", \"credentials\":&#123;\"solr\":\"IV0EHq1OnNrj6gvRCwvFwTrZ1+z1oBbnQdiVC3otuq0= Ndd7LKvVBAaZIF0QAVi1ekCfAJXr1GGfLtRUXhgrF8c=\"&#125; &#125; 参数含义如下： class：实现的插件类 credentials：定义了solr用户和该用户的密码（密码带盐值的sha256密文） 编辑插件的配置可以参考官方文档，Solr本身也有API可以进行用户管理。 Hadoop 插件此插件将所有功能委派给Hadoop authentication library。 由于Hadoop支持simple和Kerberos两种认证方式，所以使用这个插件实际上能够实现Kerberos认证。官方提供了Kerberos、Simple两种配置的security.json样例。 该插件有ConfigurableInternodeAuthHadoopPlugin和HadoopAuthPlugin两种实现类区别在于：前者同时处理内部认证和外部认证，后者只处理内部认证。 Kerberos 插件Solr集成Kerberos插件后，Solr可以实现以下效果： 使用principal和keytab在Zookeeper上完成认证； Solr集群内部完成节点的相互认证； Admin UI和其他客户端需要认证后访问solr； 官方文档比较详细的说明了该插件的配置方法，并且简要说明了如何进行Delegation Tokens配置，以及使用SolrJ完成Kerberos认证。 Znode节点添加ACLs默认情况下，Solr启用认证后只对本身API生效。Solr存储在Zookeeper上配置依然是open-unsafe配置。 Solr在ZK上的权限管理，取决于以下两个配置： zkCredentialsProvider：ZK的凭证管理类，用于获取在ZooKeeper中执行操作的权限。按照个人理解，这个配置当ZK使用digest认证时才有用，Kerberos认证时只需配置默认值即可。该配置的默认值是DefaultZkCredentialsProvider，另一个配置是VMParamsSingleSetCredentialsDigestZkCredentialsProvider。使用后者时，涉及到的配置项还有zkDigestPassword、zkDigestUsername。 zkACLProvider ：这个配置控制solr在ZK上创建ZNode时，Znode的权限。 zkACLProvider包括以下配置（org.apache.solr.common.cloud.xxx）： DefaultZkACLProvider: 默认配置创建的ZNode是所有人都可以访问的。 VMParamsAllAndReadonlyDigestZkACLProvider：ZK使用Digest认证时使用这个配置。 SaslZkACLProvider：ZK使用Kerberos认证时使用这个配置。 通常情况下，需要在将上面提及的配置添加到环境变量SOLR_ZK_CREDS_AND_ACLS，并在solr.in.sh和zkcli.sh中引入这两个配置。 Cloudera中配置Solr认证自定义security.jsonCDH默认使用 Hadoop 插件作为Solr的认证工具，在parcels包的solr/clusterconfig/目录下有security.json的模板文件。 每次Cloudera启动Solr都会加载security.json模板文件，并覆盖Zookeeper上的配置文件。如果要自定义solr授权、认证方案，需要修改/opt/cloudera/cm-agent/service/solr/solr.sh文件。 12345678910111213379 if [ $CDH_VERSION -ge 6 ]; then380 export SOLR_SEC_CONFIG_FILE=\"$&#123;SOLR_HOME&#125;/clusterconfig/hadoop_multi_scheme_config.json\"381 382 # if $SOLR_HOME/bin/zksynctool.sh; then383 # echo \"Successfully configured security for Solr cluster in Zookeeper\"384 # else385 # echo \"ERROR: Failed to configure security for Solr cluster in Zookeeper\"386 # exit 1387 # fi388 389 # set this very high to keep Solr's forced shutdown from interfering with CM's forced shutdown (value in seconds)390 export SOLR_STOP_WAIT=$&#123;SOLR_STOP_WAIT:-2592000&#125;391 fi 使用下面的命令可以上传security.json到ZK（前提是配置好zkcli.sh中的Kerberos认证信息），需要注意是每次修改security.json之后需要重启Solr。 1/opt/cloudera/parcels/CDH/lib/solr/bin/zkcli.sh -cmd putfile /solr/security.json ./security.json -z bdnode1:2181,bdnode2:2181,bdnode3:2181 Znode启用ACLs当在已有CDH中开启Kerberos时，Cloudera不会将原有Znode配置ACLs。用户需要修改zkcli.sh文件，执行以下命令将之前创建的ZNode修改ACLs配置： 123# 执行以下命令更新ZNode的ACL配置cd /opt/cloudera/cm-agent/service/solr/ &amp;&amp; ./bin/zkcli.sh -zkhost bdnode1:2181,bdnode2:2181,bdnode3:2181 -cmd updateacls /solr 参考以下方式修改zkcli.sh中的SOLR_ZK_CREDS_AND_ACLS变量： 123# Settings for ZK ACLSOLR_ZK_CREDS_AND_ACLS=\"-DzkACLProvider=org.apache.solr.common.cloud.SaslZkACLProvider \\ -Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf\" 参考官方安全文档 认证 &amp; 授权插件 ZooKeeper Access Control","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Solr","slug":"Solr","permalink":"https://linqing2017.github.io/tags/Solr/"}]},{"title":"Solr 授权配置","slug":"2-授权","date":"2019-04-15T16:00:00.000Z","updated":"2020-05-14T07:59:03.277Z","comments":false,"path":"2019/04/16/2-授权/","link":"","permalink":"https://linqing2017.github.io/2019/04/16/2-%E6%8E%88%E6%9D%83/","excerpt":"使用Solr的 Rule-Based Authorization 插件在CDH上实现权限管理！","text":"使用Solr的 Rule-Based Authorization 插件在CDH上实现权限管理！ Rule-Based Authorization Plugin 是Apache Solr提供的唯一一个权限插件。 实际实践过程中发现，该插件易用性不高。虽然可以进行细粒度权限控制，但是在网上参考资料很少，配置方式也相当反人类！ 权限模型Rule-Based Authorization通过：Permission和Role绑定，Role和User绑定的方式给用户配置权限，所有相关的权限&amp;用户定义在security.json文件中。 Rule-Based Authorization中包括两种类型的Permission：预定义的Permisson和自定义的Permisson。 所有预定义权限的限制对象都是所有Collection，包括以下13种： Permisson 说明 security-edit 编辑security.json权限 security-read 读security.json权限 schema-edit 编辑任意Collection的schema的权限 schema-read 读任意Collection的schema的权限 config-read 编辑任意Collection的solrconfig的权限 config-edit 读任意Collection的solrconfig的权限 collection-admin-read 执行/admin/collections地址下读操作的权限（不限制Collection） collection-admin-edit 执行/admin/collections地址下写操作的权限（不限制Collection） core-admin-read 执行Core API中读操作的权限（不限制Collection） core-admin-edit 执行Core API中写操作的权限（不限制Collection） update 执行任意Collection中/update操作的权限 read 执行任意Collection中读操作的权限，如/get、/select all 上述所有权限 自定义权限可以限制请求中的以下实体： Collection名称 请求类型，如GET、POST、PUT等 请求的地址，如/select、/collection 等 下面的自定义权限，定义了访问securecollection的/select的权限。官方文档关于 Authorization API 有比较详细说明。 1234567&#123; \"name\":\"secure-collection1-permission\", \"collection\":\"securecollection\", \"path\":\"/select\", \"before\":\"collection-admin-read\", \"role\":\"admin\"&#125; 例子下面的权限配置实现以下效果： 任意用户有除security.json以外的Read权限 dev用户有除security.json以外的完整编辑权限 solr、solr/node1、solr/node2有所有权限 12345678910111213141516171819202122232425\"authorization\": &#123; \"class\": \"org.apache.solr.security.RuleBasedAuthorizationPlugin\", \"permissions\": [ &#123;\"name\": \"security-read\", \"role\": \"security_role\"&#125;, &#123;\"name\": \"security-edit\", \"role\": \"security_role\"&#125;, &#123;\"name\": \"read\", \"role\": \"*\"&#125;, &#123;\"name\": \"schema-read\", \"role\": \"*\"&#125;, &#123;\"name\": \"config-read\", \"role\": \"*\"&#125;, &#123;\"name\": \"collection-admin-read\", \"role\": \"*\"&#125;, &#123;\"name\": \"core-admin-read\", \"role\": \"*\"&#125;, &#123;\"name\": \"update\", \"role\": \"dev\"&#125;, &#123;\"name\": \"schema-edit\", \"role\": \"dev\"&#125;, &#123;\"name\": \"config-edit\", \"role\": \"dev\"&#125;, &#123;\"name\": \"collection-admin-edit\", \"role\": \"dev\"&#125;, &#123;\"name\": \"core-admin-edit\", \"role\": \"dev\"&#125; ], \"user-role\": &#123; \"solr/bdnode2@LINQING.COM\": [\"security_role\",\"dev\"], \"solr/bdnode3@LINQING.COM\": [\"security_role\",\"dev\"], \"solr@LINQING.COM\": [\"security_role\",\"dev\"], \"dev@LINQING.COM\": \"dev\" &#125;&#125; 使用下面的curl请求可以测试： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 创建用户-Role映射，需要security-edit权限curl --negotiate -u : -H 'Content-type:application/json' -d '&#123; \"set-user-role\" : &#123;\"demo@LINQING.COM\": null&#125;&#125;' http://bdnode2:8983/solr/admin/authorization# 创建Collection、删除Collection、重载Collection，需要collection-admin-edit权限curl --negotiate -u : 'http://bdnode2:8983/solr/admin/collections?action=CREATE&amp;name=film&amp;numShards=2&amp;replicationFactor=2&amp;maxShardsPerNode=2&amp;collection.configName=film'curl --negotiate -u : 'http://bdnode3:8983/solr/admin/collections?action=DELETE&amp;name=film'curl --negotiate -u : 'http://bdnode3:8983/solr/admin/collections?action=RELOAD&amp;name=film'# 修改Schema，需要schema-edit权限curl --negotiate -u : -X POST -H 'Content-type:application/json' --data-binary '&#123;\"add-field\": &#123;\"name\":\"name\", \"type\":\"text_general\", \"multiValued\":false, \"stored\":true&#125;&#125;' http://bdnode2:8983/solr/film/schemacurl --negotiate -u : -X POST -H 'Content-type:application/json' --data-binary '&#123;\"add-copy-field\" : &#123;\"source\":\"*\",\"dest\":\"_text_\"&#125;&#125;' http://bdnode3:8983/solr/film/schema# update数据，需要update权限curl --negotiate -u : -X POST -H 'Content-Type: application/json' --data-binary '&#123; \"id\": \"xxxxxxxxxxxxxxxxxxxx\", \"initial_release_date\": \"2007-06-28\", \"name\": \"Harry Potter and the Order of the Phoenix\", \"genre\": [ \"Family\", \"Mystery\", \"Adventure Film\", \"Fantasy\", \"Fantasy Adventure\", \"Fiction\" ], \"directed_by\": [ \"David Yates\" ]&#125;' http://bdnode2:8983/solr/film/update/json/docs# 查询security配置，需要security-read权限curl --negotiate -u : 'http://bdnode2:8983/solr/admin/authorization'# 查询所有Collection配置，需要collection-admin-read权限curl --negotiate -u : 'http://bdnode2:8983/solr/admin/collections?action=LIST'# 查询某个Collection，read权限curl --negotiate -u : 'http://bdnode2:8983/solr/film/select?q=*:*&amp;rows=0' 注意事项 使用Kerberos认证时，在security.json中需要定义principal的fully name，不能定义短名称 使用Kerberos认证时，所有Solr节点使用的principal都需要手工配置权限 Solr确认Permission是按照顺序的，定义permissions时需要将小权限放在大权限之前，因此ALL需要放在最后定义 配置该插件之后，如果出现资源不存在的情况，错误提示也会出现403 参考文档Rule-Based Authorization Securing Solr: Tips and Tricks You Really Need to Know Securing Solr with Basic Authentication","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Solr","slug":"Solr","permalink":"https://linqing2017.github.io/tags/Solr/"}]},{"title":"Solr 学习笔记","slug":"0-入门","date":"2019-04-10T16:00:00.000Z","updated":"2020-05-14T07:59:03.215Z","comments":false,"path":"2019/04/11/0-入门/","link":"","permalink":"https://linqing2017.github.io/2019/04/11/0-%E5%85%A5%E9%97%A8/","excerpt":"Solr的学习笔记","text":"Solr的学习笔记 安装部署使用脚本安装Standalone模式Solr安装包的bin目录下提供了install_solr_service.sh脚本，通过该脚本能够在节点上安装Solr服务。 1234567891011121314./install_solr_service.sh solr-7.7.1.tgz############################################### 1. 脚本将solr-7.7.1.tgz，默认解压到/opt目录，并且创建软连接/opt/solr。# 2. 使用/var/solr作为solr的数据目录，该目录下包括solr数据、日志。# 3. 安装完成后使用 /etc/init.d/solr 启停solr服务，该脚本使用solr用户启动服务（或者用service，使用systemctl似乎有一点问题）。# 4. 默认使用8983作为Solr的端口# 5. 默认情况下solr配置文件为 /etc/default/solr-demo.in.sh ，该文件中包含ZK、Hadoop等信息的定义。# 6. /var/solr/data是默认情况下的主目录，该目录下包含：zoo.cfg和solr.xml。# 7. /opt/solr/bin/oom_solr.sh 该脚本用来杀死oom状态的Solr进程。# 8. solr的安装信息都固化在/etc/init.d/solr中，通过这个文件可以修改配置文件位置信息。############################################## SolrCloud模式当在/etc/default/solr-demo.in.sh 中配置 ZK_HOST 时，Solr通过Cloud的模式启动。 12345# 使用以下命令，可以打开一个交互式的shell，可以根据引导创建一个SolrCloud集群,所有Solr实例运行在一个物理机上bin/solr -e cloud# 下面的链接详细说明了如何使用solr命令创建一个solrcloud集群## https://lucene.apache.org/solr/guide/7_0/getting-started-with-solrcloud.html#getting-started-with-solrcloud 简单使用官方Quick Start展示了使用预置techproducts的一些简单用法： Exercise 1 123456789101112131415bin/post -c techproducts example/exampledocs/* # 使用post命令导入数据# 搜索任意Documenthttp://localhost:8983/solr/techproducts/select?indent=on&amp;q=*:*# 单个短语搜索http://localhost:8983/solr/techproducts/select?q=foundation# 只返回id字段http://localhost:8983/solr/techproducts/select?q=foundation&amp;fl=id# 搜索指定字段http://localhost:8983/solr/techproducts/select?q=cat:electronics# 搜索短语，这里+被转义为空格http://localhost:8983/solr/techproducts/select?q=\\\"CAS+latency\\\"# 联合搜索，%2B是+的转义，%20是空格的转义，等价于+electronics +musichttp://localhost:8983/solr/techproducts/select?q=%2Belectronics%20%2Bmusichttp://localhost:8983/solr/techproducts/select?q=%2Belectronics+-music Exercise 2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 创建一个新collection，使用_default配置bin/solr create -c films -s 2 -rf 2##################### # 默认情况下：# 1. _default生成的是“managed schema”，用户需要使用 Schema API 修改schema的规则。# 2. _default默认是schemaless（这个配置定义在solrconfig.xml中）。# 3. 通常情况下不应该在生产环境中使用schemaless特性，而应该手工定义schema.xml文件。# 4. ##################### 创建text_general类型的字段，stored为true表示这个字段可以用来进行查询curl -X POST -H 'Content-type:application/json' --data-binary '&#123;\"add-field\": &#123;\"name\":\"name\", \"type\":\"text_general\", \"multiValued\":false, \"stored\":true&#125;&#125;' http://172.24.26.93:8983/solr/films/schema###################### CopyField：solr 有一个字段复制机制，可以将多个不同类型字段集中到一个字段。# # 典型应用场景：# 用户创建博客索引时，需要查询title、content。 此时可以定义一个新字段，将title和content复制到这个新字段，索引的时候，直接从这个新字段查# 询，这样就达到目地了。###################### 定义一个copyField，将所有字段的值复制到_text_字段curl -X POST -H 'Content-type:application/json' --data-binary '&#123;\"add-copy-field\" : &#123;\"source\":\"*\",\"dest\":\"_text_\"&#125;&#125;' http://172.24.26.93:8983/solr/films/schema# 使用Post命令进行不同格式的数据导入bin/post -c films example/films/films.csv -params \"f.genre.split=true&amp;f.directed_by.split=true&amp;f.genre.separator=|&amp;f.directed_by.separator=|\"bin/post -c films example/films/films.xmlbin/post -c films example/films/films.json##################### Faceting：# 允许将搜索结果排列成子集，为每个子集提供计数。Solr提供以下类型的Faceting： Field Facets：对某个Field进行分类统计 Numeric Range Facets：统计某个Field的Range，如价格区间 Pivot Facets：组合分类，如统计属于A、同时又属于B的记录##################### 以下命令对genre_str进行分类统计，并返回统计结果curl \"http://localhost:8983/solr/films/select?q=*:*&amp;rows=0&amp;facet=true&amp;facet.field=genre_str\"# 以下命令对genre_str进行分类统计，但是只返回统计计数大于200的结果curl \"http://localhost:8983/solr/films/select?=&amp;q=*:*&amp;facet.field=genre_str&amp;facet.mincount=200&amp;facet=on&amp;rows=0\"# 统计发行年代，范围是[NOW-20YEAR,NOW],每一类的间隔是1年curl 'http://localhost:8983/solr/films/select?q=*:*&amp;rows=0&amp;facet=true&amp;facet.range=initial_release_date&amp;facet.range.start=NOW-20YEAR&amp;facet.range.end=NOW&amp;facet.range.gap=%2B1YEAR'# 统计类型和导演curl \"http://localhost:8983/solr/films/select?q=*:*&amp;rows=0&amp;facet=on&amp;facet.pivot=genre_str,directed_by_str\" Exercise 3 1234567# 删除数据bin/post -c localDocs -d \"&lt;delete&gt;&lt;id&gt;SP2514N&lt;/id&gt;&lt;/delete&gt;\"bin/post -c localDocs -d \"&lt;delete&gt;&lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt;\"# 重启服务./bin/solr start -c -p 8983 -s example/cloud/node1/solr./bin/solr start -c -p 7574 -s example/cloud/node2/solr -z localhost:9983 配置文件Standlone模式下比较关键的配置文件均在solr.home目录下（可以是/var/solr/data目录）： 123456789.├── films # Core 名称目录│ ├── conf ##### SolrCloud模式时放到ZK的configs目录上│ │ ├── managed-schema # 该文件是Schema的配置文件│ │ └── solrconfig.xml # 这个配置文件指定了core的一些高级配置，如数据目录等等│ ├── core.properties # Core的关键信息配置文件│ └── data├── solr.xml # 单个Solr server信息的配置 ##### SolrCloud模式时放到ZK的configs目录上└── zoo.cfg 配置文件详细信息参考： solr.xmlcore.propertiessolrconfig.xmlmanaged-schema 使用CDH部署时，采用的是SolrCloud + HDFS的部署方式，默认情况下使用/metadata/solr/server作为部署目录。 此时安装目录为/opt/cloudera/parcels/CDH/lib/solr！！ 使用SolrCloud时，config目录下的配置会在创建Collection时被上传到地址为/solr/configs/{collection name}的Znode中。 configsetsconfigsets 实际上是一组可以重用的配置文件模板，configsets中的内容实际上是/var/solr/data/xxx/conf里的内容。 默认情况下configsets在/opt/solr/server/solr/configsets目录中。CDH中这些配置文件似乎是在ZK上？？ schemaSolr的schema是一个单独的xml文件(managed-schema )，它存储以下信息： core和collection的字段和字段类型信息。 core和collection的查询、存储规则，以及copy fields、dynamic fields的生成规则。 SolrCloud关键概念群集可以托管多个Solr Collection。可以将Collection划分为多个Shards，每个Shards包含Collection的一部分内容。Shards的数量对Collection来说是确定的，是一种抽象概念。多个Shard可以让搜索请求并行执行。哪个Shard包含集合中的哪些文档取决于该Collection指定的分片策略。 replica是和Shard相对的物理概念，每个shard至少包含一个replica，并且其中一个Replicas是leader（通过ZK的选举机制获得）。 Replica由于Replica中需要进行leader，通常情况下Solr支持以下几种Replica类型： NRT：NRT副本（NRT = NearRealTime）维护一个transaction日志并在本地将新文档写入其索引。NRT类型的Replica随时都有资格被选为Leader TLOG： 此类Replica也维护事务日志，但不会更改本地的本地索引文档（不会更改索引）。TLOG类型的Replica通过从领导者复制索引来实现Index更新。TLOG同样可以成为Leader，并且在成为Leader后，他的行为和NRT类型的副本一致。 PULL：不维护事务日志，也不更改本地索引文档。无法成为leader，也无法参与Leader选举。 几种Replica配置方式： All NRT replicas：适合小型集群，或者更新频率不高的大型集群； All TLOG Replicas： 每个分片的副本数量很高时; TLOG replicas plus PULL replicas: 每个分片的副本数量很高时，增加查询请求的吞吐量； Document Routing通过API创建Collection时，可以使用router.name来自动Document使用的路由策略。 compositeId ：通过文档ID部分前缀的hash值，进行路由。通常可以使用”!”在文档ID中分割出这一部分前缀，在查询时可以使用_route_来指定需要查询的Shard地址。compositeId方案支持二级路由，如以下类型的ID：”USA!IBM!12345”。同时还支持shard_key/num!document_id格式的路由。 implicit：使用router.field指定的字段来标识所属的shard Shard SplittingShard支持Splitting命令，用户可以通过SPLITSHARD command将Shard一分为二。 Solr on HDFSSolr支持将索引和事务日志文件写入到HDFS中。通过JVM参数、solr.in.sh、solrconfig.xml三种方式可以指定Solr存储位置。 参数配置Standalone指定以下参数： 1234bin/solr start -Dsolr.directoryFactory=HdfsDirectoryFactory -Dsolr.lock.type=hdfs -Dsolr.data.dir=hdfs://host:port/path -Dsolr.updatelog=hdfs://host:port/path SolrCloud 指定以下参数 123bin/solr start -c -Dsolr.directoryFactory=HdfsDirectoryFactory -Dsolr.lock.type=hdfs -Dsolr.hdfs.home=hdfs://host:port/path 使用Apache Solr时对接HDFS需要指定的参数： 12345678SOLR_OPTS=\"$SOLR_OPTS -Dsolr.directoryFactory=HdfsDirectoryFactory\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.lock.type=hdfs\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.home=hdfs://nameservice1/test_solr\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.confdir=/etc/hadoop/conf\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.confdir=/etc/hadoop/conf\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.security.kerberos.enabled=true\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.security.kerberos.keytabfile=/opt/solr/solr.keytab\"SOLR_OPTS=\"$SOLR_OPTS -Dsolr.hdfs.security.kerberos.principal=solr/lqing93@IDATA.RUIJIE.COM\" 缓存机制HdfsDirectoryFactory 会申请对外内存用来缓存HDFS上的块信息。此缓存通常需要非常大，可能需要提高运行Solr的特定JVM的堆外内存限制。 12# 以下参数限制JVM的堆外内存大小-XX:MaxDirectMemorySize=20g 使用SolrCtl命令solrctl命令是cloudera包装过的solr命令行工具，使用方式和原生solr不同。官方使用说明可以参考链接。 12345678910# 在/var/lib/solr/demo目录创建配置文件模板solrctl instancedir --generate /var/lib/solr/demo -schemaless# 上传本地目录中的配置文件到ZK，此时生成了一个新的configsetsolrctl instancedir --create demo /var/lib/solr/demo# 创建collection，并且使用同名configsetsolrctl collection --create demo -s 2 -a -c demo -r 2 -m 2# 编辑/var/lib/solr/demo目录下的文件后更新到Zookeepersolrctl instancedir --update demo /var/lib/solr/demo# reload collection应用更新后的配置solrctl collection --reload demo 配置kerberos时参考以下命令 1234567# step1solrctl instancedir --generate /var/lib/solr/film -schemaless# step2kinit -k solrsolrctl --jaas /etc/zookeeper/conf/jaas.conf --debug --zk bdnode1:2181,bdnode2:2181,bdnode3:2181/solr instancedir --create film /var/lib/solr/film# step3solrctl collection --create film -s 2 -a -c film -r 2 -m 4 参考文档W3Cschool:Apache Solr参考指南 官网 QuickStart solr命令的使用 SolrCloud SolrCloud Configuration and Parameters HdfsDirectoryFactory 的配置参数 如何在查询时指定分片 solrcloud-configuration-and-parameters","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Solr","slug":"Solr","permalink":"https://linqing2017.github.io/tags/Solr/"}]},{"title":"HBase Tools","slug":"9-hbase运维工具","date":"2019-04-07T16:00:00.000Z","updated":"2020-05-14T07:59:03.281Z","comments":false,"path":"2019/04/08/9-hbase运维工具/","link":"","permalink":"https://linqing2017.github.io/2019/04/08/9-hbase%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7/","excerpt":"HBase Tools 介绍 ~ 持续更新！！","text":"HBase Tools 介绍 ~ 持续更新！！ CanaryReadCanary 工具可以对HBase集群的表进行”canary-test”。该检查支持一下三种模式： region mode： 从每个region的每个column-family上读取一行，如果都OK，那么测试通过。该模式是默认模式！ regionserver mode： 从每个RegionServers的所有Region中随机选择一行读取，如果OK，那么测试通过。 zookeeper mode： 读取HBase的Znode节点，如果读取正常，那么测试通过。 以下是调用示例： 1234567891011121314# 所有table以region mode执行一次检查hbase canary # 检查所有dim_开头的表hbase canary -e dim_.*# 检查table表hbase canary dim_ac_info# 以regionserver mode方式执行检查hbase canary -regionserver# 每隔5秒运行一次检查，并且即使检查发生错误，cannary进程也不会退出。超时时间为60shbase canary -daemon -interval 5 -f false -t 60000# 每隔5秒运行一次检查，检查发生错误，cannary进程退出hbase canary -daemon -interval 5 -treatFailureAsError# 在Kerberos hbase环境中进行检查hbase canary -Dhbase.client.kerberos.principal=hbase -Dhbase.client.keytab.file=/etc/krb5.keytab Write默认情况写入检查时不开启的，通过-writeSniffing配置可以开启写入检查。 进行写入检查时，canary进程会写入一张指定的表，这张表由Canary进程创建，并且该表的分区分布在所有的region servers上。 1234# 打开写入检查，默认情况下，写入表为hbase:canaryhbase canary -writeSniffing# 执行canary写入的表名，以及写入大小（默认为10b）hbase canary -writeSniffing -writeTable ns:canary -Dhbase.canary.write.value.size=100 Clouderacm页面集成了canary的配置项，默认情况下该配置不打开，并且不能调整Canary参数。 12# Cloudera 运行Canary的命令如下：org.apache.hadoop.hbase.tool.Canary -t 15000 -daemon -interval 6 -regionserver bdnode2 RegionSplitter手工Region预分区工具（参考），通过这个工具可以手工创建一张指定分区数目的新表，用户可以指定不同的Key策略，包括：HexStringSplit、DecimalStringSplit、UniformSplit 1hbase regionsplitter &#123;table name&#125; HexStringSplit -c 10 -f cf1:cf2 在hbase shell 中可以使用 split 命令和 merge 命令进行强制region拆分、合并操作。 关于 Region 拆分合并的知识可以参考这篇blog Health CheckerHBase在HBASE-7351中提供了Health Checker机制，让HBASE服务通过sh调用的方式来确认master、regionserver等服务是否可用。 123456# health checker脚本涉及到以下几个配置项 hbase.node.health.script.location hbase.node.health.script.timeout hbase.node.health.script.frequency Default is every 60seconds. hbase.node.health.failure.threshold Defaults to 3. 官方提供hbase-examples/src/main/sh/healthcheck/healthcheck.sh脚本作为设计checker脚本的参考。 DriverHBase 提供一些便利的Driver Class，这些Class可以通过bin/hbase命令来运行。 通过${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.mapreduce.UtilityName可以运行以下driver类 UtilityName 说明 示例 LoadIncrementalHFiles 将本地HFILE文件导入到HBase CopyTable 将一张表从一个HBase导出到另一个HBase Export 将表导出到HDFS中 Import 将表导入到HBase中 ImportTsv 将CSV导入到HBase中 RowCounter 统计表的行数 CellCounter 统计表的cell数目 replication.VerifyReplication 比较两个HBase集群的表 hbck 和 HBCK2hbck是一个完整性、一致性工具，在hbase-1.x 版本之前该工具可以诊断表的完整性、同时可以用于修复不一致表。hbase-2.x版本以后，该工具被HBCK2取代，只能只读运行，不提供修复功能。 使用以下命令可以进行HBase表的一致性检查： 1234# 全表检查hbase hbck -details# 检查指定表hbase hbck Table1 Table2 官方文档强烈不建议使用hbck去修复hbase 2.x的不一致，原因是hbck会绕过Master直接访问HDFS上的HFile文件，可能造成更严重的不一致。 参考hbck的深入介绍：hbck In Depth HBCK2 工具是一个独立于HBase的项目，其jar包不随HBase一同发布需要用户从源码进行编译。 由于 CDH 6.1.0 是基于 HBase 2.1.0 开发，HBCK2 说明文档中提及该工具不支持 2.0.3 和 2.1.1 之前的HBase版本，HBase只在3.x的官方文档中提到了这个工具。所以该工具用在hbase 2.1.0-CDH 6.1.0 上也不一定靠谱。 HBCK2提供了以下功能： bypass：释放一个或者多个卡住的procedure assigns/unassigns：对应region，这两个操作在 hbase shell 中也可以运行。 setTableState：设定表的状态，可以将表设定为ENABLED, DISABLED, DISABLING, ENABLING serverCrashProcedures：？？？？这个不知道干啥用的 参考这篇blog说明了HBCK2的一些常见用法( 基本上是HBCK2 说明文档的翻译)：HBase 2.0之修复工具HBCK2运维指南 HFile Tool使用HFile Tool可以查看HBase在HDFS上的hfile文件的内容 1hbase hfile -v -f hdfs://nameservice1/hbase/data/default/TestTable/d051c004ff7f7441eb7f89eed9136c57/info0/c3d62dcd7c3f4b8dacfc9688b850d723 WAL Tools主要是用来查看hbase的 WAL 日志，以及拆分WAL日志 12hbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --dump hdfs://nameservice1/hbase/MasterProcWALs/pv2-00000000000000000048.loghbase org.apache.hadoop.hbase.regionserver.wal.FSHLog --split &#123;wal_log path&#125; hbase wal 命令同样可以用来查看wal日志 CopyTable表备份工具，可以跨集群备份，也可以同一个集群备份。 HashTable/SyncTable表的同步工具，两张表可以运行在不同的集群上。使用这个工具可以进行表的增量备份。 Export支持Mapreduce-based和Endpoint-based两种方式进行Export。 1234# Mapreduce-based Exportbin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]# Endpoint-based Exportbin/hbase org.apache.hadoop.hbase.coprocessor.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]] ImportTsv、CompleteBulkLoad、Import这三个都是批量导入工具。 参考官方文档","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://linqing2017.github.io/tags/HBase/"}]},{"title":"Kafka 如何保存 Offset","slug":"Offset信息保存","date":"2019-03-31T16:00:00.000Z","updated":"2020-05-14T07:59:03.313Z","comments":false,"path":"2019/04/01/Offset信息保存/","link":"","permalink":"https://linqing2017.github.io/2019/04/01/Offset%E4%BF%A1%E6%81%AF%E4%BF%9D%E5%AD%98/","excerpt":"说明Kafka如何保存 Groups 的 Offset 信息。","text":"说明Kafka如何保存 Groups 的 Offset 信息。 Offset保存机制老版本Kafka中，Group的Offset信息保存在Zookeeper的/consumers/{group}/offsets/{topic}/{partition}目录下。此时消费者一般是使用kafka.javaapi.consumer.ConsumerConnector进行消费的，用户在 param 中配置zookeeper.connect。这种情况下，如果 Kafka 集群规模庞大会给 Zookeeper 造成比较大读写负担。 新版Kafka中（具体怎么新不太清楚~），消费者如果使用 org.apache.kafka.clients.consumer.KafkaConsumer 消费数据，Offset信息会保存在一个 Kafka 自带的 topic（__consumer_offsets）中。这种方式下 Offset 信息序列化后保存在本地。 默认情况下，__consumer_offsets 有50个分区。Group 将 group.id 哈希取模后保存到 __consumer_offsets 的对应分区中。 通过以下命令可以查看__consumer_offsets： 12345# describe topickafka-topics --describe --zookeeper localhost:2181 --topic __consumer_offsets# 消费topickafka-console-consumer --bootstrap-server bdnode1:9092,bdnode2:9092,bdnode3:9092 --topic __consumer_offsets --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" --consumer-property exclude.internal.topics=false --from-beginning 通过 kafka-consumer-groups.sh 可以管理 Offset 信息： 123456# 查询指定Group的Offset信息kafka-consumer-groups --bootstrap-server bdnode1:9092,bdnode2:9092,bdnode3:9092 --group KafkaConsumerOnSpark --describle # 重置Offsetkafka-consumer-groups --bootstrap-server bdnode1:9092,bdnode2:9092,bdnode3:9092 --group KafkaConsumerOnSpark --reset-offsets --topic my_topic --to-earliest# 删除Offset（注意删除前需要重置）kafka-consumer-groups --bootstrap-server bdnode1:9092,bdnode2:9092,bdnode3:9092 --group KafkaConsumerOnSpark --delete kafka-consumer-groups.sh 脚本的功能比较强大可以，按照需求修改Offset信息。 Spark Streaming KafkaSpark 项目提供了0.8和0.10两个版本的 Kafka 集成插件。 目前这个两个版本的代码在Spark的源码中都还在维护，但是0.8版本的许多接口已经不再维护。 两个版本提供的 Offset 更新接口也有所差异： Spark 1.6.3 之前的版本中，0.8版本提供KafkaManager类，通过这个类可以将Offset信息更新到Zookeeper中，新版本该Class已经没有了~~！ Spark 2.0.0以上的版本中，1.0版本可以通过以下方式，更新Groups的Offset信息： 123456stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // some time later, after outputs have completed stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)&#125;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://linqing2017.github.io/tags/Kafka/"}]},{"title":"Kafka 功能调研","slug":"Kafka高级特性","date":"2019-03-31T16:00:00.000Z","updated":"2020-05-14T07:59:03.312Z","comments":false,"path":"2019/04/01/Kafka高级特性/","link":"","permalink":"https://linqing2017.github.io/2019/04/01/Kafka%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/","excerpt":"","text":"Kafka StreamKafka Streams是一个客户端lib库，其中输入和输出数据存储在Kafka集群中，主要应用在简单、轻量的流式ETL操作。 Kafka Stream的特点包括： 简单轻巧的客户端库，可以非常方便的嵌入的任意JAVA程序中，并且无需依赖任何kafka之外的服务。 通过Kafka的Partition机制水平扩展 支持有状态操作，如包括 Aggregate算子、Windows算子、Join算子等 支持 exactly-once 语义（可以按照需要配置为at-least-once） 毫秒级延时，支持基于event-time的窗口操作 提供必要的流处理元语、high-level Streams DSL（提供Map、Filter、Join等高级API） 、 low-level Processor API（提供API用于用户定制功能） 分布式实现Kafka Stream 利用 Kafka 本身的分区机制来实现并行任务，Streaming 中任务被分成多个 Task，每个Task处理一个或者多个Partition上的数据。 用户可以对同一个任务启动多个实例，这些实例中通过 application.id 来辨识是否属于同一个任务。一旦某个实例失败，该实例上的task会在其他实例上恢复，并且继续工作。 Kafka Streaming在不同task之间不进行数据共享，因此进行聚合等操作时Stream任务会发生re-partition，同时join等操作也会有一些限制。 Streams DSLStreams DSL是在进行Kafka Stream开发中使用的主要API，包括了以下三个抽象概念： KStream：KStream是record stream的抽象，表示对一个或多个partition中的数据。 KTable：KTable本质上是一个changelog stream，每条流入的记录相当于对KTalbe进行一次Update、INSERT操作。当一条记录的value为null时，表示删除DELETE记录操作。在KTable中Key是唯一的！ GlobalKTable：与KTable的区别在于GlobalKTable获取所有partition中的数据，保存到Application的本地，而KTable只保存当前Application中处理的partition数据。GlobalKTable可以用作Application中的broadcast功能。 Stateless常用算子Stream中算子是强类型操作，进行所有Transform操作时均需要指明算子的输入/输出类型。 Stateless 算子，不要求State Store来存储中间状态，用户可以将结果转换为一张无状态的KTable中来进行交互式的查询。 算子 说明 Branch 根据指定逻辑将Stream分成一个或者多个，应用的场景类似于代码中的switch语法 Filter 过滤KStream或者KTable对象中的Record Inverse Filter 和Filter相反，删除返回True的元素 FlatMap 将一条Record变为多条，如果flatMap之后的操作是join或者group，那么让Stream发生repartition FlatMap (values only) 在key不变的情况下将一条Record分为多条，通常不会造成re-partitioning Peek 遍历每条记录，但是不改变其值，应用该操作的边际效应 Foreach Terminal operation.返回值为void Print Terminal operation.打印（key+”, “+value） Map 改变Stream的Key和Value Map (values only) 改变Stream和Ktable的Value取值 Merge 合并两个Stream的每条记录，保证合并的相对顺序 SelectKey 改变Key的值或者类型 GroupByKey KStream → KGroupedStream GroupBy 相比GroupByKey可以改变Key的取值和类型（对于Stream可以改变Key的类型，对于Table既能改变类型和取值）,相当于selectKey(…).groupByKey()的连续应用 Stateful算子Stateful算子需要使用“state store”来存储中间数据，state stores 的实际实现可以是kv存储、内存中的hashmap、某种数据结构。Kafka允许Stream之外的程序访问该 stream 程序创建的 state stores，这种能力是Kafka实现 Interactive Queries 的基础。 DSL中包括以下几种有状态算子： Aggregating Joining Windowing 其他自定义类型 AggregatingAggregating对于KGroupedStream和KGroupedTable由groupBy算子产生，可以在其后使用聚合类型的算子。这一类算子基于KV类型进行操作，并在相同的Key的Record上进行操作，最终生成一个KTable类型的对象。 Aggregating包括：Aggregate算子、Reduce算子、Count算子三种类型，并且支持窗口操作。 Aggregate算子对 KGroupedStream、KGroupedTable 进行滚动聚合操作，并产生KTable。使用时提供initializer 、adder、subtractor（只有操作KGroupedTable时需要提供）三种接口。 对象为 KGroupedStream 遵循下面的逻辑： null keys 的记录直接被丢弃 first Keys 到达时先调用initializer，之后调用adder 随后相同Key到达时调用adder 对象为 KGroupedTable 遵循下面的逻辑： null keys 的记录直接被丢弃 first Keys 到达时先调用initializer，之后调用adder（相当于INSERT操作） 随后相同Key到达时先调用subtractor，之后调用adder（相当于UPDATE操作） null value 相当于DELETE操作，调用subtractor，并且移除表的KEY Aggregate算子如果跟在windowedBy操作之后，则变为窗口形式。只针对当前窗口Record进行聚合。 Join在Kafka中执行Join操作基于 leftRecord.key == rightRecord.key 来进行。 由于Kafka Stream的Task中只能获取部分Partition的数据，所以如果想要获取正确的Topic结果需要满足data co-partitioning条件，即： 输入的Topic要有相同数目的分区 所有Input Topic的分区策略必须稳定，并且能够保证相同Key被路由到相同分区 支持的Join类型包括：join、leftJoin、outerJoin，并且根据左右对象的不同可以分为以下场景： KStream之间Join KTable之间Join KStream和KTable之间Join KStream-GlobalKTable Join（无需满足data co-partitioning条件） 对Kafka来说整个Join过程是流式进行的，并非在窗口内缓存数据一批执行一次join。 官方文档给出了各种情况下join结果的表格（参考）。 Windowswindows的作用和group算子类似，group算基于key对record分类，而windows则基于时间对record进行分类。同一个窗口的数据被保存在state store中，直到该窗口过期。 Windows并非在要失效后触发操作，而是每当有数据进入窗口时触发一次action。但是也可以使用suppress来抑制窗口的action Kafka支持以下四种窗口： Tumbling time window：翻滚窗口 Hopping time window：实际意义上滑动窗口 Sliding time window：特殊的滑动窗口，是非对齐的，只能用在join操作上 Session window：根据相同Key记录出现的时间间隔来划分窗口 大部分类型的windows划分基于时间，Stream在KafkaStream中包括以下三种时间： Event time：事件时间 Processing time：Stream处理记录的时间 Ingestion time：摄入时间，即数据被写入到Kafka的时间 Kafka会自动为记录嵌入一个时间戳，用户可以在broker或者topic上配置该时间的含义。 在Kafka Stream中，Processor会根据实际情况为生成的records分配时间戳，包括：继承原有timestamp、以当前时间生成新timestamp、聚合操作中继承最后一条record的timestamp。 在Kafka Stream中按照Offset而不是TimeStamp顺序来处理数据，因此对于有状态操作不能保证Record是按照时间顺序来处理的。 开发Tips依赖使用Kafka Streaming时需要包括以下三个依赖，其中最后一个依赖只有使用scala开发时需要添加（CDH版本同样有这三个依赖）。 12345678910111213141516&lt;dependency&gt; &lt;groupid&gt;org.apache.kafka&lt;/groupid&gt; &lt;artifactid&gt;kafka-streams&lt;/artifactid&gt; &lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupid&gt;org.apache.kafka&lt;/groupid&gt; &lt;artifactid&gt;kafka-clients&lt;/artifactid&gt; &lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Optionally include Kafka Streams DSL for Scala for Scala 2.12 --&gt;&lt;dependency&gt; &lt;groupid&gt;org.apache.kafka&lt;/groupid&gt; &lt;artifactid&gt;kafka-streams-scala_2.12&lt;/artifactid&gt; &lt;version&gt;2.3.0&lt;/version&gt;&lt;/dependency&gt; 配置项参考官方文档 配置项 说明 application.id 用于标识Streaming的应用名称。在整个kafka集群中，相同application.id的进程视为一个集群，任务在这些进程中自动均衡。该配置还被用于充当任务的group.id，client.id的前缀，内部topic的前缀，以及一些streaming子目录的前缀。 default.deserialization.exception.handler 处理序列化异常 default.production.exception.handler 处理Streaming程序和Kafka broker的交互异常 default.key.serde/default.value.serde 默认序列化类 num.standby.replicas task的副本数目，指local state stores的副本数目，对于n个部分的实例，需要配置n+1个Kafka Stream实例。 partition.grouper 决定task和partition的映射机制 processing.guarantee at_least_once、exactly_once replication.factor streaming 内部topic的副本数目 state.dir 用来存储local states的目录 timestamp.extractor 从Record中抽取时间戳的Class，支持FailOnInvalidTimestamp、LogAndSkipOnInvalidTimestamp、UsePreviousTimeOnInvalidTimestamp、 WallclockTimestampExtractor rocksdb.config.setter rocksdb的配置，rocksdb被用来作为Kafka Streaming的持久化存储工具 Kafka ConnectKafka Connect 是Kafka和其他系统间的数据管道，支持分布式、单机两种模式部署。用户在配置文件中指定Connectors类，Kafka通关这些Connectors类连接外部系统来传输数据（做的事实际上有点像Flume）。 Kafka 中包括 SourceConnectors 、SinkConnectors 两种类型的Connectors，每个Connectors中包含多个Task，这些Task映射到外部系统的一批数据（比如一张表、文件），是实际的工作执行者。 官网说明了用户应该如何开发一个Connectors，内容包括：接口实现、Offset管理、动态配置、配置文件校验、Schemas、任务启停等等。 相比用户直接用Consumer、Producer的SDK去开发业务，使用Kafka Connect有下面几个优势： 实现一个统一框架，遵循框架来开发能提高代码的质量 有一套 REST API 接口，可以实现传输任务的启动、停止、删除的操作 自动化的Offset管理 Confluent.IO可以获取许多线程实现 总结当业务以Kafka作为主要数据源（Kafka流处理平台）时，KStream、Kafka Connect能发挥最大价值，对当前IData来说这两个功能比较鸡肋。 KStream 是一种较为轻量的流处理框架（功能更少的Flink or Spark），可以用于取代当前IData的部分Spark Streaming业务，能够获得一定的性能提升。但是要在IData中完全引入流式处理的框架，可能需要对业务有大量的修改。 Kafka Connect 生态比较丰富，一些ETL、南北向接口的场景可能值得一用。 参考DSL API Kafka Streaming Scala Processor API interactive-queries","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Kafka","slug":"Kafka","permalink":"https://linqing2017.github.io/tags/Kafka/"}]},{"title":"Kafka 安全认证","slug":"kafka安全相关","date":"2019-03-11T16:00:00.000Z","updated":"2020-05-14T07:59:03.311Z","comments":false,"path":"2019/03/12/kafka安全相关/","link":"","permalink":"https://linqing2017.github.io/2019/03/12/kafka%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/","excerpt":"简要说明Zookeeper对接Kerberos的过程，以及在Zookeeper中使用ACL功能的方案。","text":"简要说明Zookeeper对接Kerberos的过程，以及在Zookeeper中使用ACL功能的方案。 认证在 Kafka 0.9.0 以后的版本，添加了安全相关的功能，包括以下： 支持SSL和SASL两种认证方式，其中SASL框架支持 GSSAPI 、PLAIN 、SCRAM-SHA-256 、OAUTHBEARER 等多种认证引擎。认证发生在：client-broker，broker之间，broker-zookeeper。 支持使用SSL进行数据传输加密。 支持client对topic的读写授权。 支持授权服务可插拔，并且支持与外部授权服务（比如sentry）的集成。 对接Kerberos服务端配置Cloudera导入Kerberos以后，CM自动为Kafka创建了principal账号，并且生成了jaas.conf。 1234567891011121314151617KafkaServer &#123; com.sun.security.auth.module.Krb5LoginModule required doNotPrompt=true useKeyTab=true storeKey=true keyTab=&quot;&#123; process-config &#125;/kafka.keytab&quot; principal=&quot;kafka/&#123; broker-host &#125;@IDATA.RUIJIE.COM&quot;;&#125;;Client &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=&quot;&#123; process-config &#125;/kafka.keytab&quot; principal=&quot;kafka/&#123; broker-host &#125;@IDATA.RUIJIE.COM&quot;;&#125;; 上述配置中，包括Client和KafkaServer两个context： KafkaServer：主要用于broker之间，以及client和broker之间的认证； Client：主要用于broker和zookeeper之间的认证； 安装Kafka的官方文档，Kafka在进行SASL时使用context名称为KafkaServer，在这个配置下可以定义多个不同的listenerName、同一个listenerName下可以定义多个不同的认证机制（PS:这个部分的内容官网描述的不是太清晰，没怎么看懂！）。 官方文档中，kafka.properties涉及到的配置项包括： 1234567listeners=SASL_PLAINTEXT://host.name:port # broker使用的listeners名称security.inter.broker.protocol=SASL_PLAINTEXT # broker之间使用的认证协议sasl.kerberos.service.name=kafka # broker使用的kerberos Principal名称sasl.mechanism.inter.broker.protocol=GSSAPI # 默认配置就是GSSAPIsasl.enabled.mechanisms=GSSAPI # 默认配置就是GSSAPI 涉及到的JVM环境变量包括： 123-Djava.security.krb5.conf=/etc/kafka/krb5.conf-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf 客户端配置客户端可以使用两种方式配置jaas的参数： 通过配置文件jaas.conf指定认证信息，最后通过KAFKA_OPTS=”${KAFKA_OPTS} -Djava.security.auth.login.config=jaas.conf” 指定jaas.conf参数。 1234567KafkaClient &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=&quot;/etc/security/keytabs/kafka_client.keytab&quot; principal=&quot;kafka-client-1@EXAMPLE.COM&quot;;&#125;; 通过sasl.jaas.config在producer/consumer的properties配置文件指定，该方式优先级高于使用jaas.conf指定，通过这种方式可以在同一个JVM中指定多个sasl配置； 12345sasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required \\ useKeyTab=true \\ storeKey=true \\ keyTab=&quot;/etc/security/keytabs/kafka_client.keytab&quot; \\ principal=&quot;kafka-client-1@EXAMPLE.COM&quot;; 其他client配置包括： 123security.protocol=SASL_PLAINTEXTsasl.mechanism=GSSAPIsasl.kerberos.service.name=kafka 其他关于认证的配置启用多种认证配置Kafka支持同时启用多种认证配置，下面的示例中jaas.conf同时指定了Kerberos和秘钥方式的认证： 12345678910111213KafkaServer &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true storeKey=true keyTab=&quot;/etc/security/keytabs/kafka_server.keytab&quot; principal=&quot;kafka/kafka1.hostname.com@EXAMPLE.COM&quot;; org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;admin&quot; password=&quot;admin-secret&quot; user_admin=&quot;admin-secret&quot; user_alice=&quot;alice-secret&quot;;&#125;; 用户同时在kafka.properties中指定： sasl.enabled.mechanisms=GSSAPI,PLAIN # 启动kerberos和密码两种认证方式 security.inter.broker.protocol=SASL_PLAINTEXT sasl.mechanism.inter.broker.protocol=GSSAPI # broker内部指定使用Kerberos认证 PS:Kafka甚至支持client连接同一个集群的不同Broker时，不同Broker采用不同的认证方式！ 授权授权配置Kafka 授权模块是通过插件方式实现，支持为以下ACL方式： 123456Principal P is [Allowed/Denied] Operation O From Host H on any Resource R matching ResourcePattern RP。Principal：表示用户Operation：表示用户操作，包括：Read, Write, Create, Delete, Alter, Describe, ClusterAction, AllHost： 表示发起操作的主机ResourcePattern： 表示一组正则匹配的资源 ACL涉及到的kafka.properties配置包括： 1234authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer # 指定认证插件allow.everyone.if.no.acl.found=false # 指定到任意未关联ACL的topic，是否允许任意人访问super.users=User:idata;User:kafka # 指定超级用户sasl.kerberos.principal.to.local.rules # 指定Kerberos的principal匹配规则，使用默认配置即可 ACLs命令通过Kafka自带的kafka-acls.sh，可以进行权限配置。 CDH中启用Kerberos认证CDH中开启Kerberos时，CM中已经自动生成了JAAS配。kafka在Zookeeper上的元数据依然是全局访问，用户需要进行手工迁移。 Kafka需要进行以下设置，在CDH上开启Kerberos和ACL认证： CM上导入Kerberos完成； kafka.properties安全阀追加以下配置： 123zookeeper.set.acl=trueauthorizer.class.name=kafka.security.auth.SimpleAclAuthorizer #启用sentry时，该配置走的是org.apache.sentry.kafka.authorizer.SentryKafkaAuthorizersuper.users=User:idata;User:kafka # 该配置在CM中有独立配置，但是在启用Sentry时才生效的。需要直接通过安全阀配置。 创建client.properties 配置文件，该配置文件主要是给测试工具使用的。代码中应该直接将配置项嵌入代码中 1234security.protocol=SASL_PLAINTEXTsasl.mechanism=GSSAPIsasl.kerberos.service.name=kafkasasl.jaas.config=com.sun.security.auth.module.Krb5LoginModule required useTicketCache=true; #表示使用手工kinit的方式获取权限，实际在代码中应该通过keytab的方式 使用kafka账号将元数据迁移为安全模式，迁移后只有通过kafka账号能够创建、删除topic。其他账号均不允许！ 1234kinit -kt /opt/idata_security/ktable/kafka.keytab kafka@IDATA.RUIJIE.COMexport KAFKA_OPTS=&quot;$&#123;KAFKA_OPTS&#125; -Djava.security.auth.login.config=/opt/idata_security/kafka_jaas.conf&quot;/opt/cloudera/parcels/CDH/lib/kafka/bin/zookeeper-security-migration.sh --zookeeper.acl=secure --zookeeper.connect=bdnode1:2181zookeeper-client -server bdnode1:2181,bdnode2:2181,bdnode3:2181 setAcl / world:anyone:cdrwa 参考 Kafka Security Command Line Interface","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://linqing2017.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"Kafka","slug":"Kafka","permalink":"https://linqing2017.github.io/tags/Kafka/"}]},{"title":"Impala 安装部署","slug":"2-impala-jdbc","date":"2019-03-10T16:00:00.000Z","updated":"2020-05-14T07:59:03.277Z","comments":false,"path":"2019/03/11/2-impala-jdbc/","link":"","permalink":"https://linqing2017.github.io/2019/03/11/2-impala-jdbc/","excerpt":"关于 Impala jdbc驱动的相关说明","text":"关于 Impala jdbc驱动的相关说明 几个姿势点： impalad的jdbc/odbc端口是21050，通过–hs2_port参数可以指定； impala可以使用 Cloudera JDBC Connector 和 Hive 0.13 JDBC driver 两种驱动（CDH6 官方推荐使用2.5.45、2.6.2版本的impala 驱动）； 使用Haproxy等负载平衡工具时，应该关闭重用连接的配置，同时需要保证工具的超时时间足够大； Cloudera 官方指引 Cloudera connector 驱动下载 Cloudera JDBC 官方文档 HIVE JDBC驱动安装","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Cloudera","slug":"Cloudera","permalink":"https://linqing2017.github.io/tags/Cloudera/"},{"name":"Impala","slug":"Impala","permalink":"https://linqing2017.github.io/tags/Impala/"}]},{"title":"Impala 安装部署","slug":"1-impala安全相关","date":"2019-03-10T16:00:00.000Z","updated":"2020-05-14T07:59:03.237Z","comments":false,"path":"2019/03/11/1-impala安全相关/","link":"","permalink":"https://linqing2017.github.io/2019/03/11/1-impala%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/","excerpt":"关于 Impala 认证、鉴权的一些相关说明","text":"关于 Impala 认证、鉴权的一些相关说明 概述Cloudera 在CDH的用户文档中有相关Impala安全方案，包括：认证、授权、审计。从官方说明来看，授权功能需要对接Cloudera Sentry，而审计功能需要对接付费产品Cloudera Navigator。 官方文档地址： Enabling Kerberos Authentication for Impala Enabling Sentry Authorization for Impala Auditing Impala Operations 认证配置Kerberos通过Cloudera Manager导入Kerberos时，集群已经同步配置了Impala的相关配置。 Kerberos涉及到的配置，只包括下面三个： 12345# Cloudera 将下面的配置自动更新到impala_conf的state_store_flags、impalad_flags、catalogserver_flags中-kerberos_reinit_interval=60-principal=impala/&#123; 服务运行节点 &#125;@IDATA.RUIJIE.COM-keytab_file=/var/run/cloudera-scm-agent/process/&#123; service—_process_dir &#125;/impala.keytab 当为Impalad配置proxy时，需要需要额外生成VIP使用的principal账号，并且需要在impalad的启动配置项里指定vip的principal账号 1234# 通过Cloudera页面配置Impala Daemons Load Balancer后，Cloudera在启动服务时会为用户自动创建vip节点使用的principal账号 --principal=impala/cdh.vip@IDATA.RUIJIE.COM --be_principal=impala/&#123; 服务运行节点 &#125;@IDATA.RUIJIE.COM 参考Impala Security Overview","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Cloudera","slug":"Cloudera","permalink":"https://linqing2017.github.io/tags/Cloudera/"},{"name":"Impala","slug":"Impala","permalink":"https://linqing2017.github.io/tags/Impala/"}]},{"title":"HDFS 的块检查","slug":"hdfs的块检查","date":"2019-03-09T16:00:00.000Z","updated":"2020-05-14T07:59:03.303Z","comments":false,"path":"2019/03/10/hdfs的块检查/","link":"","permalink":"https://linqing2017.github.io/2019/03/10/hdfs%E7%9A%84%E5%9D%97%E6%A3%80%E6%9F%A5/","excerpt":"简单说明HDFS DataNode 在运行过程中执行的：block scanner, volume scanner, directory scanner, disk checker","text":"简单说明HDFS DataNode 在运行过程中执行的：block scanner, volume scanner, directory scanner, disk checker DataNodeDataNode在运行过程中，通过不同的扫描机制处理以下问题： 何时/如何检查文件的完成性； 如何保证内存中的metadata信息和实际的一致性； 处理读取块时候的IO异常； Block Scanner &amp;&amp; Volume Scannerblock的checksum信息，这些信息和block文件放在相同的目录，文件名为 { block-id }-xxxx.meta。这些meta文件用来检查块的完整性。对于finalized状态的block，在hdfs上应该包括block文件+meta文件。 Block Scanner是为了检查block文件是否存在损坏。 DataNode在进行Block Scanner时，针对每个Volume会启动一个检查线程，这个检查线程称为Volume Scanner。Volume Scanner 负责读取自己磁盘目录下的所有block，并且进行校验码计算，将计算结果和meta文件中的值比较来判断块文件是否完整。 通常情况下，block scanner要撸一便整个DN下的所有块文件，会发生巨大的IO。DN通过以下机制在节约IO带宽的同时，尽快发现坏块： DN在惊醒Block Scanner时将块分为：可疑块和常规块。可疑块指client（用户和其他DN）在读取过程中发生本地IO异常的块，这些块被DN放到队列中，在扫描时优先进行检查。 任意一个块被标记为可疑块，DN会立即检查这个块，并且跟踪这个块的状态10 min。之后块被移除可疑列表。 当可疑块的队列为空时，DN依次检查其他的块。当所有块检查完成以后，Volume Scannr 进入休眠状态，等待下一个Scanner周期。默认情况扫描周期间隔为3周（dfs.datanode.scan.period.hours）。 如果一个周期内无法扫描完成所有块，那么不会进入休眠，而是直接进入下一个周期。在休眠过程中一旦有块变成可疑块，那么扫描线程会立刻被唤醒。 DN会将扫描的进度保存在scanner.cursor中，重启DN会从该文件恢复上次的扫描 dfs.block.scanner.volume.bytes.per.second 可疑限制volume scan的带宽，默认为1m每秒 Directory ScannersDN会在内存中保存Block的位置信息（指的是block文件所处的目录），以及Block的状态信息。 DN 通过Directory Scanners保证内存位置信息和实际的一致性。当Directory Scanners发现block文件、meta文件丢失时，会将block标记为corrupted，DN在下一次汇报中，将这个块汇报给NN。Directory Scanners只检查finalized状态的block，并且DN开机后立即运行。 参数 默认值 说明 dfs.datanode.directoryscan.throttle.limit.ms.per.sec 1000 扫描线程在一秒内允许运行的的毫秒数，默认为1000ms，即不限制 dfs.datanode.directoryscan.threads 1 扫描的线程数 dfs.datanode.directoryscan.interval 21600 两次扫描的间隔时间，默认为6小时 Disk CheckerDN 检查HDFS用户是否有Volume目录下，finalized、tmp、rbw这三个目录的读写权限，并且只检查这三个目录，并不会检查子目录。 Disk Checker是hdfs中非常保守的检查机制，只有在DN进程操作block发现IOException时才会发生，并且执行一次只需要5~6s。 如果Volume在Disk Checker中失败，那么整个volume会被DN禁用，当volume Checker失败的Volume超过 dfs.datanode.failed.volumes.tolerated，那么DN会关闭。 心跳和ReportDN的心跳和Block Report，即向NN汇报block以及自身的各种信息，这些信息基于上述各种扫描的结果。 心跳默认情况下DN 三秒发一次心跳，心跳的信息包括：磁盘容量，使用率等等基础信息。 Block Report分为增量Report 和 全量Report。Block Report 发送的内容包括： block ID generation stamp block在DN上的文件大小 心跳、block report的代码，均是实现在org.apache.hadoop.hdfs.server.datanode.BPServiceActor中。其中，BPServiceActor#offerService的offerService方法是DN的BP主循环，负责调用心跳（sendHeartBeat方法）、全量block Report（blockReport方法）。 同时，BPServiceActor中还实现了IncrementalBlockReportManager对象，BPServiceActor调用该对象的sendIBRs接口进行增量回报。 全量汇报会对NN和DN都产生压力，默认情况下当DN的Block超过100w时，DN会将报告分多次发送（该配置为dfs.blockreport.split.threshold）。当NN初次启动时，NN处理全量块汇报的时间长度，会影响HDFS的启动速度。 Case在生产环境的五节点环境中，间歇性发现DN出现Oom的现象。 调整JVM内存从 1G 到 2G DN进程不再推出，但是观察DN的内存使用情况发现每个6小时，DN的内存使用率有一个明显的尖峰。 由于该集群的小文件非常之多，总共有将近300w个块，平均每个DN节点需要管理180w个块文件。 由于内存尖峰的周期是6个小时，所以怀疑DN的以下两个操作（默认周期均是6小时）占用内存： 6个小时一次的全量block report 6个小时一次的Directory Scanner 以下两个方式验证： 通过 hdfs dfsadmin -triggerBlockReport ip:50020 手工出发全量block report 修改dn节点的Directory Scanners周期 结果发现，通过hdfs dfsadmin -triggerBlockReport触发全量汇报时，DN的内存没有任何波动。而改动Directory Scanners周期的DN，内存尖峰周期变为2个小时。 oozie-err 上述测试，可以验证Directory Scanner是造成DN内存上升的原因。 分析Directory Scanners 源码（org.apache.hadoop.hdfs.server.datanode.DirectoryScanner）发现，DN在进行Scanner时需要为每个Block创建一个ScanInfo对象，该对象的固定大小108byte，并且会创建大量临时File对象。 参考环境5156488块，平均每个DN节点309w个block（五节点3副本）。因此在执行Directory Scanner时需要额外申请318mb的内存，并且而外创建608w个File对象。 oozie-err 参考HDFS-4461发现，Directory Scanners在Hadoop 2.1时已经被优化过一轮。原先Scanner保存的是File对象而不是字符串。 该对象包含以下内容： 123456789101112131415161718192021222324252627282930static class ScanInfo implements Comparable&lt;ScanInfo&gt; &#123; // blockId 8byte private final long blockId; // block文件的相对地址，格式为subdir9/subdir9/blk_1074334048（随着block数目的增加subdir会多级嵌套，所以这里大小不定。固定是16bit*30/8=60byte） private final String blockSuffix; // meta文件名的后缀，格式为 blk_1074334048_593224.meta，后缀应当为_593224.meta（固定是17位16bit*12/8=24byte） private final String metaSuffix; // 引用对象保存一个指针 8byte private final FsVolumeSpi volume; // block 文件大小 8byte private final long blockFileLength; // Directory Scanner进行扫描时，需要创建临时File对象，获取block文件的句柄信息。 File getBlockFile() &#123; return (blockSuffix == null) ? null : new File(volume.getBasePath(), blockSuffix); &#125; // Directory Scanner进行扫描时，需要创建临时File对象，获取block meta文件的句柄信息。 File getMetaFile() &#123; if (metaSuffix == null) &#123; return null; &#125; else if (blockSuffix == null) &#123; return new File(volume.getBasePath(), metaSuffix); &#125; else &#123; return new File(volume.getBasePath(), blockSuffix + metaSuffix); &#125; &#125; ...... 参考HDFS DataNode Scanners and Disk Checker Explained The Hadoop Distributed File System Directory Scanner 引发的GC案例","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://linqing2017.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"HDFS","slug":"HDFS","permalink":"https://linqing2017.github.io/tags/HDFS/"}]},{"title":"Hive 安全认证相关","slug":"4-Hive安全相关","date":"2019-03-04T16:00:00.000Z","updated":"2020-05-14T07:59:03.278Z","comments":false,"path":"2019/03/05/4-Hive安全相关/","link":"","permalink":"https://linqing2017.github.io/2019/03/05/4-Hive%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/","excerpt":"Hive的Kerberos接入方案，以及ACLs！","text":"Hive的Kerberos接入方案，以及ACLs！ 认证通过 Cloudera 接入Kerberos之后，Hive 默认已经完成了用户认证的配置。无论使用 hive-cli 或 beeline ，在用户连接之前均需要手工执行kinit登录一个principal。 默认情况下，通过以下方式连接： 123456789kinit -kt /opt/idata_security/ktable/idata.keytab idata# 通过hive-clihive# 通过beeline，principal 指的是用户启动HS2的principal，而非实际用户beeline -u \"jdbc:hive2://bdnode3:10000/;principal=hive/_HOST@IDATA.RUIJIE.COM\"# 如果需要模拟任意用户，那么可以使用HDFS操作用户（组）的票据kinit -kt /opt/idata_security/ktable/hdfs.keytab hdfsbeeline -u \"jdbc:hive2://bdnode3:10000/;principal=hive/_HOST@IDATA.RUIJIE.COM;hive.server2.proxy.user=&#123;&#123;任意用户&#125;&#125;\" HiveServer2的认证方式HiveServer2的认证方式通过 hive.server2.authentication 配置，默认情况下为None，即不进行认证。 HS2支持：NONE (uses plain SASL), NOSASL, KERBEROS, LDAP, PAM, CUSTOM 。配置为KERBEROS时需要指定下面两个配置： hive.server2.authentication.kerberos.principal hive.server2.authentication.kerberos.keytab 配认证之后，需要将HS2的hive.server2.enable.doAs打开！并且将hadoop.proxyuser.hive.hosts/groups配置为*！ MetaStore的认证方式参考下面的配置： 123456789101112&lt;property&gt; &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;value&gt;hive/_HOST@IDATA.RUIJIE.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt; &lt;value&gt;hive.keytab&lt;/value&gt;&lt;/property&gt; HIVE访问HBase外表配置HBase的security之后，如果需要由Hive访问外表，那么需要添加以下环境变量（客户端以及hs2的hive-env.sh） 1HIVE_OPTS=\"-hiveconf hbase.security.authentication=kerberos -hiveconf hbase.master.kerberos.principal=hbase/_HOST@IDATA.RUIJIE.COM -hiveconf hbase.regionserver.kerberos.principal=hbase/_HOST@IDATA.RUIJIE.COM -hiveconf hbase.zookeeper.quorum=bdnode1,bdnode2,bdnode3\" 并且需要将上面的三个配置加入到HS2的白名单中： 1hbase\\.security\\.authentication|hbase\\.master\\.kerberos\\.principal|hbase\\.regionserver\\.kerberos\\.principal|hbase\\.zookeeper\\.quorum 授权默认情况下Cloudera只会完成 Hive 的认证配置，不会进行授权相关配置。 当用户操作Hive中的数据时，需要元数据和数据文件的相关权限。默认情况下： metastore没有ACLs控制，用户可以任意通过drop、alter等命令修改元数据； 数据文件处在hadoop的权限控制之下，hive client用户只能修改自己在HDFS被授权的部分； 由于操作元数据是没有ACLs控制的，因此可以认为默认情况下所有HIVE都是有最大权限的！！！ HIVE创建DB、表、分区、执行load/insert等命令时，在HDFS中创建的文件（夹）遵循hdfs的权限集成规则： user identity 为客户端的用户ID group identity 继承父目录的group信息 权限信息集成HDFS的umask配置 通过hive.warehouse.subdir.inherit.perms配置可以改变上面的集成策略： DB目录的权限继承warehouse的权限 Table目录的权限继承DB目录的权限（或者warehouse的权限） 外部表的权限继承父目录的权限 Partition目录的权限继承表目录的权限 table文件的权限继承父文件夹的权限 在CDH6.1中。Cloudera将/user/hive/warehouse的权限设置为777t，并且hive.warehouse.subdir.inherit.perms配置为True。 当hive.warehouse.subdir.inherit.perms打开时，权限继承可能失败，但是这不会让HIVE操作失败，文件会回退到HDFS的默认继承规则。 用户类型安全模式下，Hive的授权主要有以下两种场景： 直接操作HDFS类型，包括：同通过 HCatalog API 操作的各类组价，如 Apache Pig、MR、Impala、Spark SQL等等。这类用户需要同时满足HDFS和metastore的用户认证 Hive作为SQL引擎的类型：主要指通过HiveServer2进行JDBC、ODBC操作的用户 授权方式Hive的授权方式当前包括以下几种： 认证方式 说明 Storage Based Authorization in the Metastore Serve 这种方式基于HDFS的文件权限来控制用户的行为，适合直接操作HDFS文件的用户。 如果用户通过HiveServer2来访问Hive数据，那么需要配置hive.server2.enable.doAs为true。这种方式可以提供Database、Table、Partition级别的控制,并且能够对所有的用户起作用。该模式下，在Hive中进行授权的是metastore服务。 SQL Standards Based Authorization in HiveServer2 主要针对使用HiveServer2场景的权限控制，能够提供row、column的权限控制。 对直接访问HDFS数据的用户，这种安全方式没有作用。 这种模式中进行授权的是HiveServer2 Ranger &amp; Sentry Ranger和Sentry是两款安全相关的插件，提供动态row、column的acls，以及审计等功能。Ranger是基于策略的管理，而Sentry是基于传统的RBAC管理 Legacy Mode hive 2.0.0之前的默认授权方式，当前已经被废弃。官方文档中介绍这是一种不完善的认证机制？ 基于文件系统的授权这是一个简陋授权系统，主要包括： 基于HDFS的权限控制对数据库、表、分区进行授权 metastore基于HDFS的文件（夹）权限，对元数据进行保护 涉及的配置项为： 配置项 值 hive.metastore.pre.event.listeners org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener hive.security.metastore.authorization.manager org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider hive.security.metastore.authenticator.manager org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator hive.security.metastore.authorization.auth.reads true 下面的例子简要说明文件系统授权的Demo： 1234567891011121314# 通过idata用户创建一张外部表create EXTERNAL table idata_acls ( id int, name string)row format delimitedfields terminated by ','LOCATION '/tmp/idata_acls';# 上传表数据到hdfs的/tmp/idata_acls目录，修改test.txt文件权限hdfs dfs -put test.txt /tmp/idata_acls# 修改文件的所有组为idata:idata, 权限为600。 、# 这时候idata用户可以正常删表、读表，使用root用户登录，执行select、drop表等操作会发生错误hdfs dfs -chmod 600 /tmp/idata_acls/test.txthdfs dfs -chown idata:idata /tmp/idata_acls/test.txt 这种ACLs机制，有以下特点： 可以保护metastore中的元数据不被误删，在无ACLs控制时任意用户都能删除metastore的元数据； 需要手工调用HDFS命令，来控制DB、Table、Partition的文件夹，实现ACLs； 配合HDFS的ACLs功能，能够一定程度的提高灵活性（dfs.namenode.acls.enabled配置为true）； 对于Spark SQL、Impala等服务，只能通过这种方式控制权限； hdfs的超级用户，有所有表权限； 除了MetaStore，HCatalog同样支持该方式（可以参考应用中的文档） 标准SQL授权标准SQL授权主要针对使用HS2的用户进行授权，提供细粒度的权限控制，HiveServer2在编译过程中，进行权限识别。对 Hive Cli、Spark SQL、Impala等直接通过元数据操作 HDFS 文件的client，这种授权模式不起作用。 HS2的标准SQL授权可以和metastore文件系统的授权同时使用。通常情况下 Hive 的安全控制遵循以下方式： 通过Kerberos提供权限认证，任意用户需要获得TGT才可使用； metastore 使用文件系统的授权，Impala、Spark SQL、MR 集群内部用户各自作为特权用户管理各自账户下的数据； HS2提供标准SQL授权，对集群外部用户实行严格的分级授权，用户不应该使用 Hive-Cli 操作HIVE； 权限模型HS2使用标准的RBAC授权，通过Role和User绑定进行授权。 通常情况下，一个用户可以关联多个Role。但是用户在执行hive查询时，只拥有当前用户的的权限。用户使用”show current roles;” 命令可以查询当前的role，通过”set role”命令可以切换到其他用户。 Hive中包括以下两个特殊Role：public 和 admin 所有用户都属于public角色，如果需要给所有用户赋权，那么可以将权限和public角色绑定 通常情况下database的管理员和admin绑定，admin用户可以创建/删除role，默认情况下用户需要通过”set role admin;”获取admin权限 在HIVE中，用户名和组名有以下特定： role不是大小写敏感的，user是大小写敏感的； 角色/用户名，可以包含任意Unicode字符（前提是使用 hive.support.quoted.identifiers 配置的转义符转义） Hive包括以下Privileges，用户相关操作需要的权限可以参考官网的Privileges Required for Hive Operations： Privileges 说明 SELECT 对象的读权限 INSERT 对象的追加写权限 UPDATE 对象的update权限 DELETE 对象的删除权限 ALL PRIVILEGES 上述所有权限集合 涉及的对象包括： 表 视图 table、view 的创建者默认拥有所有权限 权限模型不涉及Database的权限管理，只有部分操作时会考虑DB的所有权。通过 alter database 命令可以将数据库的owner指定为用户或者组 相关SQL命令ROLE相关命令 123456789101112131415161718192021222324--- admin 相关命令CREATE ROLE role_name;DROP ROLE role_name;SHOW ROLES;--- 关联roles到用户（或者其他role）,使用WITH ADMIN OPTION配置时，用户可以将获得role转售GRANT role_name [, role_name] ...TO USER linqing, ROLE idata ...[ WITH ADMIN OPTION ];--- 收回roles，ADMIN OPTION FOR 表示回收有用户的role转售权限REVOKE [ADMIN OPTION FOR] role_name [, role_name] ...FROM USER linqing, ROLE idata ... ;--- 查询用户/Role当前关联的roleSHOW ROLE GRANT (USER|ROLE) principal_name;--- 查询所有关联到role的用户/roleSHOW PRINCIPALS role_name;--- 普通用户相关命令SHOW CURRENT ROLES;SET ROLE (role_name|ALL|NONE); 对象相关命令 12345678910111213141516171819202122232425--- 给予表/视图的相关权限GRANT priv_type [, priv_type ] ... ON table/view table_or_view_name TO principal_specification [, principal_specification] ... [WITH GRANT OPTION];--- 从用户上回收表/视图的相关权限REVOKE [GRANT OPTION FOR] priv_type [, priv_type ] ... ON table/view table_or_view_name FROM principal_specification [, principal_specification] ... ;--- 展示当前用户在某张表（或者所有表）上的权限SHOW GRANT [principal_specification] ON (ALL | [TABLE] table_or_view_name);/*principal_specification : USER user | ROLE role priv_type : INSERT | SELECT | UPDATE | DELETE | ALL*/ 限制HS2配置标准SQL授权之后，部分命令在beeline中被限制使用： dfs、add、delete、compile、reset 命令在权限模式下被禁用，通过配置 hive.security.command.whitelist 可以解除禁用； set 命令能够设定的参数会被部分限制，参考配置hive.security.authorization.sqlstd.confwhitelist 、hive.security.authorization.sqlstd.confwhitelist.append、hive.conf.restricted.list ； 默认情况下允许执行所有内建udf，配置hive.server2.builtin.udf.whitelist、hive.server2.builtin.udf.blacklist可以限制用户可以执行的udf； 只有管理员角色，才能添加、删除函数或者宏。通过hive.users.in.admin.role可以指定管理员用户； 无法使用 transform 函数 配置项全局配置项： 配置项 值 说明 hive.server2.enable.doAs false 禁止使用代理，所有操作通过HS2完成。 但是在HS2中是以实际的Client用户ID进行Check的！！ hive.users.in.admin.role hive,idata 需要配置为admin角色的用户 hive.security.metastore.authorization.manager org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly 添加该配置后，所有访问远程metastore的authorization api 调用会被拒绝。此时，HS2使用一个内嵌的metastore工作，拥有authorization api的调用权限 hive.security.authorization.manager org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory 配置所有table、view的创建者有所有的权限 HS2服务端配置项： 配置项 值 hive.security.authorization.manager org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory hive.security.authorization.enabled true hive.security.authenticator.manager org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator hive.metastore.uris ‘ ‘ 在CDH上配置SQLstd授权由于CDH魔改了HIVE的几个配置导致开启授权后，有两个配置hive.query.redaction.rules和hive.exec.query.redactor.hooks无法用set命令修改，当用户使用beeline连接HS2失败。 出现下面的异常： 12319/03/07 19:21:38 [main]: WARN jdbc.HiveConnection: Failed to connect to bdnode3:10000Error: Could not open client transport with JDBC Uri: jdbc:hive2://bdnode3:10000/;principal=hive/_HOST@IDATA.RUIJIE.COM: Failed to open new session: java.lang.IllegalArgumentException: Cannot modify hive.query.redaction.rules at runtime. It is not in list of params that are allowed to be modified at runtime (state=08S01,code=0)Beeline version 2.1.1-cdh6.1.0 by Apache Hive 在HS2日志也会出现，下面的异常： 1Cannot modify hive.exec.query.redactor.hooks at runtime. It is not in list of params that are allowed to be modified at runtime 可以将hive.query.redaction.rules和hive.exec.query.redactor.hooks添加到白名单，解决该问题。在HS2的hive-site.xml配置下面的参数开启sqlstd授权。 12345678910111213141516171819202122&lt;!-- 注意：hive.server2.enable.doAs 需要配置为false --&gt;&lt;property&gt; &lt;name&gt;hive.users.in.admin.role&lt;/name&gt; &lt;value&gt;idata&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.authorization.sqlstd.confwhitelist.append&lt;/name&gt; &lt;value&gt;hive\\.exec\\.query\\.redactor\\.hooks|hive\\.query\\.redaction\\.rules&lt;/value&gt;&lt;/property&gt; 参考LanguageManual+Authorization HCatalog Authorization Setting Up HiveServer2 Using Hive to Run Queries on a Secure HBase Server HiveServer2 Security Configuration Hive Metastore Server Security Configuration Permission Inheritance in+Hive Configuration Properties","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Cloudera","slug":"Cloudera","permalink":"https://linqing2017.github.io/tags/Cloudera/"},{"name":"Hive","slug":"Hive","permalink":"https://linqing2017.github.io/tags/Hive/"},{"name":"安全","slug":"安全","permalink":"https://linqing2017.github.io/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"在Kerberos环境中运行Spark On Yarn","slug":"运行任务","date":"2019-03-03T16:00:00.000Z","updated":"2020-05-14T07:59:03.318Z","comments":false,"path":"2019/03/04/运行任务/","link":"","permalink":"https://linqing2017.github.io/2019/03/04/%E8%BF%90%E8%A1%8C%E4%BB%BB%E5%8A%A1/","excerpt":"Kerberos接入的场景下，提交spark任务。","text":"Kerberos接入的场景下，提交spark任务。 Yarn当Hadoop接入Kerberos之后，CDH对向Yarn的提交任务的用户进行了限制： 不能是root用户（参考：hadoop-yarn-project\\hadoop-yarn\\hadoop-yarn-server\\hadoop-yarn-server-nodemanager\\src\\main\\native\\container-executor\\impl\\container-executor.c） 用户的id必须大于min.user.id 用户不能再黑名单（banned.users）中 如果用户需要使用小于min.user.id的账号提交yarn任务（比如impala、hue、hive等账号），那么需要将用户加入到allowed.system.users配置中 Spark对Spark任务来说，在yarn上运行时可以自动获取tgt，并自动刷新票据。 通过以下配置可以调整Kerberos登录参数： 参数名称 说明 备注 spark.yarn.keytab 使用的keytab文件 可以在spark-submit中通过–principal指定 spark.yarn.principal kerberos指定的用户名 可以在spark-submit中通过–keytab指定 spark.yarn.access.hadoopFileSystems 需要访问Hadoop，可以指定多个 默认是none spark.yarn.kerberos.relogin.period 检查票据失效的周期 默认是1m spark.security.credentials.${service}.enabled 访问Hive、HBase等服务时是否通过Kerberos用户访问 默认是true 参考security kerberosrunning on yarn","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://linqing2017.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"Yarn","slug":"Yarn","permalink":"https://linqing2017.github.io/tags/Yarn/"},{"name":"Spark","slug":"Spark","permalink":"https://linqing2017.github.io/tags/Spark/"}]},{"title":"Zookeeper 安全认证","slug":"zookeeper安全认证","date":"2019-02-28T16:00:00.000Z","updated":"2020-05-14T07:59:03.316Z","comments":false,"path":"2019/03/01/zookeeper安全认证/","link":"","permalink":"https://linqing2017.github.io/2019/03/01/zookeeper%E5%AE%89%E5%85%A8%E8%AE%A4%E8%AF%81/","excerpt":"简要说明Zookeeper对接Kerberos的过程，以及在Zookeeper中使用ACL功能的方案。","text":"简要说明Zookeeper对接Kerberos的过程，以及在Zookeeper中使用ACL功能的方案。 Kerberos认证Zookeeper 支持SASL框架，能够支持Kerberos、DIGEST-MD5等认证机制。 下面简要说明，Cloudera给出Zoookeeper集成Kerberos的配置方法： 修改zoo.cfg安装完成KDC后，修改 zoo.cfg 配置文件,修改添加以下内容： 123456789101112# 移除Principal中的Realm和host信息kerberos.removeHostFromPrincipal=truekerberos.removeRealmFromPrincipal=true# SASL插件authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider # Zookeeper 节点使用 Principal _HOST表示zk server所在的节点主机名quorum.auth.kerberos.servicePrincipal=zookeeper/_HOST# 配置Zk几点之间使用SASL认证quorum.auth.learnerRequireSasl=true quorum.auth.serverRequireSasl=true 创建Kerberos账号为每个zookeeper节点创建对应的krb账号，账号名为zookeeper/_HOST。 导出keytab文件到各个节点目录。 创建jaas.conf文件在ZK的配置文件目录创建，jaas.conf文件。在启动Zookeeper进程时指定环境变量，JVMFLAGS=”-Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf”。 1234567891011121314151617181920212223242526Server &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;zookeeper.keytab&quot; #按照实际情况修改 storeKey=true useTicketCache=false principal=&quot;zookeeper/bdnode1@IDATA.RUIJIE.COM&quot;; #按照实际情况修改&#125;; QuorumServer &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;zookeeper.keytab&quot; storeKey=true useTicketCache=false principal=&quot;zookeeper/bdnode1@IDATA.RUIJIE.COM&quot;;&#125;; QuorumLearner &#123; com.sun.security.auth.module.Krb5LoginModule required useKeyTab=true keyTab=&quot;zookeeper.keytab&quot; storeKey=true useTicketCache=false principal=&quot;zookeeper/bdnode1@IDATA.RUIJIE.COM&quot;;&#125;; Client连接ZK客户端连接Zookeeper时，需要在/etc/zookeeper/conf目录下创建以下两个文件： 1234567891011# 创建jaas.confClient &#123; com.sun.security.auth.module.Krb5LoginModule required ticketCache=&quot;/tmp/krb5cc_0&quot; useTicketCache=true;&#125;;# 创建java.envexport JVMFLAGS=&quot;-Djava.security.auth.login.config=/etc/zookeeper/conf/jaas.conf&quot; ZooKeeper Access ControlZookeeper官方文档有针对ACLs章节的相关讨论，下面简单终结。 Zookeeper 的权限包括：CREATE、READ、WRITE、DELETE、ADMIN。 Zookeeper 的身份的认证方式： 认证方式 说明 world 表示所有客户端都能够通过认证 auth 表示任何经过身份验证的用户，当使用Kerberos时表示对应的principal账户通过认证 digest 用户名/密码方式通过认证 host 客户端主机名通过认证 ip 客户端IP通过认证 常用的ACLs命令 12345678# 获取当前Node的权限信息getAcl path# 指定当前Node的权限，使用Kerberos时，参考下面的命令，表示给当前用户所有权限setAcl /test auth::cdrwa# 给任意用户只读权限setAcl /test world:anyone:r 参考Hardening Apache ZooKeeper Security: SASL Quorum Peer Mutual Authentication and Authorization ZooKeeper and SASL ZooKeeper Authentication jaas.conf参数说明 ZooKeeperAccessControl","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://linqing2017.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://linqing2017.github.io/tags/Zookeeper/"}]},{"title":"HBase 安全相关配置","slug":"8-HBase安全相关","date":"2019-02-24T16:00:00.000Z","updated":"2020-05-14T07:59:03.280Z","comments":false,"path":"2019/02/25/8-HBase安全相关/","link":"","permalink":"https://linqing2017.github.io/2019/02/25/8-HBase%E5%AE%89%E5%85%A8%E7%9B%B8%E5%85%B3/","excerpt":"HBase 安全相关配置。","text":"HBase 安全相关配置。 Kerberos认证HBase配置Kerberos时，需要Hadoop和Zookeeper同时配置Kerberos接入。 Server的hbase-site.xml配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!-- 默认情况下是simple，表示不使用认证 --&gt;&lt;property&gt; &lt;name&gt;hbase.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt;&lt;!-- 开启Kerberos的安全授权，通常接入Kerberos以后授权必须一起打开 --&gt;&lt;property&gt; &lt;name&gt;hbase.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- Master、regionserver、thrift使用的principal账号 --&gt;&lt;property&gt; &lt;name&gt;hbase.master.kerberos.principal&lt;/name&gt; &lt;value&gt;hbase/_HOST@IDATA.RUIJIE.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.kerberos.principal&lt;/name&gt; &lt;value&gt;hbase/_HOST@IDATA.RUIJIE.COM&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.thrift.kerberos.principal&lt;/name&gt; &lt;value&gt;hbase/_HOST@IDATA.RUIJIE.COM&lt;/value&gt;&lt;/property&gt;&lt;!-- Master、regionserver、thrift不同组件使用的keytable文件 --&gt;&lt;property&gt; &lt;name&gt;hbase.master.keytab.file&lt;/name&gt; &lt;value&gt;hbase.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.keytab.file&lt;/name&gt; &lt;value&gt;hbase.keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.thrift.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hbase/conf/hbase.keytab&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置相关的coprocessor --&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessControllerorg.apache.hadoop.hbase.security.visibility.VisibilityController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController,org.apache.hadoop.hbase.security.visibility.VisibilityController,org.apache.hadoop.hbase.security.token.TokenProvider,org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint&lt;/value&gt;&lt;/property&gt; 配置jaas.conf在Apache hbase版本的配置文件中，可以在hbase-env.sh文件中指定jaas.conf文件的地址，配置该文件主要是为了让HBase通过特权账户访问Zookeeper。 1export HBASE_OPTS=\"-Djava.security.auth.login.config=$HBASE_CONFIG_DIR/jaas.conf\" Client相关配置当HBase配置Kerberos认证之后，Client需要以下额外配置： 12345678910111213141516&lt;property&gt; &lt;name&gt;hbase.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt;&lt;!-- 以下两个参数需要2.2.0以上版本才可生效，CDH6.1.0集成的2.1.0刚好差一个版本 --&gt;&lt;property&gt; &lt;name&gt;hbase.client.keytab.file&lt;/name&gt; &lt;value&gt;/local/path/to/client/keytab&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.client.keytab.principal&lt;/name&gt; &lt;value&gt;foo@EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; Thrift、REST 访问HBase略 数据安全对接Kerberos后，可以针对用户需要，为HBase开启不同的数据安全策略（必须配置）。 HBase提供以下安全策略： 基于 RBAC 的访问控制，包括：命名空间、表、列族、列的ACL 基于可见性标签的 Cell 访问控制 HFiles 和 WAL 的数据透明加密 Tag机制HBase的部分安全功能需要 HFile V3 格式的支持。 1234&lt;property&gt; &lt;name&gt;hfile.format.version&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; Tag 是 HFile v3 的新特性。 Tag是cell的metadata资源，被直接存储在HFiles中，一个 Cell 可以包括一个或者多个 tag 。HBase的cell级别ACL，以及visibility labels 需要通过 Tag 能力支持。 Tag可以在列族级别指定压缩的格式（当WAL使用加密时，不支持对Tag进行压缩）。用户读取Cell时，Coprocessors 会在RPC传输层分离Tag，因此对用户来说Tag是不可见的。 ACLsHBase 提供用户/用户组的权限认证功能。 由于HBase本身不保存user-groups映射，所有的映射关系来自Hadoop，通常情况下在HBase的RBAC体系中，Group被作为Role。 ACLs相关配置HBase 的ACL控制通过 Coprocessors 实现，大部分ACL逻辑实现在org.apache.hadoop.hbase.security.access.AccessController类中。 配置HBase 的ACL包括以下配置： 1234567891011121314151617181920212223&lt;property&gt; &lt;name&gt;hbase.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController, org.apache.hadoop.hbase.security.token.TokenProvider&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.regionserver.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;&lt;/property&gt;&lt;!-- 启用X权限的相关配置 --&gt;&lt;property&gt; &lt;name&gt;hbase.security.exec.permission.checks&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 权限类型HBase提供的权限包括： 权限 说明 备注 Read 读权限 Write 写权限 Execute 执行权限，即执行相关的Coprocessors的权限 Create 创建表、删除表的权限 Admin 管理员权限，包括：表的balance操作、region assin、grant、revoke等操作 由于Admin权限能够给自己赋权，因此该权限是最高权限 权限范围包括以下权限范围： Scope 说明 备注 Superuser HBase中的超级用户，默认情况下，启动用户hbase是超级用户。HBase中可以配置多个超级用户。超级用户有一切权限 hbase.superuser配置项可以指定超级用户 global 整个HBase空间的权限范围 namespaces 命名空间范围 tables 表级权限范围 ColumnFamily 列族级权限范围 常用权限命令12345678910111213141516grant 'user', 'RWXCA', 'TABLE', 'CF', 'CQ'-- 为admin用户组提供global权限grant '@admin', 'RWXCA'-- 为用户提供命名空间权限grant 'root','RWXCA','@my-NS'-- 为用户提供表级权限grant 'root','RW','my-table'-- 为用户提供列族级权限grant 'root', 'RW', 'my-table', 'cf1'-- 为用户提供列权限grant 'root', 'RW', 'my-table', 'cf2','A' 1234/***需要注意的是revoke操作会一次回收，用户（组）在某个scope内的所有权限。当前好像没有办法只回收部分权限***/revoke 'user', '@&lt;namespace&gt;', 'table','cf','qualifier' 1234567-- 打印某个范围内的所有权限信息user_permission '&lt;scope&gt;'/***在HBase中所有权限信息保存在，hbase:acl表中，可以通过超级用户查看***/scan 'hbase:acl' Cell/Row级别的权限Cell/Row级别的权限控制，需要而外配置以下参数： 1234567891011&lt;!-- Cell权限需要启动下面两个配置 --&gt;&lt;property&gt; &lt;name&gt;hfile.format.version&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.security.access.early_out&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 通过以下命令提供Cell权限： 1234567grant 'table', &#123;'user'=&gt;'RW', ... &#125;, &#123; 筛选条件 &#125;-- 提供root、admin用户在linqing:test，表中的相关行的权限grant 'linqing:test',&#123;'root'=&gt;'RW','admin'=&gt;'W'&#125;,&#123;FILTER =&gt; \"(PrefixFilter('roww'))\"&#125;-- 收回root用户在linqing:test，表中的相关行的权限grant 'linqing:test',&#123;'root'=&gt;''&#125;,&#123;FILTER =&gt; \"(PrefixFilter('roww'))\"&#125; PS: 关于筛选条件可以参考官网 可视化标签Cell权限控制是通过可视化标签实现的。 通常情况下可视化标签这个feature，可以作为HBase的视图特性！ 可视化标签只需要VisibilityController类就可以运行！不需要配置AccessController 通过标签，可以实现以下效果： 在A用户有整张表的读权限是，使某些Cell对A来说不可见。 12345678910111213141516171819-- 创建一个标签add_labels 'linqing'-- 将表的cf1，打上可见性标签set_visibility 'linqing:test','linqing',&#123;COLUMN =&gt;'cf1'&#125;/****这时对任意一个，没有标签授权的用户，linqing:test:cf1 都不可见****/-- 将标签分配给用户，linqing:test:cf1 可见set_auths 'root','linqing'-- 清除用户对应标签clear_auths 'root','linqing'-- 查看当前用户的标签get_auths 'root'-- 展示指定可是化标签的Cell，当cell没有标签时也可以展示scan 'linqing:test',&#123; AUTHORIZATIONS =&gt;['linqing']&#125; PS：HBase可以自定义标签算法，参考 参考How-to: Enable User Authentication and Authorization in Apache HBase 权限矩阵 Apache HBase Cell Level Security, Part 1 Apache HBase Cell Level Security, Part 2","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://linqing2017.github.io/tags/HBase/"}]},{"title":"Impala 安装部署","slug":"0-安装部署","date":"2019-02-24T16:00:00.000Z","updated":"2020-05-14T07:59:03.216Z","comments":false,"path":"2019/02/25/0-安装部署/","link":"","permalink":"https://linqing2017.github.io/2019/02/25/0-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/","excerpt":"关于Impala安装部署的一些说明。","text":"关于Impala安装部署的一些说明。 安装方案Impala作为Cloudera主导开发的一个开源SQL工具，在github上有Apache/impala,Cloudera/impala两个代码仓库。从提交的活跃度上看，Cloudera依然是Impala的主导者。 当前,Impala支持运行在有以下这几个系统上: CDH MapR AWS S3 比较令人震惊的是：官方没有任何声明 Impala 能够运行在 Apache Hadoop 上，虽然谁都知道肯定没有问题。Cloudera 至今没有提供在 Apache Hadoop 版本的安装包，也就是说Impala on Apache Hadoop 官方无法保证兼容性完全没有问题。 目前，Impala有以下几种部署方案： 通过Cloudera Manager的Parcels包安装； 通过Cloudera 提供的RPM包安装； 通过代码编译安装； 通过上面三种方案，安装的Impala连接Hive、Hadoop、HBase的Client均是CDH版本。 Requirements官网给出了Requirements,包括以下几点： Hive metastore service 依赖 指明依赖Oracle JDK，其他版本的JDK可能引发ISSUSE 2.2以上的版本需要运行在 SSSE3 指令集的CPU上 官方推荐单点Impalad内存128GB以上（呵呵） 官方不推荐使用root运行Impala，原因是会影响性能","categories":[{"name":"大数据","slug":"大数据","permalink":"https://linqing2017.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"Cloudera","slug":"Cloudera","permalink":"https://linqing2017.github.io/tags/Cloudera/"},{"name":"Impala","slug":"Impala","permalink":"https://linqing2017.github.io/tags/Impala/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-02-16T16:00:00.000Z","updated":"2020-05-14T07:59:03.311Z","comments":false,"path":"2019/02/17/hello-world/","link":"","permalink":"https://linqing2017.github.io/2019/02/17/hello-world/","excerpt":"通过GitPage和Hexo搭建个人博客！我要做一个有逼格的程序员！！！！","text":"通过GitPage和Hexo搭建个人博客！我要做一个有逼格的程序员！！！！ 建站原理Github有一个功能叫做“GitHub Pages”，这是一个对小码农提高逼格非常有用的功能。我们可以创建一个名字为“{用户名}.github.io”的仓库，在这个仓库发布静态网页可以通过“https://{用户名}.github.io”访问。 搭建博客的原理非常简单：利用博客工具将写好的Markdown文档转换成静态网页，推送到Github的仓库中，最后通过Github Page访问。 当然如果你是前端狗，又买得起域名、VPS~ 关掉浏览器吧！这篇博客太小儿科了！ 博客框架当前有非常多的博客框架，可以快速的将Markdown、主题、评论、Tags、文章归档等功能生成相应的静态网页，非常适合“前端水准低幼人士”！ 下面简单介绍一下，两个静态博客主流框架：jekyll和hexo。 JekyllJekyll是Github原生支持的解析器，我深入研究半天以后发现：这个玩意用着好像有点麻烦，好多功能需要自己开发，问题是我完全不会前端呀！！！？相比Hexo的唯一优势是：在Github上只需要维护一个分支，所有Markdown文章都是动态解析成Index的。 如果你完全不想定制，只想用别人的模板，可以参考下面的使用步骤： Step 1： 到github搜索Jekyll关键字，找一个功能齐全的仓库（比如这位老兄的就不错）fork之； Step 2： 把仓库名称改成“{你的Github用户名}.github.io”，pull到本地； Step 3： 读完README，发挥你的聪明才智，改改_config.yml文件、改改图片……弄的这个工程好像是你自主研发的一样就差不多了； Step 4： 删除_post目录下面的文档，换成你自己的； Step 5： Push到仓库，差不多就这样了； 参考jekyll 官网 jekyll 中文网 HexoHexo比Jekyll的优势在于：能找的更多功能齐全的主题，并且能够生成完全静态网页，不会像Jekyll一样被Github限制使用插件。 可以参考下面的步骤建站： Step 1： 安装nodejs，通过npm命令安装hexo工具； Step 2： 创建一个hexo工程； Step 3： copy一个好看的主题，放到hexo工程目录下的themes。我拷贝的是这个兄弟定制的主题)； Step 4： 发挥想象力修改一下工程的_config.yml、主题的_config.yml； Step 5： 编译Hexo，推送编译内容到仓库； 参考Hexo中文网 操作步骤搭建Hexo开发环境123456789101112131415# Hexo基于NodeJS，用npm进行包管理npm install hexo-cli -g # 安装 hexo-cli 工具。-g 表示全局安装，安装完成后在任意目录都可以使用hexo模块。hexo init lqblog # 创建hexo工程，lqblog是工程目录cd lqblog# 在工程目录下下安装以下，hexo插件，注意这些插件只有在工程目录下可用npm install hexo-generator-feed --savenpm install hexo-renderer-less --savenpm install hexo-generator-json-content --savenpm install hexo-helper-qrcode --savenpm install hexo-deployer-git --savenpm install Hexo基本操作12345hexo init lqblog #初始化化工程hexo generate #编译hexo server #运行博客hexo clean #清除编译内容hexo deploy #push编译结果到远程仓库 搭建Jekyll开发环境Jekyll基于Ruby需要先装ruby环境。 Centos下安装ruby环境，可以使用RVM安装（YUM源上装的Ruby版本太久无法安装Jekyll）。","categories":[{"name":"小工具","slug":"小工具","permalink":"https://linqing2017.github.io/categories/%E5%B0%8F%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://linqing2017.github.io/tags/Hexo/"}]}]}